Anvar, [07.02.21 11:12]

а есть какие-то работы которые показывают что произвольная нейронная сеть эквивалентна какому-то ядерному методу? 
я знаю только от такую https://arxiv.org/pdf/1806.07572.pdf но она вроде немногим раньше была написана

@khamenya думаю тебе будет интересно https://arxiv.org/pdf/2012.00152.pdf

Every Model Learned by Gradient Descent Is Approximatelya Kernel Machine


[In reply to Valery]
сразу можешь проглядеть вот этот пост https://www.reddit.com/r/MachineLearning/comments/k8h01q/r_wide_neural_networks_are_feature_learners_not/

там в некотором смысле противоположный взгляд представлен (но сама статья в отличие от первой на 63 страницы:), я не осилил)

=============================


Многие, наверно, видели эти чудо картинки, которые делает UMAP просто из 
множества целых чисел используя разложение на простые числа https://mathoverflow.net/a/355631/10446 . 
Остается загадкой отражают ли они структуру простых чисел или артифакт UMAP.  
Вот тут аргуметация за второе https://twitter.com/hippopedoid/status/1318917878364672001 (что вообщем и раньше ожидалось)

А кто-ть слышал о trimap ? - пишут, что якобы лучше других методов снижение размерности ловит глобальную структуру.  (Лучше tsne umap  ) https://arxiv.org/abs/1910.00204

Alexander Chervov, [25.01.21 07:30]
[In reply to Yuri Baramykov]
https://www.kaggle.com/alexandervc/moa-data-visualization-via-dimensional-reduct - тут я упражнялся - разные методы (больше 10 ) на датасете МоА


Alexander Chervov, [25.01.21 07:30]
[In reply to Yuri Baramykov]
https://www.kaggle.com/alexandervc/moa-data-visualization-via-dimensional-reduct - тут я упражнялся - разные методы (больше 10 ) на датасете МоА

Alexander Chervov, [25.01.21 07:33]
[In reply to Yuri Baramykov]
Если прям совсем красоту хочешь - https://johnhw.github.io/umap_primes/index.md.html но может долго считаться



Alexander Chervov, [25.01.21 07:30]
[In reply to Yuri Baramykov]
https://www.kaggle.com/alexandervc/moa-data-visualization-via-dimensional-reduct - тут я упражнялся - разные методы (больше 10 ) на датасете МоА

Alexander Chervov, [25.01.21 07:33]
[In reply to Yuri Baramykov]
Если прям совсем красоту хочешь - https://johnhw.github.io/umap_primes/index.md.html но может долго считаться

Sviatoslav Iguana, [25.01.21 08:24]
[ Photo ]
Вот что с Рикардо делал Ivis

Alexander Chervov, [25.01.21 08:50]
[Forwarded from iggisv9t channel]
[ Photo ]
Чуваки из @sberlogawithgraphs опять не дают мне спокойно работать и подстрекают понижать размерность Рикардо Милоса. Вот такую штуку попробовал. Я бы сказал, что это самый забавный результат. Не уверен, что хороший, т.к. можно похожую штуку сделать просто накидывая на оси оригинальных данных случайные простые функции. Но это надо пробовать, я не знаю.
https://pypi.org/project/trimap/

===========================================================


Максим Ермаков, [24.01.21 18:54]
Ребят, немного прослушал на вводной лекции: какой графовый движок будет в курсе рассматриваться? Поиграл с WebVOWL, интересно, хочу себе что-нибудь поставить self-hosted.


migalkin, [24.01.21 19:45]
[In reply to Максим Ермаков]
В целом материал вендоро-независимый - хватит и питоновской rdflib с in-memory хранилищем (Apache Jena если вы адепт java). 
Если вы подразумеваете субд, то есть открытые и проприетарные движки. Из открытых, например, есть Virtuoso (https://virtuoso.openlinksw.com/) . 
GraphDB (https://graphdb.ontotext.com/) не открытый, но есть free tier. 
Из более навороченных есть Stardog (https://www.stardog.com/) c 30-дневным триалом. 
У всех есть докер-контейнеры, чтобы задеплоить по-быстрому


==========================

+100

В некотором смысле весь нынешний практический успех нейронных сетей — это пощёчина математикам, заигравшимся в параметрической статистике.

а ведь на практике как? — у вас есть  какие-то данные "из чудовищной смеси хрен знает каких распределений" — именно с такими данными нам приходится выживать. 

крутые фундаментальные математики не осчастливили нас внятным семантическим инструментарием для этого случая, а ребята с инженерным/прикладным стилем мышления стали собирать работающие статоцениватели "из того, что валялось в математическом гараже"


Ну это вроде еще вапник в далекие года писал, что отличие мл от статистики, что бы не пытаемся узнать точно, что же за распределение сгенерировало данные, потому что часто это задача оч сложная

кст, я недавно общался с одним очень опытным математиком, директором небольшого института математической статистики тут в Германии. Когда я попытался обратить его внимание на происходящее в DS/ML, попытаться заинтересовать его, то тот по сути взорвался в негодовании: "я этой вашей хайповой фигнёй заниматься не стану, наука идёт своим путём, а ваша хайповая пена будет смыта историей". На этом разговор и закончился. А специалист в матстате реально глубокий, сильный. 


Мои два знакомых немецких полных профессора вполне активно в сторону ML работают, но конечно со своей, довольно специфической стороны


PANOV:
Сделаю некоторое количество утверждений, основанных на моем опыте работы в области матстатистики:
1. Параметрические задачи были более менее исследованы к 80м годам 20 века. С тех пор основной фокус был как раз на непараметрике и она была тоже очень подробно изучена. 

2. Непараметрика от параметрического случая, если совсем формально, отличается только тем, что вместо конечномерного вектора параметров берется бесконечномерный. При этом семейство распределений все равно остается, просто оно менее жестко задано (например, все функции плотности, которые дважды непрерывно дифференцируемы и норма второй провизводной не превышает какого-то числа). 

3. Тестирование гипотез для непараметрических моделей тоже вполне себе существует.

4. Статей по выбору моделей конечно меньше чем просто по оцениванию, но тоже довольно много. Их меньше просто потому, что вопрос более сложный для изучения. Тем не менее этот самый выбор моделей (построение оценок, который адаптивны к классу распределений данных) очень хорошо изучен в матстатистике.

5. С нейронными сетями у статистиков действительно был прокол, так как никто не ожидал, что они могут так хорошо работать. Сколько-нибудь хорошего понимания, каким чудом современные нейросети дают результат нет до сих пор. Но в последние годы есть заметные продвижения, в частности теория double descent.

SHESTOV:
А почему нет хорошего понимания, поч сети работают? Просто очень хороший аппроксиматор + регуляризация...

PANOV:
потому что современные сети по сути работают как интерполяторы, т.е. регуляризация в классическом смысли ни при чем
по идее должны переобучаться в хлам

SHESTOV:
В смысле не причем? Всякие стохастические спуски регуляризуют поверхность ошибок. Residual слои тоже.

PANOV:
в смысле, что в классическом смысле не регуляризует, т.к. все равно получается интерполяция
т.е. точный фит всей выборки

SHESTOV:
Да почему? Есть импликация в одну сторону - если модель переобучается, то можно это пофиксить, но за счет увеличения смещегия, bias'а. Но импликации в другую то нет, что если модель без смещения, то у нее большая дисперсия.

CHERVOV:
В лекциях Ян Лекунна он часто обращает внимание на  проблему отсутсвия математического понимания работы сетей. В частности - его гипотеза - у сетей есть оргромное количество локальных экстремумов, но видимо чудо в том, что они все примерно дают одно и тоже значение целевой функции - то есть да мы можем получать разные параметры при обучении - но это типа не сильно важно. Также как   Максим правильно написал по идее сети должны переобучаться, но нет, почему - тоже вроде не ясно


SHESTOV:
Ну они же дают такое за счет регуляризвции стохастическим градиентным спуском, как я понимаю? То есть это как бы сглаживает поверхность ошибок, и мы от всякмх плохих и ненадежных мелких минимумлв избавляемся..

CHERVOV:
если я правильно понимаю, то нет. Дело в самой структуре сети , ну по крайней мере в том числе
При фиксации обсучающей выборки - сеть -  функция от парамеров. Вопрос какие у этой функции экстремумы- гипотеза - большинство на одном уровне - и именно туда идет град спуск. Если я правильно понимаю Лекунна

SHESTOV:
Double descent это же совсем не новая вещь и ничего загадочного в нем нет. Типа, есть например переобпределенные и несовместные системы линейных уравнений, и есть недоопределенные и совместные. И те, и те решаются за счет регуляризации. При этом в недоопределенных системах как раз та самая интерполяция и возникает же.

PANOV:
Lepsky method решает эту задачу. 30 лет известен уже

SHESTOV:
Вроде если обычным градиентным спуском обучать сеть, то она просто не обучится, не от времени работы, а от того, что хороший минимум не найдет

Моя основная мысль в том, что никакой общепринятой теории пока нет. Попыток делается много, но чего-то явного пока не выкристаллизовалось.

Конечно, про knn очень просто доказать ил про ядерную регрессию
Обычная ядерная регрессия является оптимальной для оценки широких классов непараметрических функций

Ну knn это просто непараметрическая оцннка плотности.

А про какой нибудь svm такого нет

CHERVOV:
Тут вот Лекун говорит о графах, графах знаний в том числе https://www.youtube.com/watch?v=UGPT64wo7lU мне кажется там же он упоминает это гипотезу - но не уверен , сейчас быстро не могу найти
он выкладывал кстати 3  лекции свооих, каак понял 

1. Введение в GNN
https://lnkd.in/gF_NZ-5
Слайды: https://rb.gy/yabvud
2. Последние события в ГННС
https://lnkd.in/gH-JS9u
Слайды: https://rb.gy/quo3n6
3. Бенчмаркинг ГННС
https://lnkd.in/gVX3VB7
Слайды: https://rb.gy/fjv659

YouTube (https://lnkd.in/gF_NZ-5)
Week 13 – Lecture: Graph Convolutional Networks (GCNs)
Course website: http://bit.ly/pDL-home
Playlist: http://bit.ly/pDL-YouTube
Speaker: Xavier Bresson

Кстати, в той лекции он обсуждает еще перевод без паралельных корпусов


SHESTOV:
Моя основная мысль это то, что то, что нейронки не переобучаются жестко, не противоречит никакой теории, это просто противоречит старой интуиции использования ml

- SGD Generalizes Better Than GD (And Regularization Doesn't Help). (arXiv:2102.01117v1 [cs.LG])
 http://arxiv.org/abs/2102.01117

PANOV:
Ну почему нет, есть http://proceedings.mlr.press/v125/bousquet20a/bousquet20a.pdf
Может не в общем случае, но есть


COOKIE THIEF:
В тему недавнего разговора про SGD и генерализацию
https://twitter.com/DeepMind/status/1356931190448664577?s=19

Twitter (https://twitter.com/DeepMind/status/1356931190448664577?s=19)
DeepMind
Why does Stochastic Gradient Descent generalise well in deep networks? Our team shows that if the learning rate is small but finite, the mean iterate of random shuffling SGD stays close to the path of gradient flow, but on a modified loss landscape https://t.co/JUAzPujWfP



--------------
Alina Mikhaylenko:
Один из них включает в себя метод машинного перевода, то есть там символьные вычисления скорее преобладали, чем численное решение. Есть еще https://github.com/DiffEqML/torchdyn – это я ещё пока не успела разобрать, к сожалению, но всё собираюсь, ибо сейчас как раз работаю на стыке всего этого. Хотя не совсем, мы  предсказываем сейчас эволюцию нелинейных уравнений

GitHub (https://github.com/DiffEqML/torchdyn)
DiffEqML/torchdyn
A PyTorch based library for all things neural differential equations - DiffEqML/torchdyn



VALERA:
о, вы, похоже, занимаетесь гомеоморфизмами инвариантов на нелинейных топологиях? может есть желание в Journal Club формате что-то рассказать?

Alina Mikhaylenko:
https://www.linkedin.com/in/alina-mikhaylenko-9059571a5/
Нет, гомеоморфизмами я баловалась при анализе спектрограмм, полученных с одного прибора, что разработали в институте автоматики СО РАН, с ним куча проблем и никто не может выяснить до конца, как и что, и у меня рождались безумные идеи, которые вполне имели место быть, как оказалось. А сейчас у меня основная задачка – применение нейронных сетей для прямой спектральной задачи Захарова-Шабата для решения нелинейных уравнений методом обратной задачи рассеяния. А что такое у нас метод обратной задачи рассеяния — это, по сути своей, метод решения задачи Коши для нелинейных эволюционных уравнений, то есть по эволюции данных рассеяния мы восстанавливаем эволюцию решения нелинейного уравнения. В прямой задаче рассеяния, например, в основе лежит оператор Шредингера, точнее его спектр, состоящий из множества чисел, характеризующих линейный оператор :)



VALERA:
Бляха-муха, хоть в НГУ ещё чему-то нормально учат.


CHERVOV:
в "прошлой жизни" я занимался обратной задачей. Расскажи подробней как туда нейронки можно засунуть ? Это для численного решения или как ? Ну и если численно, то обычные методы конечных разностей неплохо решают УРЧП, чем может помочь МОЗР ? В численных схемах одна из хотелок - строить схемы, для которых бы законы сохранения (которых бесконечный набор) точно бы сохранялись, не знаю какяя там СОТА сейчас, лет десять назад вроде это не вполне умели




Alina Mikhaylenko, [04.02.21 01:49]
[In reply to Alexander Chervov]
а какой именно обратной задачей?

Alexander Chervov, [04.02.21 01:54]
[In reply to Alina Mikhaylenko]
Да разными , интегрируемыми системами , квантовыми интегрируемыми системами типа спин цепочек и тд

Alina Mikhaylenko, [04.02.21 04:02]
[In reply to Alexander Chervov]
к примеру, существует уравнение, точные решения которого представляют собой нелинейные волны, одно из таких — уравнение Кортевега-де Фриза. Такое уравнение имеет решения солитонного типа, и любые возмущения (начальные) с течением времени эволюционируют в конечный набор солитонов в пространстве. И поиск данных решений как раз и заключаетя в использовании метода обратной задачи рассеяния, о которой я написала ранее

Alexander Chervov, [04.02.21 04:07]
[In reply to Alina Mikhaylenko]
Это я все прекрасно понимаю. А нейронки тут причем ?


Alina Mikhaylenko, [04.02.21 04:10]
[In reply to Alexander Chervov]
Ну, пытаемся как раз таки предсказывать эту самую эволюцию, начало было у наших коллег с НГУ: 
https://research.aston.ac.uk/en/publications/application-of-neural-networks-to-determine-the-discrete-spectrum

Alexander Chervov, [04.02.21 04:15]
[In reply to Alina Mikhaylenko]
Спасибо за ссылку.  Предсказать число солитонов - это вопрос о том сколько дискретных значений в Шредингере. Как тут нейронка поможет ?

Valery, [04.02.21 04:27]
[In reply to Alexander Chervov]
вопрос не мне, но вроде очевидно же. нейронки — это способ представления топологии исходного (вероятностного) пространства.  

то есть нейронка может описывать (вероятностное) пространство, в котором и будут пролегать эволюционные дорожки (последовательности (гомеоморфных) преобразований).

т.е. вполне естественно использовать нейронки.

P.S. сначала не туда подклеил ответ, сорри.

Alexander Chervov, [04.02.21 04:41]
Для уравнение  Кортевега де Фриза - пространство - это просто линейное бесконечномерное про-во функций на прямой. Если обозначать функцию как u(x) то уравнение - нелинейное уравнение в частных производных, которое описывает как u(x) изменяется во времени. Если задачать специфическое начальное условие (солитон - специальный горбик) то этот горбик - не будет менять свою форму со временем , а просто центр горбика будет бежать по прямой.

Alexander Chervov, [04.02.21 04:46]
Много великих математиков (Филдсовких медалистов) поработала вокруг этого поскольку оказалось что это все связано с огромным набором сложных математических теорий - алгебраическая геоматрия , группы Ли и тд. Но все же развитие этого сюжета мне всегда представлялось несколько ортогональным к нейронным сетям

Alexander Chervov, [04.02.21 04:52]
Чтобы хоть как-то привязать это к теме чата )) Иногда люди хотя подсчитать сколько графов какого специального типа бывает . Иногда пишут производящие функции для подобных вещей . Оказывается эти функции иногда удовлятеворяют разногого рода солитонным уравнениям. Ну и морально - это общий принцип - если правильно записать подсчет правильных вещей (не только графов, а "чего угодно")- производящие функции будут решениями солитонных урчп.
































===============
🗞 Виз Ньюз, [01.02.21 21:39]
[Forwarded from iggisv9t channel]
[ Photo ]
Ковырял репозиторий umap на гитхабе и через его автора вышел на интересный ноутбучек с анализом реддита https://github.com/lmcinnes/subreddit_mapping/blob/master/Subreddit%20Mapping%20and%20Analysis.ipynb

Ноутбук хорош не только картинками и темой. Это можно считать прямо туториалом по задачам снижения размерности, визуализации и как вообще жить с данными высоких размерностей. Всё разжёвано, очень основательно написан код. Можно брать себе куски в проекты 👀

Ещё он обернул мой любимый LargeViz в sklearn-style интерфейс.


===============


Valery, [02.02.21 15:31]
[In reply to KONST]
диффуры в дескретном виде — это рекуррентные соотношения (https://ru.wikipedia.org/wiki/%D0%A0%D0%B5%D0%BA%D1%83%D1%80%D1%80%D0%B5%D0%BD%D1%82%D0%BD%D0%B0%D1%8F_%D1%84%D0%BE%D1%80%D0%BC%D1%83%D0%BB%D0%B0). Что интересно, и те и другие решаются похожими методами.

то есть, по сути, RNN — это и есть NN-овский брат близнец диффуров.





Здесь тоже так думают.

https://github.com/SciML/DiffEqFlux.jl

GitHub (https://github.com/SciML/DiffEqFlux.jl)
SciML/DiffEqFlux.jl
Universal neural differential equations with O(1) backprop, GPUs, and stiff+non-stiff DE solvers, demonstrating scientific machine learning (SciML

ну простейшая сеть есть в склеарн - пару строк на питоне - если данные есть проще попробовать


ух ты ссылка про sciML прямо бомба



















Nikita Varganov, [03.02.21 04:38]
[In reply to 🇻 🇱 🇦 🇩]
Библиотека для создания data-приложений

Yuri Baramykov, [03.02.21 04:39]
Это типа фронт

Nikita Varganov, [03.02.21 04:39]
Быстро, удобно и красиво

🇻 🇱 🇦 🇩, [03.02.21 04:41]
😍 Круть, нужно будет заценить

Yuri Baramykov, [03.02.21 04:43]
[In reply to 🇻 🇱 🇦 🇩]
https://share.streamlit.io/optimizationguys/craftml/main/app.py

Yuri Baramykov, [03.02.21 04:44]
https://github.com/OptimizationGuys/CraftML/raw/main/screenshots/CraftML%20-%20video

Yuri Baramykov, [03.02.21 04:56]
Наверное лучше загрузить на Ютюб

Andrey Lukyanenko, [03.02.21 04:57]
streamlit - топовая штука. очень удобно

Yuri Baramykov, [03.02.21 05:05]
[In reply to 🇻 🇱 🇦 🇩]
https://youtu.be/JmNdwzVpS5E







==================================


https://www.notion.so/yads/Knowledge-Graphs-Course-2021-312c41e528b247d6921bfaa82bcd99ea общий нотион для совместных записок по курсу

Boyadzhi-notion on Notion (https://www.notion.so/yads/Knowledge-Graphs-Course-2021-312c41e528b247d6921bfaa82bcd99ea)
Knowledge Graphs Course 2021
A new tool for teams & individuals that blends everyday work apps into one.



https://twitter.com/mataneyal1/status/1359978174885605379?s=09

Twitter (https://twitter.com/mataneyal1/status/1359978174885605379?s=09)
Matan Eyal
How do you train a relation extractor without training data? We show simple and effective bootstrap approaches that work with as little as 3 user-provided examples. Our EACL is now available on https://t.co/fw2Xvp7Jko With https://twitter.com/asaf_amr https://twitter.com/hilleltt and https://twitter.com/yoavgo

