<!--META http-equiv="content-type" content="text/html; charset=windows-1252"-->
<meta http-equiv="content-type" content="text/html;charset=utf-8" />
<html>
<TITLE>NLP_bookmarks</TITLE>

C:\_PyNotes\DL_Keras_TF\TensorFlow-Tutorials-master\TensorFlow-Tutorials-master<br>
C:\_PyNotes\NLP\CNN_W2V<br>
NLP Challenge: "—á–∞–π–Ω–∏–∫—É —á–∞–π–Ω–∏–∫–æ–º –±–∏–ª–∏ –ø–æ —á–∞–π–Ω–∏–∫—É", "–ö–æ—Å–æ–π –∫–æ—Å–æ–π –∫–æ—Å–∏–ª –∫–æ—Å–æ–π –∫–æ—Å–æ–π." <br>

<a href="https://github.com/ftk1000/tools/blob/master/GIT_COMMANDS.txt"> Git Commands </A> |<br>
<a href="https://tsamsonov.github.io/r-geo-course/autocorrelation.html">–ì–ª–∞–≤–∞ 15 –ü—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ–Ω–Ω–∞—è —Ä–µ–≥—Ä–µ—Å—Å–∏—è | –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∏ –∞–Ω–∞–ª–∏–∑ –≥–µ–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö –Ω–∞ —è–∑—ã–∫–µ R</a>|



============================================================
============================================================
============================================================



<A HREF=""></A> |<br>
https://www.youtube.com/watch?v=-UjytpbqX4A –î–æ–±–∞–≤–ª—é —Å–µ–±–µ –≤ —Å–ø–∏—Å–æ–∫ - –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –ø—Ä–∏ –Ω–∞–ª–∏—á–∏–∏ –≤—Ä–µ–º–µ–Ω–∏

YouTube (https://www.youtube.com/watch?v=-UjytpbqX4A)
Graph Neural Networks (GNN) using Pytorch Geometric | Stanford University
This is the Graph Neural Networks: Hands-on Session from the Stanford 2019 Fall CS224W course. 
<A HREF=""></A> |<br>


Alexander Chervov, [29.01.21 04:23]
[ Poll : –í–∞—à–∏ –ø–æ–∂–µ–ª–∞–Ω–∏—è ]
- –°–¥–µ–ª–∞–ª –¥–∑ –º–æ–≥—É —Ä–∞—Å—Å–∫–∞–∑–∞—Ç—å
- –•–æ—á—É –æ–±—Å—É–¥–∏—Ç—å –ø–µ—Ä–≤—É—é –ª–µ–∫—Ü–∏—é –∏ –¥–∑
- –°–∫–∏–ø–∞–µ–º –æ–±—Å—É–∂–¥–µ–Ω–∏–µ –∏–¥—ë–º –∫ —Å–ª–µ–¥—É—é—â–µ–π –ª–µ–∫—Ü–∏–∏

Nikolai Bragin, [29.01.21 04:39]
[In reply to üáª üá± üá¶ üá©]
–í—Å–µ–≥–¥–∞ –Ω–∞–π–¥–µ—Ç—Å—è –∞–∑–∏–∞—Ç, –∫–æ—Ç–æ—Ä—ã–π –≤—ã–ª–æ–∂–∏—Ç –¥–æ–º–∞—à–∫–∏. https://github.com/Hoonst/cs224w_assignment

üáª üá± üá¶ üá©, [29.01.21 04:45]
–ù—É –º—ã –∏—Ö –≤–æ–æ–±—â–µ –Ω–∞ –∫–∞–≥–ª–µ –¥–µ–ª–∞–ª–∏ ü§£

üáª üá± üá¶ üá©, [29.01.21 04:45]
–í –∫–µ—Ä–Ω–µ–ª–∞—Ö, –∞ –¥–∞–Ω–Ω—ã–µ –≤ –¥–∞—Ç–∞—Å–µ—Ç—ã –∑–∞–ª–∏–ª–∏ ü§£

üáª üá± üá¶ üá©, [29.01.21 04:46]
–î–æ–º–∞—à–∫–∏ —Ç–æ –Ω–µ –º–µ–Ω—è—é—Ç—Å—è

Alexander Chervov, [29.01.21 04:46]
[In reply to üáª üá± üá¶ üá©]
–í—Ä–æ–¥–µ –º–µ–Ω—è—é—Ç—Å—è

Nikolai Bragin, [29.01.21 04:50]
–ù—É —ç—Ç–∏ —ç—Ç–æ–≥–æ –≥–æ–¥–∞, –æ—Ç–ª–∏—á–∞—é—Ç—Å—è –ª–∏ –æ–Ω–∏ –æ—Ç –ø—Ä–æ—à–ª–æ–≥–æ, –Ω–µ –∑–Ω–∞—é

<A HREF=""></A> |<br>
<A HREF="https://www.nytimes.com/interactive/2021/01/28/opinion/climate-change-risks-by-country.html">–ù–µ–¥–∞–≤–Ω—è—è –ø—É–±–ª–∏–∫–∞—Ü–∏—è –Ø—Ä–∏–Ω—ã –°–µ—Ä–∫–µ–∑ –≤ New York Times –æ –±—É–¥—É—â–∏—Ö –∫–ª–∏–º–∞—Ç–∏—á–µ—Å–∫–∏—Ö —Ä–∏—Å–∫–∞—Ö —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω –≤—Å–ª–µ–¥—Å—Ç–≤–∏–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –∫–ª–∏–º–∞—Ç–∞. –í —Ä–∞–±–æ—Ç–µ –∫—Ä–∞—Å–∏–≤–∞—è –≥—Ä–∞—Ñ–∏–∫–∞, –Ω–æ, –≤ —Ç–æ –∂–µ –≤—Ä–µ–º—è, –ø–æ–∫–∞ —è —á–∏—Ç–∞–ª –ø—É–±–ª–∏–∫–∞—Ü–∏—é, —Ç–æ –ª–æ–≤–∏–ª —Å–µ–±—è –Ω–∞ –º—ã—Å–ª–∏, —á—Ç–æ —Ç–∞–∫–æ–≥–æ —Ä–æ–¥–∞ —Å–∫—Ä–æ–ª–ª–∏-—Ç–µ–ª–ª–∏–Ω–≥ (—Å –ø—Ä–æ—Å—Ç—ã–º–∏ –ª–µ—Ç–∞—é—â–∏–º–∏ –ø–æ —Å—Ç—Ä–∞–Ω–∏—Ü–µ —Ç–µ–∫—Å—Ç–æ–≤—ã–º–∏ –±–ª–æ–∫–∞–º–∏) —É–∂–µ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–µ–ª—Å—è.

https://www.nytimes.com/interactive/2021/01/28/opinion/climate-change-risks-by-country.html</A> |<br>

<A HREF="https://www.nytimes.com/interactive/2015/03/19/upshot/3d-yield-curve-economic-growth.html">–õ–∞–º–ø–æ–≤–∞—è 3D –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∫—Ä–∏–≤–æ–π –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏ –≥–æ—Å—É–¥–∞—Ä—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ–±–ª–∏–≥–∞—Ü–∏–π –°–®–ê, –ì–µ—Ä–º–∞–Ω–∏–∏ –∏ –Ø–ø–æ–Ω–∏–∏ –∑–∞ –∞–≤—Ç–æ—Ä—Å—Ç–≤–æ–º –ê–º–∞–Ω–¥—ã –ö–æ–∫—Å (Amanda Cox) –∏ –ì—Ä–µ–≥–æ—Ä–∞ –ê–∏—à–∞ (Gregor Aish).
–¢–∞–∫ –≤—ã–≥–ª—è–¥–µ–ª–∞ –¥–∞—Ç–∞-–∂—É—Ä–Ω–∞–ª–∏—Å—Ç–∏–∫–∞ –≤ The New York Times –≤ 2015 –≥–æ–¥—É –¥–æ –∏–∑–æ–±—Ä–µ—Ç–µ–Ω–∏—è —Å–∫—Ä–æ–ª–ª–∏-—Ç–µ–ª–ª–∏–Ω–≥–∞. –ê —Å–ø—É—Å—Ç—è –ø–∞—Ä—É –ª–µ—Ç –ì—Ä–µ–≥–æ—Ä —Å—Ç–∞–Ω–µ—Ç —Å–æ–æ—Å–Ω–æ–≤–∞—Ç–µ–ª–µ–º –∏ CTO Datawrapper.
https://www.nytimes.com/interactive/2015/03/19/upshot/3d-yield-curve-economic-growth.html
</A> |<br>




–ó–¥—Ä–∞–≤—Å—Ç–≤—É–π—Ç–µ. –ú–æ–∂–µ—Ç–µ –ø–æ–¥—Å–∫–∞–∑–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –±—ã—Å—Ç—Ä—ã–π —Å–ø–æ—Å–æ–± –∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–∞—Ä–∏–π –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è —Ç–∞–∫–∏—Ö –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–π.
–ï—Å—Ç—å –ª–∏ —á—Ç–æ-–Ω–∏–±—É–¥—å –Ω–∞ –ø–∏—Ç–æ–Ω–µ?


Farid –°–∞–º—ã–π –ø—Ä–æ—Å—Ç–æ–π —Å–ø–æ—Å–æ–± —ç—Ç–æ —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ Plotly https://plotly.com/javascript/3d-charts. –£ –Ω–µ–≥–æ –µ—Å—Ç—å –±–∏–Ω–¥–∏–Ω–≥–∏ –¥–ª—è –ü–∏—Ç–æ–Ω–∞.

Plotly (https://plotly.com/javascript/3d-charts/)
3D Charts
Plotly.js makes interactive, publication-quality graphs online. Examples of how to make 3D graphs such as 3D scatter and surface charts.
<A HREF=""></A> |<br>

–ü—Ä–µ–∫—Ä–∞—Å–Ω–µ–π—à–∞—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è [1] Codex Atlanticus (–ê—Ç–ª–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ –∫–æ–¥–µ–∫—Å–∞) –õ–µ–æ–Ω–∞—Ä–¥–æ –î–∞ –í–∏–Ω—á–∏ [2] –∏–∑ –±–æ–ª–µ–µ —á–µ–º 1119 —Å—Ç—Ä–∞–Ω–∏—Ü —Ç–µ–∫—Å—Ç–∞ –∏ –∏–ª–ª—é—Å—Ç—Ä–∞—Ü–∏–π. 

–í –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏ –≤—Å–µ –µ–≥–æ –º–∞—Ç–µ—Ä–∏–∞–ª—ã –ø—Ä–æ—Å–∏—Å—Ç–µ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω—ã, –∫–ª–∞—Å—Å–∏—Ñ–∏—Ü–∏—Ä–æ–≤–∞–Ω—ã –∏ —Ä–∞–∑–¥–µ–ª–µ–Ω—ã –ø–æ —Ö—Ä–æ–Ω–æ–ª–æ–≥–∏–∏.

–ê–≤—Ç–æ—Ä—ã —Ä–∞–±–æ—Ç—ã The Visual Agency [3], –æ–Ω–∏ –∂–µ –ø—É–±–ª–∏–∫—É—é—Ç –¥–∞–Ω–Ω—ã–µ –ø—Ä–æ–µ–∫—Ç–∞ –≤ CSV —Ñ–æ—Ä–º–∞—Ç–µ [4]

–ü—Ä–æ–µ–∫—Ç—É —É–∂–µ –Ω–µ –º–µ–Ω—å—à–µ 1.5 –ª–µ—Ç, –µ–≥–æ –∞–≤—Ç–æ—Ä—ã –ø–æ–ª—É—á–∏–ª–∏ –∑–∞ –Ω–µ–≥–æ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–µ—Å—Ç–∏–∂–Ω—ã—Ö –ø—Ä–µ–º–∏–π. 

–õ–∏—á–Ω–æ —è –æ—Ç–Ω–µ—Å –±—ã –µ–≥–æ, –∫–∞–∫ –∏ –º–Ω–æ–≥–∏–µ –¥—Ä—É–≥–∏–µ –ø—Ä–æ–µ–∫—Ç—ã –ø–æ –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏, –∫ —Ü–∏—Ñ—Ä–æ–≤–æ–º—É –∫—É–ª—å—Ç—É—Ä–Ω–æ–º—É –Ω–∞—Å–ª–µ–¥–∏—é. –ö —Å–æ–∂–∞–ª–µ–Ω–∏—é, –Ω–µ–¥–æ–ª–≥–æ–≤–µ—á–Ω–æ–º—É, –ø–æ—Ç–æ–º—É —á—Ç–æ –ø—Ä–æ–π–¥–µ—Ç 5-10 –ª–µ—Ç, —Å–º–µ–Ω—è—Ç—Å—è —Ç–µ—Ö–Ω–æ–ª–æ–≥–∏–∏ –∏ –Ω–µ —Ñ–∞–∫—Ç —á—Ç–æ –æ—Å—Ç–∞–Ω–µ—Ç—Å—è –∏ —Å–∞–π—Ç –∏ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å –≤–∏–¥–µ—Ç—å –µ–≥–æ —Ç–µ–º–∏ —Å—Ä–µ–¥—Å—Ç–≤–∞–º–∏ —á—Ç–æ –º—ã —Å–µ–π—á–∞—Å –∏—Å–ø–æ–ª—å–∑—É–µ–º.

–°—Å—ã–ª–∫–∏:
[1] https://codex-atlanticus.it
[2] https://en.wikipedia.org/wiki/Codex_Atlanticus
[3] https://thevisualagency.com/
[4] https://codex-atlanticus.it/data/Leonardo.csv

<A HREF=""></A> |<br>
https://medium.com/criteo-labs/top-applications-of-graph-neural-networks-2021-c06ec82bfc18?sk=ae5cb2a78f323a69ffa5f97d0cb669cf

Medium (https://medium.com/criteo-labs/top-applications-of-graph-neural-networks-2021-c06ec82bfc18?sk=ae5cb2a78f323a69ffa5f97d0cb669cf)
Top Applications of Graph Neural Networks 2021
GNNs have come a long way in academia. But do we have good applications of them in industry?
<A HREF=""></A> |<br>

Boost then Convolve: Gradient Boosting Meets Graph Neural Networks

In our new work (https://openreview.net/forum?id=ebS5NUfoMKL) at ICLR 2021, we explore how to apply Gradient Boosted Decision Trees to graphs. Surprisingly, I haven't encountered before papers that test performance of pure GBDT on graphs, for example for node classification. 

GBDTs are usually used for heterogeneous data (e.g. in Kaggle competitions): the columns can be categorical, of different scale and meaning (e.g. income column vs age column). Such data is quite common in the real world, but most of the research graph datasets have sparse homogeneous nodes features (e.g. bag-of-words features or word embeddings). So we asked a question whether GNNs are efficient on graphs with heterogeneous features. 

The first insight is that you can just pretrain GBDT on the node features and use the predictions of GBDT for training GNN model. This already gives a boost to GNN model.

Second, we proposed a scheme how to train GBDT and GNN end-to-end, and this would additionally boost performance. 

Third, this combo of GBDT and GNN, which we call BGNN, converges much faster than GNN and therefore usually is faster to train than pure GNN.

Some limitations. 
* BGNN works well with heterogeneous features. So Cora datasets and others with homogeneous features are still better of with plain GNN. 
* The approach works for node regression and classification. We have some ideas how to extend it to link prediction or graph classification, but haven't worked it out yet. If you have some interest in continuing this line of work, let me know. 

The code and datasets are available here (https://github.com/nd7141/bgnn).

OpenReview (https://openreview.net/forum?id=ebS5NUfoMKL)
Boost then Convolve: Gradient Boosting Meets Graph Neural Networks
Graph neural networks (GNNs) are powerful models that have been successful in various grap


<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>
<A HREF=""></A> |<br>


============================================================
============================================================
============================================================
============================================================
 


<a href=""> </A> |<br>
<a href=""> </A> |<br>

https://pnoiman.github.io/etc/div_stocks.txt <br>

<H1>NLP_bookmarks</H1>
yandexdataschool/nlp_course<br>
ODS #audio_and_speech: 
https://app.slack.com/client/T040HKJE3/CH4JPJ4AD/thread/C04N3UMSL-1571514472.329900 <br><br>

=================================<br>

<a href="#YandexDataSchool">YandexDataSchool</a> |
<a href="#YandexNLP_2018">YandexNLP_2018</a> |
<a href="#YandexNLP_2019">YandexNLP_2019</a> |
<a href="#Simulation_RL_ReinforcementLearning">Simulation_RL_ReinforcementLearning</a>|
<a href="#PyTorch_PyTorch">PyTorch_PyTorch</a>|
<a href="#PySpark_PySpark">PySpark_PySpark</a>|
<a href="#HIVE_HIVE">HIVE_HIVE</a>|
<a href=""></a>|
<a href=""></a>|
<a href=""></a>|

<p>

<a href="#AB_Testing_AB_Testing">AB_Testing_AB_Testing</a> |
<a href="#Chervov_Chervov">Chervov_Chervov</a> |
<a href="#PyVisualize_Clustering">PyVisualize_Clustering</a> |
<a href="#OPTIMIZATION_SEMINAR">OPTIMIZATION_SEMINAR</a>|
<a href="#GRAPH_NETWORKS">GRAPH_NETWORKS</a>|
<a href="#XGBoost_XGBoost">XGBoost_XGBoost</a> |
<a href="#TIME_SERIES">TIME_SERIES</a>|
<a href=""></a>|
<a href=""></a>|
<a href=""></a>|


<p>
<a href="#Transformers/BERT">Transformers/BERT</a> |
<a href="#Punctuation">Punctuation</a> |
<a href="#SENTIMENT_ANALYSIS">SENTIMENT_ANALYSIS</a> |
<a href="#TEXT_Summarization">TEXT_Summarization</a> |
<a href="#NLU_SUMMARY">NLU_SUMMARY</a> |
<a href="#STANFORD_NLP">STANFORD_NLP</a> |
<a href="#NER_PyTextRank">NER_PyTextRank</a> |
<a href="#EMBEDDING_EMBEDDING">EMBEDDING_EMBEDDING</a> |
<a href="#Allen_NLP">Allen_NLP</a> |
<a href="#CLIP_NLTK">CLIP_NLTK</a> |
<a href="#Text_Classf">Text_Classf</a> |
<a href="#DIALOGS_DIALOGS">DIALOGS_DIALOGS</a>|
<a href=""></a>|
<a href=""></a>|

<p>
<a href="#NLP_VIDEOS">NLP_VIDEOS</a> |
<a href="#KAGGLE_KAGGLE">KAGGLE_KAGGLE</a> |
<a href="#KAGGLE_CURRENT_COMP">KAGGLE_CURRENT_COMP</a> |
<a href="#COLAB_COLAB">COLAB_COLAB</a> |
<a href="#CLASS">CLASS</a> |
<a href="#Research_Google">Research_Google</a> |
<a href="#udacity">UDACITY</a> |
<a href="#CONFERENCE">CONFERENCE</a> |
<a href=""></a>|
<a href=""></a>|


<a href="#GANS">GANS</a> |
<a href="#StyleTransfer">StyleTransfer</a> |
<a href="#CNN_CNN">CNN_CNN</a> |
<a href="#RNN_LSTM">RNN_LSTM</a> |
<a href="#TF2">TF2</a> |
<a href="#KERAS_KERAS">KERAS_KERAS</a> |
<a href="#CHAT_MINING">CHAT_MINING</a> |
<a href=""></a>|
<a href=""></a>|
<p>
<a href="#TopicModeling">TopicModeling</a> |
<a href="#REGEX_REGEX">REGEX_REGEX</a> |
<a href="#NN_Theory">NN_Theory</a> |
<a href="#VAE">VAE</a> |
<a href="#DATASETS_DATASETS">DATASETS_DATASETS</a> |
<a href="#DOCS_BOUND_DETECTION__COMP">DOCS_BOUND_DETECTION__COMP</a> |
<a href="#GPUGPU">GPUGPU</a> |
<a href="#PYPY">PYPY</a> |
<a href="#CRF">CRF</a> |
<a href="#NODEJS">NODEJS_NODEJS</a> |
<a href="#CODE_SERVER">CODE_SERVER</a> |
<a href="#WEBAPPWEBB">WEBAPPWEBB</a> |
<a href=""></a>|
<a href=""></a>|

<p>
<a href="#PEOPLE_PEOPLE">PEOPLE_PEOPLE</a> |
<a href="#HABR_HABR">HABR_HABR</a> |
<a href="#SET_THEORY_SET_THEORY">SET_THEORY_SET_THEORY</a>|
<a href="#SALARY_SALARY">SALARY_SALARY</a> |
<a href="#MoneyMoneyMoney">MoneyMoneyMoney</a> |
<a href="#JOBS_JOBS">JOBS_JOBS</a> |
<a href="#BOOKS_BOOKS">BOOKS_BOOKS</a>|
<a href="#FTK_Links_FTK_Links">FTK_Links_FTK_Links</a>|
<a href="#GIT_GIT_GIT">GIT_GIT_GIT</a>|
<a href="#COURSERA_COURSERA">COURSERA_COURSERA</a>|
<a href="#BITCOIN_BITCOIN">BITCOIN_BITCOIN</a>|<br>
<a href=""></a>|<br>
<a href=""></a>|
<a href=""></a>|<br>

<p>

<a href="https://ftk1000.github.io/NLP_Demos/">ftk1000.github.io/NLP_Demos/</A> |
<a href="https://www.analyticsvidhya.com/blog/2019/08/complete-list-important-frameworks-nlp/?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter"> NLP Frameworks </a> |

https://www.youtube.com/results?search_query=JAXON.ai+snorkel
https://www.youtube.com/watch?v=RUPbYvzSrg0
Snorkel: Programming Training Data with Paroma Varma of Stanford University (2019)



<p>
<hr>
<hr>
<a name="BITCOIN_BITCOIN">
<div id="BITCOIN_BITCOIN">
<p> BITCOIN_BITCOIN:<p>

<a href="https://www.nytimes.com/2021/01/12/technology/bitcoin-passwords-wallets-fortunes.html">NYT: –ö–∞–∂–¥—ã–π –ø—è—Ç—ã–π –±–∏—Ç–∫–æ–∏–Ω –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω –≤–ª–∞–¥–µ–ª—å—Ü–∞–º. –ù–∞–¥–æ –î–° —É—Å–∏–ª–∏—è –Ω–∞–ø—Ä–∞–≤–∏—Ç—å –Ω–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ "—É—Ç–æ–Ω—É–≤—à–∏—Ö" –±–∏—Ç–∫–æ–∏–Ω–æ–≤.
Of the existing 18.5 million Bitcoin, around 20 percent ‚Äî currently worth around $140 billion ‚Äî appear to be in lost or otherwise stranded wallets, according to the cryptocurrency data firm Chainalysis. 
</a>|<br>
<a href="https://arxiv.org/pdf/1709.00440.pdf">PassGAN: A Deep Learning Approach for Password Guessing<  https://arxiv.org/pdf/1709.00440.pdf/a>|<br>
<a href="https://github.com/brannondorsey/PassGAN">https://github.com/brannondorsey/PassGAN</a>|<br>
<a href="https://www.sciencemag.org/news/2017/09/artificial-intelligence-just-made-guessing-your-password-whole-lot-easier">Researchers at the Stevens Institute of Technology in New Jersey, and the New York Institute of Technology have devised what they claim is a highly effective way to guess passwords using a deep learning tool called Generative Adversarial Networks (GANs). In their experiments the researchers were able to match nearly 47% ‚Äî or some 2,774,269 out of 5,919,936 passwords ‚Äî from a testing set comprised of real user passwords that were publicly leaked after a 2010 data breach at RockYou.</a>|<br>
<a href="https://www.forbes.com/sites/baldwin/2020/02/16/can-all-of-bitcoin-be-hacked/?sh=13e8ca3b1dc1">FORBES: Sony used elliptic curves to protect its PlayStation. In order to run, a game would have to provide a digital signature constructed from Sony‚Äôs secret key, the same kind of key that protects your bitcoin. The signature routine uses, as one of its inputs, a different randomly chosen number for each validating signature. Sony goofed, recycling the same number. It turns out that this enabled anyone possessing two legitimate games and a knowledge of high-school algebra to compute the secret key and run pirated games.
Some bitcoin owners, trying to manage their own coin wallets, have made the same mistake Sony did with its game console. Writes one security expert: ‚ÄúA lot of Russian bitcoin hackers have coded bots to automatically grab coins from vulnerable addresses.‚Äù
</a>|<br>
<a href=""></a>|<br>
<a href=""></a>|<br>
<a href=""></a>|<br>







================================














<p>
<hr>
<hr>
<a name="GIT_GIT_GIT">
<div id="GIT_GIT_GIT">
<p> GIT_GIT_GIT:<p>
git clone https://github.com/ftk1000/math.git<br>
cd math<br>
touch .gitignote<br>
git add .<br>
git commit -m '2020.10.24'<br>
git push -u origin master<br>

<p>
Quick setup ‚Äî if you‚Äôve done this kind of thing before<br>
or	<br>
https://github.com/ftk1000/math.git<br>
Get started by creating a new file or uploading an existing file. We recommend every repository include a README, LICENSE, and .gitignore.<br>
<br>
‚Ä¶or create a new repository on the command line<br>
echo "# math" >> README.md<br>
git init<br>
git add README.md<br>
git commit -m "first commit"<br>
git branch -M main<br>
git remote add origin https://github.com/ftk1000/math.git<br>
git push -u origin main<br>
                <br>
‚Ä¶or push an existing repository from the command line<br>
git remote add origin https://github.com/ftk1000/math.git<br>
git branch -M main<br>
git push -u origin main<br>
‚Ä¶or import code from another repository<br>
You can initialize this repository with code from a Subversion, Mercurial, or TFS project.<br>
</p>

<a href="https://kbroman.org/github_tutorial/pages/init.html"> kbroman.org/github_tutorial </a>|<br>
<a href="https://www.atlassian.com/git/tutorials/saving-changes/gitignore"> Atlasian Tutorial on Git</a>|<br>
<a href="https://cli.github.com/manual/gh_repo_create">https://cli.github.com/manual/gh_repo_create</a>|<br>
<a href="https://git-scm.com/book/en/v2/Git-Basics-Recording-Changes-to-the-Repository"> GIT BASICS: Gitignor file example in the middle </a>|<br>
<a href=""></a>|<br>
<a href=""></a>|<br>
<a href=""></a>|<br>



<p>
<hr>
<hr>
<a name="FTK_Links_FTK_Links">
<div id="FTK_Links_FTK_Links">
<p> FTK_Links_FTK_Links:<br>
<a href=""></a>|<br>
<a href=""></a>|<br>
<a href="https://github.com/ftk1000/pyviz/blob/master/Map_US_States.ipynb">https://github.com/ftk1000/pyviz/blob/master/Map_US_States.ipynb</a>|<br>
<a href="https://github.com/ftk1000/pyviz">https://github.com/ftk1000/pyviz</a>|<br>
<A HREF="https://github.com/ftk1000/diss/blob/master/FourierTransform.md"> https://github.com/ftk1000/diss/blob/master/FourierTransform.md </A> |<br>
<A HREF="https://github.com/ftk1000/etc/blob/master/lte.md"> https://github.com/ftk1000/etc/blob/master/lte.md</A> |<br>


<A HREF="https://github.com/ftk1000/GraphNeuralNets/blob/main/Geometric_DL.md"> https://github.com/ftk1000/GraphNeuralNets/blob/main/Geometric_DL.md </A> |<br>

<a href="https://towardsdatascience.com/do-we-need-deep-graph-neural-networks-be62d3ec5c59"> –¢–æ–≤–∞—Ä–∏—â –ë—Ä–æ–Ω—à—Ç–µ–π–Ω</A> |<br>
This year, deep learning on graphs was crowned among the hottest topics in machine learning.<br>

Training deep graph neural networks is hard. Besides the standard plights observed in deep neural architectures such as vanishing gradients in back-propagation and overfitting due to a large number of parameters, there are a few problems specific to graphs. One of them is over-smoothing, the phenomenon of the node features tending to converge to the same vector and become nearly indistinguishable as the result of applying multiple graph convolutional layers [1].<br>

This behaviour was first observed in GCN models [2,3], which act similarly to low-pass filters.<br>

Another phenomenon is a bottleneck, resulting in ‚Äúover-squashing‚Äù of information from exponentially many neighbours into fixed-size vectors [4].<br>


problem of depth in graph neural networks, in hope to achieve better performance and perhaps avoid embarrassment in using the term ‚Äúdeep learning‚Äù when referring to graph neural networks with just two layers. <br>


Typical approaches can be split into two families. First, regularisation techniques such as edge-wise dropout (DropEdge) [5], pairwise distance normalisation between node features (PairNorm) [6], or node-wise mean and variance normalisation (NodeNorm) [7]. Second, architectural changes including various types of residual connection such as jumping knowledge [8] or affine residual connection [9]. <br>

the use of deep architectures often results in decreased performance. <br>


<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>


<p>
<hr>
<hr>
<a name="COURSERA_COURSERA">
<div id="COURSERA_COURSERA">
<p> COURSERA_COURSERA:<br>

<A HREF="https://www.coursera.org/learn/deep-neural-networks-with-pytorch?recoOrder=5&utm_medium=email&utm_source=recommendations&utm_campaign=O66HoFeGEeuYkZ98AOleHQ"> 
COURSERA: Deep Neural Networks with PyTorch (by IBM)</A> |<br>
<A HREF="https://www.coursera.org/specializations/practical-data-science-matlab?recoOrder=6&utm_medium=email&utm_source=recommendations&utm_campaign=O66HoFeGEeuYkZ98AOleHQ"> Practical Data Science with MATLAB Specialization (by MathWorks)</A> |<br>
<A HREF=""> </A> |<br>
<A HREF="https://atcold.github.io/pytorch-Deep-Learning/"> DEEP LEARNING
DS-GA 1008 ¬∑ SPRING 2020 ¬∑ NYU CENTER FOR DATA SCIENCE</A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>


<hr>
<hr>
<a name="MoneyMoneyMoney">
<div id="MoneyMoneyMoney">
<p> MoneyMoneyMoney:<br>
<a href="https://pnoiman.github.io/etc/div_stocks.txt">pnoiman.github.io/etc/div_stocks.txt</a> | <br>
<a href="https://stackoverflow.com/questions/40139537/scrape-yahoo-finance-financial-ratios">scrape-yahoo-finance-financial-ratios</a>|<br>
<a href="https://towardsdatascience.com/farewell-rnns-welcome-tcns-dd76674707c8">TEMPORAL CONV NETS FOR STOCK TREND PREDICTIONS: farewell RNN, welcome TCN </a>|<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<hr>
<hr>
<a name="BOOKS_BOOKS">
<div id="BOOKS_BOOKS">
<p> BOOKS_BOOKS:<br>
<a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/Index.ipynb"> Python Data Science Handbook by Jake VanderPlas ON COLAB !!!! </A> |<br>
<a href="https://sites.ualberta.ca/~szepesva/books.html"> Algorithms for Reinforcement Learning (2010) + OTHER BOOKS by Csaba Szepesv√°ri</A> |<br>
<a href="http://incompleteideas.net/book/the-book-2nd.html"> Reinforcement Learning: An Introduction by Richard S. Sutton and Andrew G. Barto</A> |<br>
<a href="http://read.pudn.com/downloads666/ebook/2699449/Understanding_LTE_with_MATLAB.pdf"> Understanding_LTE_with_MATLAB.pdf</A> |<br>
<a href="http://search.pudn.com/"> http://search.pudn.com/ </A> |<br>
<a href="http://read.pudn.com/"> http://read.pudn.com/ </A> |<br>
<a href="http://search.pudn.com/DownloadEN"> http://search.pudn.com/DownloadEN</A> |<br>
<a href="http://en.bookfi.net"> http://en.bookfi.net </A> |<br>

<a href="http://libgen.gs"> Intro to GNN </A> |<br>

<a href="https://www.cs.mcgill.ca/~wlh/grl_book/"> Graph Representation Learning Book by William L Hamilton</A> |<br>
<a href="https://cse.msu.edu/~mayao4/dlg_book/"> Deep Learning on Graphs by Yao Ma et al</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

<hr>
<hr>
<a name="PyVisualize_Clustering">
<div id="PyVisualize_Clustering">
<p> PyVisualize_Clustering:<br>
<a href="https://lauren-mccarthy.com/p5-js">https://lauren-mccarthy.com/p5-js </A> |<br>
<a href="https://p5js.org/get-started/">https://p5js.org/get-started/ </A> |<br>
<a href="https://kylemcdonald.net/"> Kyle Mc Donald's site </A> |<br>
<a href="https://github.com/kylemcdonald/"> https://github.com/kylemcdonald/</A> |<br>
<a href="https://informationisbeautiful.net/visualizations/covid-19-coronavirus-infographic-datapack/"> https://informationisbeautiful.net/visualizations/covid-19-coronavirus-infographic-datapack/</A> |<br>
<a href="http://rokotyan.com/dataviz/"> http://rokotyan.com/dataviz/</A> |<br>
<a href="https://interacta.io/about/"> https://interacta.io/about/</A> |<br>
<a href=""> </A> |<br>
<a href="https://github.com/lmcinnes/subreddit_mapping/blob/master/Subreddit%20Mapping%20and%20Analysis.ipynb"> Building and Exploring a Map of Reddit with Python</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://towardsdatascience.com/how-to-use-ggplot2-in-python-74ab8adec129"> plotnine: ggplot2-like in Python </A> |<br>
<a href="https://www.youtube.com/watch?v=t9Ed5QyO7qY"> YT: python-folium </A> |<br>
<a href="https://github.com/ftk1000/python-folium"> Git: python-folium </A> |<br>
<a href="https://gist.github.com/scrapehero">https://gist.github.com/scrapehero </A> |<br>
<a href="https://machinelearningmastery.com/seaborn-data-visualization-for-machine-learning/"> 
2020.08: Jason Brownlee: How to use Seaborn Data Visualization for Machine Learning</A> |<br>
<a href="https://colab.research.google.com/github/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.00-Introduction-To-Matplotlib.ipynb#scrollTo=TZGGO5cuCvXL"> 
<a href="https://compsciclub.ru/courses/datavisualization/2016-autumn/classes/"> –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö, –æ—Å–µ–Ω—å 2016 </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

COLAB Visualization with Matplotlib</A> |<br>
=========> CLUSTERING: <br>
<a href="https://scikit-learn.org/stable/auto_examples/neighbors/plot_nca_dim_reduction.html#sphx-glr-auto-examples-neighbors-plot-nca-dim-reduction-py">
Dimensionality Reduction with Neighborhood Components Analysis (as well as PCA and LDA) </A> |<br>
<a href="https://scikit-learn.org/stable/auto_examples/cluster/plot_digits_linkage.html#sphx-glr-auto-examples-cluster-plot-digits-linkage-py"> Agglomerative Clustering on a 2D embedding </A> |<br>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html"> sklearn.manifold.SpectralEmbedding [Spectral embedding for non-linear dimensionality reduction.]</A> |<br>
<a href="https://scikit-learn.org/stable/modules/manifold.html#spectral-embedding"> 2.2.6. Spectral Embedding (GUIDE) </A> |<br>
<a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html#sklearn.manifold.TSNE"> sklearn.manifold.TSNE [t-distributed Stochastic Neighbor Embedding] </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

 
<hr>
<hr>
<a name="AB_Testing_AB_Testing">
<div id="AB_Testing_AB_Testing">
<p> AB_Testing_AB_Testing:<br>
<a href="https://www.youtube.com/watch?v=KvIJ8FCJzr4"> 2020: YANDEX –ê–Ω–∞—Ç–æ–ª–∏–π –ö–∞—Ä–ø–æ–≤: 001. –ú–µ—Ç–æ–¥—ã —Å–æ–∫—Ä–∞—â–µ–Ω–∏—è –¥–∏—Å–ø–µ—Ä—Å–∏–∏, –∏ –∑–∞—á–µ–º —ç—Ç–æ –Ω—É–∂–Ω–æ</A> |<br>
<a href="https://medium.com/convoy-tech/the-power-of-bayesian-a-b-testing-f859d2219d5"> 2018: Michael Frasco: 
The Power of Bayesian A/B Testing </A> |<br>
<a href="https://cdn2.hubspot.net/hubfs/310840/VWO_SmartStats_technical_whitepaper.pdf"> 2015: Chris Stucchio: Bayesian A/B Testing at VWO </A> |<br>
<a href="https://www.chrisstucchio.com/blog/index.html">Chris Stucchio's BLOG </A> |<br>
<a href="https://towardsdatascience.com/exploring-bayesian-a-b-testing-with-simulations-7500b4fc55bc"> 2019: Blake Arnold: Bayesian A/B testing ‚Äî a practical exploration with simulations </A> |<br>
<a href="https://towardsdatascience.com/a-b-testing-with-chi-squared-test-to-maximize-conversions-and-ctrs-6599271a2c31">
2020: Terence S: A/B Testing with Chi-Squared Test to Maximize Conversions and CTRs
A Data Science Project Walkthrough for Aspiring Experimentalists  </A> |<br>
<a href="https://www.kaggle.com/zhangluyuan/ab-testing"> AB-testing data set from KAGGLE</A> |<br>
<a href="https://www.evanmiller.org/ab-testing/chi-squared.html"> Evan's Awesome A/B Tools </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<hr>
<hr>
<a name="JOBS_JOBS">
<div id="JOBS_JOBS">
<p> JOBS_JOBS:<br>
<a href="https://www.edureka.co/blog/data-science-interview-questions/">data-science-interview-questions </a>|<br>
<a href="https://www.edureka.co/blog/google-data-science-interview-questions/"> google-data-science-interview-questions </A> |<br>
<a href="http://allendowney.blogspot.com/2016/09/bayess-theorem-is-not-optional.html"> 2016.09 bayess-theorem-is-not-optional</A> |<br>
<a href=""> </A> |<br>
<a href="https://www.youtube.com/watch?v=NM91QI2uUqI&feature=youtu.be"> Solving Facebook's Tricky Raining In Seattle Interview Question </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<hr>
<hr>
<a name="PySpark_PySpark">
<div id="PySpark_PySpark">
<p> PySpark_PySpark:<br>
<a href="https://towardsdatascience.com/distributed-deep-learning-pipelines-with-pyspark-and-keras-a3a1c22b9239">2019: Andre Violante: Distributed Deep Learning Pipelines with PySpark and Keras </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<hr>
<hr>
<a name="HIVE_HIVE">
<div id="HIVE_HIVE">
<p> HIVE_HIVE:<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<hr>
<hr>
<a name="PyTorch_PyTorch">
<div id="PyTorch_PyTorch">
<p> PyTorch_PyTorch:<br>
<a href="https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html"> PyTorch Geometric : Data Handling by Graphs </A> |<br>
<a href="https://pytorch.org/tutorials/"><https://pytorch.org/tutorials/>|<br>
<a href="https://github.com/udacity/deep-learning-v2-pytorch"><https://github.com/udacity/deep-learning-v2-pytorch>|<br>
<a href="https://bitly.com/60minblitz"> PyTorch Modules</A> |<br>
<a href="https://github.com/PyTorchLightning/pytorch-lightningnew-project.html">Lightning in 2 steps ‚Äî PyTorch Lightning 1.0.8 </A> |<br>
<a href="https://www.youtube.com/watch?v=0HQCK_l-njI&feature=youtu.be&ab_channel=PyTorchLightning"> PyTorchLightning Usage by  Kaggle Grandmaster from NVIDIA </A> |<br>

<a href="https://habr.com/ru/post/540312/"> HABR: –°–æ–±–∏—Ä–∞–µ–º –Ω–µ–π—Ä–æ—Å–µ—Ç–∏. –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –∂–∏–≤–æ—Ç–Ω—ã—Ö –∏–∑ –º—É–ª—å—Ç—Ñ–∏–ª—å–º–æ–≤. –ë–µ–∑ –¥–∞–Ω–Ω—ã—Ö –∏ –∑–∞ 5 –º–∏–Ω—É—Ç. CLIP: –û–±—É—á–µ–Ω–∏–µ –±–µ–∑ –û–±—É—á–µ–Ω–∏—è + –∫–æ–¥</A> |<br>

<hr>
<hr>
<a name="Simulation_RL_ReinforcementLearning">
<div id="Simulation_RL_ReinforcementLearning">
<p> Simulation_RL_ReinforcementLearning:<br>
<a href="https://storage.googleapis.com/lds-media/documents/Reinforcement-Learning-Animation.gif">Reinforcement-Learning-Animation.gif </A> |<br>

=====> QL:<br>
<a href="https://towardsdatascience.com/simple-reinforcement-learning-q-learning-fcddc4b6fe56">
2019: Andre Violante: Simple Reinforcement Learning: Q-learning </A> |<br>
<a href="https://www.learndatasci.com/tutorials/reinforcement-q-learning-scratch-python-openai-gym/"> Reinforcement Q-Learning from Scratch in Python with OpenAI Gym</A> |<br>
<a href="https://arxiv.org/pdf/1811.04122.pdf">2018: Spieker et al, Reinforcement Learning for Automatic Test Case Prioritization and Selection in Continuous Integration</A> |<br>
<a href="https://web.engr.oregonstate.edu/~erwig/papers/LightweightAutomatedTesting_ISSRE12.pdf"> Alex Groce et al, Lightweight Automated Testing with Adaptation-Based Programming</A> |<br>
<a href="https://towardsdatascience.com/reinforcement-learning-101-e24b50e1d292"> reinforcement-learning-101 </A> |<br>
<a href=""> </A> |<br>

<p>
RL:<br>
<a href="https://github.com/yandexdataschool/Practical_RL">https://github.com/yandexdataschool/Practical_RL</a> |<br>


<a href=""> </A> |<br>
<a href="https://www.cs.ubc.ca/~poole/demos/"> https://www.cs.ubc.ca/~poole/demos/ </A> |<br>
<a href="https://www.cs.ubc.ca/~poole/demos/mdp/"> https://www.cs.ubc.ca/~poole/demos/mdp/ </A> |<br>
<a href="https://www.cs.ubc.ca/~poole/aibook/2e/html/ArtInt2e.html"> Artificial Intelligence: Foundations of Computational Agents,  2nd Edition, by 
David L. Poole and   Alan K. Mackworth</A> |<br>
<a href=""> </A> |<br>
<a href="https://github.com/yandexdataschool/Practical_RL/tree/spring20/week04_approx_rl"> YDS RL Spring 2020: Practical_RL/week04_approx_rl/</A> |<br>
<a href="https://yadi.sk/i/Gd9yWV1dpuB7BQ"> YDS L04: Deep reinforcement learning ‚Äì –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –§—Ä–∏—Ü–ª–µ—Ä –æ—Ç 15.03.19.mp4 </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
====================> David Silver<br>
<a href="https://youtu.be/uPUEq8d73JI"> 2020: David Silver: AlphaGo, AlphaZero, and Deep Reinforcement Learning - Lex Fridman Podcast #86 (start from 4:09 - First program) </A> |<br>
    - DL Representational ability w/o search was not surprising 
	- DL Learning ability was surprising: perhaps search over high dimensional space is not stop easily at local minima<br>
    - Knowledge acquisition bottleneck: More knowledge makes system more brittle<br>
    - <br>
	
<a href="https://www.davidsilver.uk/teaching/"> David Silver, UCL Course on RL</A> |<br>
<a href="http://www.incompleteideas.net/book/RLbook2018trimmed.pdf"> BOOK: RL An intro, by R.S. Sutton and A.G. Barto</A> |<br>

<a href="https://sites.google.com/view/deep-rl-bootcamp/lectures"> 2017 deep-rl-bootcamp/lectures </A> |<br>
<a href="https://www.youtube.com/watch?v=qaMdN6LS9rA">Deep RL Bootcamp Lecture 1 </A> |<br>
<a href="https://www.youtube.com/watch?v=S_gwYj1Q-44&feature=youtu.be"> Deep RL Bootcamp Lecture 4A: Policy Gradients </A> |
<br>
<a href="https://www.youtube.com/watch?v=tqrcjHuNdmQ"> 2017.08 Andrej Karpathy (Tesla): Deep RL Bootcamp Lecture 4B Policy Gradients Revisited </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://blog.paperspace.com/intro-to-optimization-momentum-rmsprop-adam/"> optimization in deep learning: Momentum, RMSProp and Adam </A> |<br>
<a href="https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net"> tutorial-ridge-lasso-elastic-net </A> |<br>
<a href="https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/"> ridge-lasso-regression-python-complete-tutorial </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>













<hr>
<hr>
<a name="GRAPH_NETWORKS">
<div id="GRAPH_NETWORKS">
<p> GRAPH_NETWORKS:<br>
<a href="https://colab.research.google.com/github/ftk1000/graph_nets/blob/master/GCN/GCN_Blog%2BCode.ipynb"> graph_nets/blob/master/GCN/GCN_Blog%2BCode.ipynb IN COLAB</A> |<br>
<a href="https://www.notion.so/Machine-Learning-with-Graphs-12fafe3224e1483eb435a16aa990e1a1"> Links on ML with Graphs </A> |<br>
<a href="https://www.youtube.com/playlist?list=PLYeFZ_T6PUrLmS3dE-8iYv9_tlseBvhxx"> SBERLOGA SberGraph Presenations </A> |<br>
<a href="https://www.notion.so/yads/Sberloga-with-Graphs-12fafe3224e1483eb435a16aa990e1a1"> SberGraph:
–¥–æ–±–∞–≤–∏–ª –≤—Å–µ –≤–∏–¥–µ–æ –∏ —Å–ª–∞–π–¥—ã </A> |<br>
<a href="https://www.notion.so/yads/Sberloga-Data-Club-5c39a1a41f9e40798e0dbc567e332522"> 
SberData: —Å—é–¥–∞ –¥–æ–±–∞–≤–∏–ª –∏–Ω—Ñ—É –ø–æ –º–µ—Ä–æ–ø—Ä–∏—è—Ç–∏—è–º</A> |<br>


<p> =====> Open Vaccine Kaggle Comp:<p>
<a href="https://www.kaggle.com/jameschapman19/openvaccine-gcn"> 
2020.09: OpenVaccine - GCN: Graph Neural Network With Pytorch Geometric (Kaggle jameschapman19)</A> |<br>
<a href="https://www.kaggle.com/zurman/openvaccine-gcn/edit">My copy:
 OpenVaccine - GCN: Graph Neural Network With Pytorch Geometric (Kaggle jameschapman19) </A> |<br>

<a href="https://www.kaggle.com/symyksr/openvaccine-deepergcn"> 
2020.09: Mickey: OpenVaccine - DeeperGCN
</A> |<br>
<a href="https://www.kaggle.com/c/champs-scalar-coupling/discussion/106304"> 
2019: Andrew Lukyanenko: This is my first gold medal!!! Experiencing emotional overflow!</A> |<br>
<a href=""> </A> |<br>


==========> HOLOGRAPHIC EMBEDDING:<p>
<a href="https://github.com/mnick/holographic-embeddings"> Git Page w/ code </A> |<br>
<a href="https://colab.research.google.com/github/HybridNLP2018/tutorial/blob/master/02_knowledge_graph_embeddings.ipynb#scrollTo=-xScJEUjoM3C"> COLAB CODE </A> |<br>
<a href="https://colab.research.google.com/github/HybridNLP2018/tutorial/blob/master/02_knowledge_graph_embeddings.ipynb#scrollTo=HbemiIAs4VCC"> COLAB CODE </A> |<br>
<a href=""> </A> |<br>

===========> Michael Bronstein:<p>
<a href="https://towardsdatascience.com/manifold-learning-2-99a25eeb677d"> 2020:
Michael Bronstein: 
Latent graph neural networks: Manifold learning 2.0? </A> |<br>
<a href="https://towardsdatascience.com/@michael.bronstein"> https://towardsdatascience.com/@michael.bronstein</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

===========> OTHER STUFF:<p>
<a href="https://www.youtube.com/watch?v=2Bw5f4vYL98"> 2 minute paers: How Well Can an AI Learn Physics?</A> |<br>
<a href="https://arxiv.org/pdf/2002.09405.pdf"> 2020: Learning to Simulate Complex Physics with Graph Networks by Alvaro Sanchez-Gonzalez et al </A> |<br>
<a href=""> </A> |<br>
<a href="https://github.com/ftk1000/w2v-/blob/master/mikolov_papers.md"> MIKOLOV_PAPERS.MD</A> |<br>
<a href="https://github.com/ftk1000/w2v-/blob/master/graph2vec.md"> GRAPH-2-VEC.MD</A> |<br>

<a href="http://bit.ly/2otUSpH"> http://bit.ly/2otUSpH </A> |<br>
<a href=""> </A> |<br>





<hr>
<hr>
<a name="XGBoost_XGBoost">
<div id="XGBoost_XGBoost">
<p> XGBoost_XGBoost:<br>
<a href="https://www.youtube.com/watch?v=OtD8wVaFm6E"> YT: XGBoost part 1 - regression</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>





<hr>
<hr>
<a name="OPTIMIZATION_SEMINAR">
<div id="OPTIMIZATION_SEMINAR">
<p> OPTIMIZATION_SEMINAR:<br>
<a href="https://www.youtube.com/user/PreMoLab/featured">ALL VIDEOS: https://www.youtube.com/user/PreMoLab/featured </A> |<br>
<a href="https://www.youtube.com/watch?v=j8hKc7N4BXo"> VIDEO: –ë–æ—Ä–∏—Å –¢–µ–æ–¥–æ—Ä–æ–≤–∏—á –ü–æ–ª—è–∫ "–í—ã–±–æ—Ä –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –∑–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏".</A> |<br>
<a href="http://www.mathnet.ru:8080/PresentFiles/27192/seminar.pdf"> SLIDES: –ë–æ—Ä–∏—Å –¢–µ–æ–¥–æ—Ä–æ–≤–∏—á –ü–æ–ª—è–∫ "–í—ã–±–æ—Ä –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏ –≤ —Å–∏—Å—Ç–µ–º–∞—Ö —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –∫–∞–∫ –∑–∞–¥–∞—á–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏"</A> |<br>
<a href="https://www.youtube.com/watch?v=tX_MeIbfEmw"> VIDEO: –ö.–í. –í–æ—Ä–æ–Ω—Ü–æ–≤ "–û–±–∑–æ—Ä –ø–æ—Å—Ç–∞–Ω–æ–≤–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è" </A> |<br>
<a href="http://www.mathnet.ru:8080/PresentFiles/27231/voron2020_06_03_opt.pdf"> SLIDES: –ö.–í. –í–æ—Ä–æ–Ω—Ü–æ–≤ "–û–±–∑–æ—Ä –ø–æ—Å—Ç–∞–Ω–æ–≤–æ–∫ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–æ–Ω–Ω—ã—Ö –∑–∞–¥–∞—á –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è"</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<hr>
<hr>
<a name="WEBAPPWEBB">
<div id="WEBAPPWEBB">
<p> WEBAPPWEBB:<br>
==> Krish Naik:<br>
<a href="https://youtu.be/UbCWoMf80PY"> 
Deploy Machine Learning Model using Flask </A> |<br>
<a href="https://www.youtube.com/watch?v=mrExsjcvF4o">2019: Tutorial 2- Deployment of ML models in Heroku using FLASK </A> |<br>
<a href="https://youtu.be/mu-R0dQ3-Qo"> Integrating a Machine Learning Model into a Web app </A> |<br>
<a href="https://youtu.be/__ByCy0PKKI"> Machine Learning App Examples </A> |<br>
<a href="https://colab.research.google.com/drive/1m5aLHPnwIhVX8zgMvZUtK4iG9xSaMbk8"> Colab presentation for ML App Examples</A> |<br>
<a href="https://youtu.be/FDUfaYsFQrc"> Machine Learning magic for your web application with TensorFlow.js  </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<hr>
<hr>
<a name="COLAB_COLAB">
<div id="COLAB_COLAB">
<p> COLAB_COLAB:<br>
<a href="https://youtu.be/tGw-ZACouik"> Youtube: Kaggle data in colab  </A> |<br>
<a href="https://www.youtube.com/playlist?list=PLdxQ7SoCLQANQ9fQcJ0wnnTzkFsJHlWEj&feature=share">Ahlad Kumar: DL with Colab</a>|<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<hr>
<hr>
<a name="NODEJS_NODEJS">
<div id="NODEJS_NODEJS">
<p> NODEJS_NODEJS:<br>

<a href="https://www.youtube.com/watch?v=fBNz5xF-Kx4"> 2019.02: Node.js crash course  </a> |<br>
<a href="https://www.youtube.com/watch?v=U8XF6AFGqlc&feature=youtu.be"> 2016.08: Node.js Tutorial For Absolute Beginners </a> |<br>
====> Node.js <br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


 



 
 
<hr>
<hr>
<a name="Transformers/BERT">
<div id="Transformers/BERT">
<p> Transformers/BERT:<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://blog.exxactcorp.com/what-can-you-do-with-the-openai-gpt-3-language-model/"> 
2020: Geoffrey Hinton: Exploring GPT-3: A New Breakthrough in Language Generation</A> |<br>
<a href="https://arxiv.org/abs/1810.04805"> 
"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding", by
Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova. 2019.05
 </A> |<br>
<a href="http://mccormickml.com/2019/11/11/bert-research-ep-1-key-concepts-and-sources/">
Chris McCormick blog post on BERT </A> |<br>
<a href="https://www.youtube.com/channel/UCoRX98PLOsaN8PtekB9kWrw/videos"> ChrisMcCormickAI Videos </A> |<br>

<a href="https://www.youtube.com/watch?v=FKlPCK1uFrc&feature=youtu.be"> Chris McCormick video on BERT Ep1 </A> |<br>
<a href="https://www.youtube.com/watch?v=zJW57aCBCTk"> Chris McCormick video on BERT Ep2</A> |<br>
<a href="https://www.youtube.com/watch?v=x66kkDnbzi4"> Chris McCormick video on BERT Ep3 p1</A> |<br>
<a href="https://www.youtube.com/watch?v=Hnvb9b7a_Ps"> Chris McCormick video on BERT Ep3 p2</A> |<br>
<a href="https://www.youtube.com/watch?v=_eSGWNqKeeY"> Chris McCormick video on BERT Document Classification </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://colab.research.google.com/github/ftk1000/BERT_demos/blob/master/BERT_Fine_Tuning_Sentence_Classification.ipynb#scrollTo=VRCMRWpAZ4DM"> ftk copy of sentence classification with BERT on COLAB </A> |<br>
<a href=""> </A> |<br>
<a href="https://youtu.be/B_P0ZIXspOU">  Abhishek Thakur  "BERT on Steroids: Fine-tuning BERT for a dataset using PyTorch and Google Cloud TPUs" on YouTube </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://towardsdatascience.com/bert-text-classification-in-3-lines-of-code-using-keras-264db7e7a358"> 
2019: Arun Maiya: BERT Text Classification in 3 Lines of Code Using Keras </A> |<br>
<a href=""> </A> |<br>
<a href="github.com/google-research/bert"> GitHub BERT repo github.com/google-research/bert </A> |<br>
<a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">2018: BERT on Google AI Blog </A> |<br>
<a href="https://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf">2017: "Attention Is All You Need" by Vaswani et al, 2017.06 </A> |<br>
<a href="https://nlp.seas.harvard.edu/2018/04/03/attention.html"> 2018: The Annotated Transformer (Blog Post) by Alexander Rush, Harvard NLP </A> |<br>
<a href="https://www.youtube.com/watch?v=KZ9NXYcXVBY&feature=youtu.be">2020: –ì—Ä–∏–≥–æ—Ä–∏–π –°–∞–ø—É–Ω–æ–≤ | Transformer Zoo</a>|
<a href="https://blog.inten.to/speeding-up-bert-5528e18bb4ea"> 2019: –ì—Ä–∏–≥–æ—Ä–∏–π –°–∞–ø—É–Ω–æ–≤  Speeding up BERT </A> |<br>

<br>
==> Jay Alammar‚Äôs Posts:<br>
<a href="http://jalammar.github.io/illustrated-bert/"> The Illustrated BERT, ELMo, and co. (How NLP Cracked Transfer Learning) </A> |<br>
<a href="https://jalammar.github.io/illustrated-transformer/"> The Illustrated Transformer </A> |<br>
<a href="https://jalammar.github.io/visualizing-neural-machine-translation-mechanics-of-seq2seq-models-with-attention/"> Visualizing A Neural Machine Translation Model (Mechanics of Seq2seq Models With Attention) </A> |<br>
<p>
<a href="https://www.youtube.com/watch?v=2E65LDnM2cA&list=PL1w8k37X_6L_s4ncq-swTBvKDWnRSrinI&index=3"> RNN + more by A.Ng Coursera</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://nickcdryan.com/tag/nlp/"> Nick Ryan web page </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://github.com/malteos/pytorch-bert-document-classification"> https://github.com/malteos/pytorch-bert-document-classification</A> |<br>
<a href=""> </A> |<br>

 <a href="https://arxiv.org/abs/1907.11692"> FB's RoBERTa paper </A> |<br>
<a href="https://venturebeat.com/2019/07/29/facebook-ais-roberta-improves-googles-bert-pretraining-methods/">
FB's RoBERTa improves GOOG's BERT </A> |<br>

<a href="https://towardsdatascience.com/breaking-bert-down-430461f60efb"> Breaking BERT Down
A complete breakdown of the latest milestone in NLP </A> |<br>

<p> ==> NLI:<br>
<a href="https://www.kaggle.com/c/contradictory-my-dear-watson">KAGGLE NLI: contradictory-my-dear-watson </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

======================<br>
You can download models from here: 
https://github.com/huggingface/transformers/tree/master/src/transformers <br>

For example for BERT it is located there: 
https://github.com/huggingface/transformers/blob/master/src/transformers/modeling_bert.py  <br>

An example of a link from the file modeling_bert.py : 
https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz<br>



After that you can use this models as following: 
model = BertModel.from_pretrained('THE-PATH-TO-MODEL/bert-base-uncased.tar.gz')<br>

https://huggingface.co/transformers/quickstart.html<br>


<a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html"> Open Sourcing BERT: State-of-the-Art Pre-training for Natural Language Processing</A> |<br>
<a href="https://towardsdatascience.com/bert-explained-state-of-the-art-language-model-for-nlp-f8b21a9b6270"> 2018 Rani Horev: BERT Explained: State of the art language model for NLP</A> |<br>
<br>
<a href="https://huggingface.co/transformers/quickstart.html"> huggingface.co: Transformers Quickstart</A> |<br>
<a href="https://huggingface.co/transformers/pretrained_models.html"> huggingface.co: Pre-Trained Models </A> |<br>
<a href="https://deepset.ai/german-bert"> deepset.ai: Open Sourcing German BERT </A> |<br>
<br>
<a href="https://github.com/kamalkraj/BERT-SQuAD"> https://github.com/kamalkraj/BERT-SQuAD</A> |<br>
<a href="https://web.stanford.edu/class/cs224n/reports/default/15848021.pdf"> BERT for Question Answering on SQuAD 2.0, by Zhaozhuo Xu, Yuwen Zhang </A> |<br>
<a href="https://web.stanford.edu/class/cs224n/posters/15848021.pdf"> POSTER: BERT for Question Answering on SQuAD 2.0, by Zhaozhuo Xu, Yuwen Zhang</A> |<br>
<a href=""> </A> |<br>

<a href=""> </A> |<br>
<a href="http://jalammar.github.io/illustrated-transformer/"> http://jalammar.github.io/illustrated-transformer/</A> |<br>
<a href=""> </A> | <br><br>
<a href="https://www.aclweb.org/anthology/P19-1580/"> 'Analyzing Multi-Head Self-Attention:
Specialized Heads Do the Heavy Lifting, the Rest Can Be Pruned' by
Elena Voita; David Talbot et al , 2019, 57th ACL Proceedings</A> | <br>
<a href=""> </A> |
<a href="https://towardsdatascience.com/https-medium-com-gaganmanku96-fine-tune-ernie-2-0-for-text-classification-6f32bee9bf3c"> Fine-Tune ERNIE 2.0 for Text Classification </A> |<br>
<br>
<br>
TF:
<a href="https://www.tensorflow.org/tutorials/text/transformer"> Transformer model for language understanding </A> |<br>
<a href="https://github.com/tensorflow/models/tree/master/official/transformer"> Transformer Translation Model</A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
</div>
</a>
 




<hr>
<hr>
<a name="Text_Classf">
<div id="Text_Classf">
<p> Text_Classf:
<br>
<a href="https://freecontent.manning.com/deep-learning-for-text/"> https://freecontent.manning.com/deep-learning-for-text/ </A> |<br>
<a href="https://github.com/fastai/fastai/"> https://github.com/fastai/fastai/</A> |<br>


<a href="https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3"> universal-sentence-encoder  https://tfhub.dev/google/universal-sentence-encoder-multilingual-qa/3   (pre=requisite   !pip3 install tensorflow_text>=2.0.0rc0)</A> |<br>
<a href="https://github.com/TobiasLee/Text-Classification"> https://github.com/TobiasLee/Text-Classification </A> |<br>
<a href="https://github.com/zackhy/TextClassification"> https://github.com/zackhy/TextClassification </A> |<br>
<a href="https://github.com/dongjun-Lee/rnn-text-classification-tf/tree/master/models"> https://github.com/dongjun-Lee/rnn-text-classification-tf/tree/master/models</A> |<br>
<a href="https://github.com/topics/bidirectional-lstm"> https://github.com/topics/bidirectional-lstm </A> |<br>
<a href="ttps://github.com/AlexGidiotis/Document-Classifier-LSTM"> https://github.com/AlexGidiotis/Document-Classifier-LSTM </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"> Kaggle Gensim w2v tutorials </A> |<br>
<a href="https://www.kaggle.com/tags/spacy"> KAGGLE SpaCy Tutorials </A> |<br>
<a href="https://www.kaggle.com/enerrio/scary-nlp-with-spacy-and-keras">Scary NLP with SpaCy and Keras </A> |<br>

<P> SPACY:<br> 
conda install -c conda-forge spacy<br>
python -m spacy download en_core_web_lg<br>
python -m spacy download en_core_web_sm<br>
python -m spacy download en<br>









<hr>
<hr>
<a name="PYPY">
<div id="PYPY">
<p> PYPY:
<br>
<a href="https://stackoverflow.com/questions/39081766/what-are-formatted-string-literals-in-python-3-6"> formatting in py3.6</A> |<br>
<a href="https://www.youtube.com/watch?v=cwAors_xDA4"> –ß–µ–º —Ç–∞–∫ –∫—Ä—É—Ç Python ‚Äî —Ä–µ–∞–ª—å–Ω—ã–π –ø—Ä–∏–º–µ—Ä. –ü—Ä–æ–¥—É–º–∞–Ω–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ Python</A> |<br>
<a href=""> </A> |<br>
<a href="https://stackoverflow.com/questions/19560498/faster-way-to-remove-stop-words-in-python"> faster-way-to-remove-stop-words-in-python </A> |<br>
<p>
========> PROFILING PY CODE <p>
Try caching the stopwords object, as shown below. Constructing this each 
time you call the function seems to be the bottleneck.<br>
<br>
    from nltk.corpus import stopwords<br>
<br>
    cachedStopWords = stopwords.words("english")<br>
<br>
    def testFuncOld():<br>
        text = 'hello bye the the hi'<br>
        text = ' '.join([word for word in text.split() if word not in stopwords.words("english")])<br>
<br>
    def testFuncNew():<br>
        text = 'hello bye the the hi'<br>
        text = ' '.join([word for word in text.split() if word not in cachedStopWords])<br>
<br>
    if __name__ == "__main__":<br>
        for i in xrange(10000):<br>
            testFuncOld()<br>
            testFuncNew()<br>
I ran this through the profiler: python -m cProfile -s cumulative test.py. The relevant lines are posted below.<br>
<br>
nCalls Cumulative Time<br>
<br>
10000 7.723 words.py:7(testFuncOld)<br>
<br>
10000 0.140 words.py:11(testFuncNew)<br>
<br>
So, caching the stopwords instance gives a ~70x speedup.<br>
<br>

<hr>
<hr>
<a name="GPUGPU">
<div id="GPUGPU">
<p> GPUGPU:
<br>
<a href="https://medium.com/3blades-blog/an-introduction-to-gpu-programming-with-python-637818be6f7d"> An Introduction to GPU Programming With Python </A> |<br>
<a href="https://medium.com/walmartlabs/how-gpu-computing-literally-saved-me-at-work-fc1dc70f48b6"> How GPU Computing literally saved me at work?</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<hr>
<hr>
<a name="STANFORD_NLP">
<div id="STANFORD_NLP">
<p> STANFORD_NLP:
<br>
<a href="https://github.com/yannick-c/expander/"> Stanford Core NLP models to expand common contractions </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
</div>
</a>

<hr>
<hr>
<a name="NLU_SUMMARY">
<div id="NLU_SUMMARY">
<p> NLU_SUMMARY:
<br>
<a href="https://www.newscientist.com/article/2220438-deepmind-ai-beats-humans-at-deciphering-damaged-ancient-greek-tablets/">DeepMind AI beats humans at deciphering damaged ancient Greek tablets </a> |<br>
<a href="https://arxiv.org/abs/1910.06262"> Restoring ancient text using deep learning: a case study on Greek epigraphy, by
Yannis Assael et al</a> |<br>
<a href=""> </a> |<br>

<a href="https://arxiv.org/pdf/1905.03197.pdf"> 2019.10 Li Dong et al: Unified Language Model Pre-training for
Natural Language Understanding and Generation </a> |<br>
<a href="https://github.com/microsoft/unilm"> 2019 UniLM - Unified Language Model Pre-training : https://github.com/microsoft/unilm</a> |<br>
<a href=""> </a> |<br>
</div>
</a>





<hr>
<hr>
<a name="KERAS_KERAS">
<div id="KERAS_KERAS">
<p> KERAS_KERAS:
<br>
<a href="https://github.com/keras-team/keras-applications/tree/master/keras_applications"> keras-team: keras_applications</a>|
<a href="https://www.tensorflow.org/tutorials/text/text_classification_rnn"> Text classification with an RNN </a> |
<a href="https://www.tensorflow.org/tutorials/text/transformer">Transformer model for language understanding </a> |
<a href="https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2/notebook"> www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2/notebook </a> |
<a href=""> </a> |
</div>
</a>





<hr>
<hr>
<a name="DOCS_BOUND_DETECTION__COMP">
<div id="DOCS_BOUND_DETECTION__COMP">
<p> DOCS_BOUND_DETECTION__COMP:
<br>
<a href="https://github.com/topics/sentence-boundary-detection"> sentence-boundary-detection</a> |
<a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3002123/"> Evaluation of a Method to Identify and Categorize Section Headers in Clinical Documents, by Joshua C. Denny, et al</a> |
<a href="http://ceur-ws.org/Vol-710/paper23.pdf">A Machine Learning Approach to Identifying Sections in Legal Briefs, Scott Vanderbeck et al </a> |
<a href="https://link.springer.com/chapter/10.1007/978-3-540-30586-6_92"> A Paragraph Boundary Detection System, Dmitriy Genzel </a> |

<a href="https://stackoverflow.com/questions/3237624/how-to-use-nlp-to-separate-a-unstructured-text-content-into-distinct-paragraphs"> How to use NLP to separate a unstructured text content into distinct paragraphs? </a> |
<a href="https://stackoverflow.com/questions/8897593/how-to-compute-the-similarity-between-two-text-documents?rq=1"> 
How to compute the similarity between two text documents? </a> |
<a href="https://arxiv.org/abs/1803.11175">  Universal Sentence Encoder, Daniel Cer et al</a> |
<a href="https://github.com/yannick-c/expander"> expand common contractions </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
</div>
</a>






<hr>
<hr>
<a name="DATASETS_DATASETS">
<div id="DATASETS_DATASETS">
<p> DATASETS_DATASETS:
<br>
<a href="https://www.kaggle.com/docs/datasets?utm_medium=youtube&utm_source=channel&utm_campaign=yt-datast"> [Kaggle Data Sets]</a> |<br>
<a href="https://www.kaggle.com/search?q=summarization"> [Kaggles data sets for summarization]</a> |<br>
<a href="https://www.kaggle.com/pariza/bbc-news-summary"> [BBC News Summary: Extractive Summarization of BBC News Articles (4450 files, ~7MB)] </a> |<br>
<a href="https://www.kaggle.com/goodletterhead/blog-summarization"> [Blog summarization: Human and Algoriithm Summarized data - very small set]</a> |<br>
<br>

<a href="https://breakend.github.io/DialogDatasets/"> A Survey of Available Corpora for Building Data-Driven Dialogue Systems</a> |<br>
<a href="https://ai.google/tools/datasets/coached-conversational-preference-elicitation"> coached-conversational-preference-elicitation </a> |<br>
<a href="https://www.kaggle.com/Cornell-University/movie-dialog-corpus"> https://www.kaggle.com/Cornell-University/movie-dialog-corpus</a> |<br>
<a href="https://www.kaggle.com/tovarischsukhov/southparklines"> https://www.kaggle.com/tovarischsukhov/southparklines</a> |<br>
<br>
<a href="https://storage.googleapis.com/dialog-data-corpus/CCPE-M-2019/landing_page.html"> Accessing the CCPE-M dataset </a> |<br>
<a href="https://storage.googleapis.com/dialog-data-corpus/CCPE-M-2019/data.json"> CCPE-M-2019/data.json </a> |
<a href="http://ufal.mff.cuni.cz/udpipe#language_models"> UDPipe </a> |<br>
<a href="http://ufal.mff.cuni.cz/~straka/papers/2017-conll_udpipe.pdf"> Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0 with UDPipe, by Milan Straka and Jana Strakova</a> |<br>

<a href="https://github.com/ftk1000/data_files/blob/master/howto_get_data2colab.ipynb"> EXAMPLES OF HOW TO DOWNLOAD DATA SETS </a> |<br>
<a href="https://github.com/ryanleeallred/datasets"> https://github.com/ryanleeallred/datasets </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<p>
===> Appen/resources/datasets/:
<a href="https://appen.com/resources/datasets/"> LOTS OF PUCLIC DATA SETS appen.com/resources/datasets/</a> |<br>
<a href="https://appen.com/datasets/medical-sentence-summary-and-relation-extraction/"> medical-sentence-summary-and-relation-extraction </a> |<br>
<a href="https://appen.com/datasets/audio-recording-and-transcription-for-medical-scenarios/"> Medical Speech, Transcription, and Intent (English) </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>

</div>
</a>


<hr>
<hr>
<a name="SALARY_SALARY">
<div id="SALARY_SALARY">
<p> SALARY_SALARY:
<br>
<a href="https://datasciencedegree.wisconsin.edu/data-science/data-scientist-salary/"> data-scientist-salary </A> |
<a href="https://www.youtube.com/watch?v=GnYOcJfH8bQ"> Data Scientist Salary 2019: base salary 200k+ </A> |
<a href="https://insights.dice.com/2019/08/01/google-senior-software-engineer-salary/?CMPID=EM_RE_UP_JS_AD_DA_CP_B_&utm_campaign=Advisory_DiceAdvisor_B&utm_source=Responsys&utm_medium=Email"> DICE: Google Senior Software Engineer Salary </A> |
<a href="https://www.levels.fyi/SE/Microsoft/Google/Facebook/"> Salaries: www.levels.fyi </A> |
<a href="https://www.levels.fyi/comp.html?track=Software%20Engineer"> Levels.fyi: Salaries at all companies </A> |
<a href=""> </A> |
<a href="https://www.foreignlaborcert.doleta.gov/performancedata.cfm"> US Dept of Labor: H1 scam numbers </A> |
<a href="https://www.comparably.com/companies/datarobot/salaries"> comparably.com </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |

<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
</div>
</a>


<hr>
<hr>
<a name="VAE">
<div id="VAE">
<p> VAE:<br>
<a href="https://www.youtube.com/watch?v=9zKuYvjFFS8&feature=youtu.be"> Arxiv Insights: Variational Autoencoders </A> |
<a href="https://www.youtube.com/watch?v=fcvYpzHmhvA"> CodeEmporiumL Variational Autoencoders - EXPLAINED! </A> |
<a href="https://www.youtube.com/watch?v=uaaqyVS9-rM"> Ali Ghodsi, Lec : Deep Learning, Variational Autoencoder, Oct 12 2017 [Lect 6.2] </A> |

<a href="https://arxiv.org/abs/1606.05579"> Disentangled VAE's (DeepMind 2016) </A> |
<a href="https://arxiv.org/abs/1707.08475"> Applying disentangled VAE's to RL: DARLA (DeepMind 2017) </A> |
<a href="https://arxiv.org/abs/1312.6114"> Original VAE paper (2013) </A> |
<a href=""> </A> |
<a href="http://bjlkeng.github.io/posts/semi-supervised-learning-with-variational-autoencoders/"> Semi-supervised Learning with Variational Autoencoders </a> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
</div>
</a>







<hr>
<hr>
<a name="NER_PyTextRank">
<div id="NER_PyTextRank">
<p> NER_PyTextRank:
<br>
NER:
<a href="https://nlpforhackers.io/named-entity-extraction/"> Name Entity Recognition </a> |
<a href="https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da"> Named Entity Recognition with NLTK and SpaCy</a>
<a href="https://github.com/susanli2016/NLP-with-Python/blob/master/NER_NLTK_Spacy.ipynb"> (code is here) </A> |
<a href="https://web.stanford.edu/~jurafsky/slp3/17.pdf"> Speech and Language Processing (3rd ed. draft) </A> |
<a href="https://www.kaggle.com/poonaml/text-classification-using-spacy"> Kaggle: text classif w/ SpaCy </A> |
<a href="https://www.kaggle.com/hubert0527/spacy-name-entity-recognition"> Kaggle: SpaCy NER </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<br>
pytextrank
<a href="http://web.eecs.umich.edu/~mihalcea/papers/mihalcea.emnlp04.pdf"> pytextrank paper </A> |
<a href="https://xang1234.github.io/textrank/"> Xang: pytextrank example </A> |
<a href="">  </A> |
<a href="">  </A> |
<a href="">  </A> |
<a href="">  </A> |
<a href="">  </A> |
</div>
</a>



<hr>
<hr>
<a name="NN_Theory">
<div id="NN_Theory">
<p> NN_Theory:<br>
<a href="http://neuralnetworksanddeeplearning.com/chap4.html"> Michael Nielson's book (ch4: on universality theorem) </a> |
<a href="http://www.dartmouth.edu/~gvc/Cybenko_MCSS.pdf"> Approximation by Superposition of a Sigmoid Function by G.Cybenko, 1989 </a> |
<a href="https://www.sciencedirect.com/science/article/pii/0893608089900208"> Multilayer feedforward networks are universal approximators by Kurt Hornik et al, 1989</a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
</div>
</a>


<hr>
<hr>
<a name="CLIP_NLTK">
<div id="CLIP_NLTK">
<p> CLIP_NLTK:
CLiPS (Computational Linguistics and Phycholinguistics Research Center):<br>
<a href="https://www.clips.uantwerpen.be/pages/pattern"> clips.uantwerpen.be/pages/pattern </A> |<br>
<a href="https://www.clips.uantwerpen.be/pages/pattern-en#parser"> clips.uantwerpen.be/pages/pattern-en#parser </A> |<br>
<a href="https://www.uantwerpen.be/en/research-groups/clips/research/software/demos/"> clips/research/software/demos/ </A> |<br>
<a href="https://github.com/clips/pattern"> github.com/clips/pattern </A> |<br>
<a href="https://www.nltk.org/api/nltk.tokenize.html">https://www.nltk.org/api/nltk.tokenize.html </A> |<br>
<a href="https://web.stanford.edu/~jurafsky/slp3/14.pdf"> Statistical Constituency Parsing , Ch 14, Jurafski et al</A> |<br>
<a href="http://text-processing.com/demo/"> DEMOS: Python NLTK Demos for Natural Language Text Processing </A> |<br>
<a href=""> </A> |<br>
<a href="https://www.nltk.org/book/ch08.html"> Analyzing Sentence Structure: Parsing the sentence using POS-tagging:  nltk.org-book-ch08.html</A> |<br>
<a href=""> </A> |
<a href=""> </A> |
</div>
</a>





<hr>
<hr>
<a name="HABR_HABR">
<div id="HABR_HABR">
<p> HABR_HABR:<br>
<a href="https://habr.com/ru/post/460321/"> –ì–∞–ª–µ—Ä–µ—è –ª—É—á—à–∏—Ö –±–ª–æ–∫–Ω–æ—Ç–æ–≤ –ø–æ ML –∏ Data Science </A> |<br>
<a href="https://habr.com/ru/company/sberbank/blog/418701/"> –î–µ–Ω–∏—Å –ö–∏—Ä—å—è–Ω–æ–≤: –ò–∑—É—á–∞–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–µ –ø–∞—Ä—Å–µ—Ä—ã –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞</A> |<br>
<a href="https://opendatascience.slack.com/archives/C043ZEF6K/p1578326816292700">https://opendatascience.slack.com/archives/C043ZEF6K/p1578326816292700 </A> |<br>
<a href="https://github.com/DenisVorotyntsev/AutoSeries">https://github.com/DenisVorotyntsev/AutoSeries </A> |<br>

</div>
</a>


<hr>
<hr>
<a name="TIME_SERIES">
<div id="TIME_SERIES">
<p> TIME_SERIES:<br>
<a href="https://towardsdatascience.com/automl-for-time-series-forecasting-6caaf194d268">
Denis Vorotyntsev: AutoML for Time Series Forecasting</A> |<br>

<a href="http://www.bizkit.ru/2019/11/11/15143/"> –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Ä—è–¥–æ–≤ –Ω–µ–π—Ä–æ–Ω–Ω—ã–º–∏ —Å–µ—Ç—è–º–∏. Keras. –ß–∞—Å—Ç—å 1.</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
</div>
</a>



<hr>
<hr>
<a name="REGEX_REGEX">
<div id="REGEX_REGEX">
<p> REGEX_REGEX:<br>
<a href="https://www.regular-expressions.info/tutorial.html"> regular-expressions.info/tutorial.html</A> |<br>
<a href="https://www.regular-expressions.info/hipowls.html"> www.regular-expressions.info</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
</div>
</a>





<hr>
<hr>
<a name="TopicModeling">
<div id="TopicModeling">
<p> Topic Modeling / Chat Mining :<br>
<a href="https://eight2late.wordpress.com/2015/09/29/a-gentle-introduction-to-topic-modeling-using-r/"> G Intro Topic Modeling in R </a> |
<a href="https://eight2late.wordpress.com/2015/05/27/a-gentle-introduction-to-text-mining-using-r/"> G Intro Text Mining R</A> | 
<a href="http://tidytextmining.com/topicmodeling.html"> Topic modeling (tidytextmining.com)</a> |

<a href="http://www.vladsandulescu.com/topic-prediction-lda-user-reviews/"> topic-prediction-lda-user-reviews</a> |
<a href="https://www.yelp.com/html/pdf/YelpDatasetChallengeWinner_ImprovingRestaurants.pdf"> Improving Restaurants by Extracting Subtopics from Yelp Reviews by J.Huang et al</a> |
<a href="https://github.com/vladsandulescu/topics"> https://github.com/vladsandulescu/topics </a> |
<a href="https://www.kaggle.com/morrisb/compare-lda-topic-modeling-in-sklearn-and-gensim"> kaggle: LDA example </a> |
<a href="http://stats.stackexchange.com/questions/9315/topic-prediction-using-latent-dirichlet-allocation"> Stackexchange: topic pred using LDA</a> |
<a href="https://www.quora.com/Could-latent-Dirichlet-allocation-solved-by-Gibbs-sampling-versus-variational-EM-yield-different-results"> Quora: Gibbs sampling vs VEM in LDA</a> |
<a href="https://www.quora.com/When-should-I-prefer-variational-inference-over-MCMC-for-Bayesian-analysis"> Quora: variational-inference-over-MCMC-for-Bayesian-analysis </A> |
<a href="https://stats.stackexchange.com/questions/24441/two-r-packages-for-topic-modeling-lda-and-topicmodels"> Pros and cons of TOPICMODELING and LDA pkgs </A> | 
<a href="https://towardsdatascience.com/light-on-math-machine-learning-intuitive-guide-to-latent-dirichlet-allocation-437c81220158">
Intuitive Guide to Latent Dirichlet Allocation by 
Thushan Ganegedara
</a> |
<a href="https://www.youtube.com/watch?v=3mHy4OSyRf0&t=1058s"> Andrius Knispelis
 youtube video: LDA Topic Models </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
</div>
</a>


<hr>
<hr>
<a name="CHAT_MINING">
<div id="CHAT_MINING">
<p> CHAT_MINING: <br>
<a href="https://pdfs.semanticscholar.org/647f/924c452d7c558a82fca7fd09b922aeab9265.pdf">Chat Mining: Predicting User and Message Attributes, Cevdet Aykanat et al </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
</div>
</a>















<hr>
<hr>
<a name="TF2">
<div id="TF2">
<p> TF2.0:<br>
<a href="https://www.tensorflow.org/hub"> TensorFlow Hub is a library for reusable machine learning modules. </A> |<br>

<a href="https://www.youtube.com/watch?v=k5c-vg4rjBw"> youtube: TF 2.0 </a> |
pip install -U --pre tensorflow<br>
<a href="bit.ly/mini-dream"> 9-deep-dream-minmal.ipynb at colab </a> |
<a href="bit.ly/mini-nmt"> mini google translator </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<br>
<a href="https://www.youtube.com/watch?v=3O-5DuqKaRo"> edureka! TF2 tutorial</a> |
<a href=""> </a> |
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>

</div>
</a>

<hr>
<hr>
<a name="CODE_SERVER">
<div id="CODE_SERVER">
<p> CODE_SERVER:<br>
<a href="https://www.youtube.com/watch?v=ArygUBY0QXw&feature=youtu.be"> Abhishek Thakur Episode 1.1: Intro and building a machine learning framework </a> |<p>
<a href="https://github.com/github/gitignore"> https://github.com/github/gitignore </a> |<br>
<a href="https://github.com/github/gitignore/blob/master/Python.gitignore"> https://github.com/github/gitignore/blob/master/Python.gitignore </a> |<br>
====> CODE SERVER by CODER.COM <p>
<a href="https://github.com/cdr/code-server"> https://github.com/cdr/code-server </a> |<br>
<a href="https://github.com/cdr/code-server/releases"> https://github.com/cdr/code-server/releases</a> |<br>
<a href="https://www.youtube.com/watch?v=N5WojMutddQ"> 2019.01: Run VS Code in the browser with massive computing resources </a> |<br>
<p>
====> Code-Server on Digital Ocean<br>
<a href="https://www.youtube.com/watch?v=UeP6VQjMw8Q"> 2019.04: How To Setup VSCode In The Cloud - code-server </a> |<br>
<a href="https://cloud.digitalocean.com/projects/faf2f060-0311-4687-b24e-a32ba71ebc8b/resources?i=5af120"> Digital Ocean Droplet </a> |<br>
1. make SSH connection to 142.93.251.226 (goto <a href="file:///C:/PUTTY_TOOLS/mtputty.exe"> file:///C:/PUTTY_TOOLS/mtputty.exe</a>) <br>
2. cd to /root/vscode/code-server2.1698-vsc1.41.1-linux-x86_64 <br>
3. run "./code-server" <br>
4. from chrome access url: <a href="http://142.93.251.226:8080/">http://142.93.251.226:8080/</a> use password given by the ./code-server <p>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>


<hr>
<hr>
<a name="KAGGLE_KAGGLE">
<div id="KAGGLE_KAGGLE">
<p> KAGGLE_KAGGLE:<br>
<a href="https://www.kaggle.com/learn/computer-vision">https://www.kaggle.com/learn/computer-vision </a> |<br>
<a href="https://www.kaggle.com/c/ms-ai-sg-text-classification/data"> Simple Text Classification Challenge (www.kaggle.com/ms-ai-sg-text-classification)
This is an in-class competition for your to test your skills and apply your learnings.</a> |<br>
<a href="https://www.kaggle.com/c/contradictory-my-dear-watson/notebooks"> Contradictory, My Dear Watson
Detecting contradiction and entailment in multilingual text using TPUs </a> |<br>
<a href="https://www.kaggle.com/c/made-thousand-facial-landmarks/discussion/143089">2020: Andre Cpc: 
–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å —Å –∫–æ–ª–∞–±–æ–º —Å –ø–æ–º–æ—â—å—é –¥–∞—Ç–∞—Å–µ—Ç–∞ –∫–∞–≥–≥–ª–∞ </a> |<br>
<a href="https://www.youtube.com/watch?v=tGw-ZACouik">2018 YT: Ahlad Kumar: Deep Learning 3: Importing Kaggle's dataset in Google Colaboratory  </a> |<br>
<a href=""> </a> |<br>
<a href="https://www.kaggle.com/c/nlp-getting-started/discussion/134890">KAGGLE: Best NLP Kernels For Beginners </A> |<br>

=======> KAGGLE DATASETS<p>
<a href="https://www.kaggle.com/luishpinto/textclassificationdb">luishpinto/textclassificationdb </a> |<br>
<a href=""> </a> |<br>
<br>

<a href="https://github.com/github/gitignore"> https://github.com/github/gitignore </a> |<br>
<a href="https://github.com/github/gitignore/blob/master/Python.gitignore"> https://github.com/github/gitignore/blob/master/Python.gitignore </a> |<br>
<a href="https://www.youtube.com/watch?v=ArygUBY0QXw&feature=youtu.be"> Abhishek Thakur Episode 1.1: Intro and building a machine learning framework </a> |<p>
<a href="https://www.youtube.com/watch?v=zcqgj-Udcqs"> Abhishek Thakur Episode 1.2: Building an inference for the machine learning framework</a> |<br>
<a href="https://www.youtube.com/watch?v=VRVit0-0AXE&ab_channel=AbhishekThakur"> 2020: Pair Programming: Deep Learning Model For Drug Classification With Andrey Lukyanenko </a> |<br>


<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>

<a href="https://www.youtube.com/watch?v=G0_Yd6Gvf6U"> Quick Kaggle Update for my NLP Analyze Two Sentences Challenge </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href="https://www.youtube.com/watch?v=tSzrecBdyJc&list=PLjy4p-07OYzvyN1IzfCFRXiSXhKd0ijhh"> Kaggle: Jeff Heaton's Guide and Strategies for Top 10% (or higher) Finishes
Jeff Heaton </a> |<br>
<a href="https://www.youtube.com/watch?v=eNkayufkT04"> Jeff Heaton: Kaggle: How to Place in the Top 10% (part 1) </a> |<br>
<a href="https://www.youtube.com/watch?v=tSzrecBdyJc&list=PLjy4p-07OYzvyN1IzfCFRXiSXhKd0ijhh&index=2"> Python Toolkit that Used for Two Kaggle Top 10% Leaderboard Finishes </a> |<br>

<a href="https://www.youtube.com/watch?v=ArPaAX_PhIs&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF"> Andrew Ng ConvNets 41 lectures Nov 2017 </A> |
<a href="https://www.youtube.com/watch?v=R39tWYYKNcI&list=PLkDaE6sCZn6Gl29AoE31iwdVwSG-KnDzF&index=38"> C4W4L07 What are deep CNs learning? </A> |
<a href="https://www.kaggle.com/learn/feature-engineering"> Feature Engineering by Pavel Pleskov </A> |
<a href=""> </A> |



<p> KAGGLE-NLP-Spooky:
<a href="https://www.kaggle.com/c/spooky-author-identification/kernels"> Spooky Author Identification Kernels </A> |
<a href="https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial/notebook"> A KERNEL:  Spooky NLP and Topic Modelling tutorial </A> |
<a href="https://www.kaggle.com/baghern/a-deep-dive-into-sklearn-pipelines"> KERNEL: A Deep Dive Into Sklearn Pipelines - Good example of FeatureUnion </A> |
<a href="https://www.kaggle.com/nzw0301/simple-keras-fasttext-val-loss-0-31">Simple Keras FastText: val_loss 0.31 </A> |
<a href="https://www.kaggle.com/yevhent/keras-lstm-cnn-and-transfer-learning"> Keras LSTM, CNN and... Transfer Learning </A> |
<a href="https://www.kaggle.com/kashnitsky/vowpal-wabbit-tutorial-blazingly-fast-learning/notebook"> Vowpal Wabbit tutorial: blazingly fast learning </A> |
<a href="https://www.kaggle.com/metadist/work-like-a-pro-with-pipelines-and-feature-unions"> Work like a Pro with Pipelines and Feature Unions </A> |
<a href="https://www.kaggle.com/arthurtok/spooky-nlp-and-topic-modelling-tutorial"> NLTK + Topic Modelling with LDA and NNMF </A> |
<a href=""> </A> |
<a href="https://www.kaggle.com/maheshdadhich/creative-feature-engineering-lb-0-35"> Creative Feature Engineering (LB 0.35) - Has Animated Word Cloud </A> |
<a href=""> </A> |
<a href=""> </A> |

<p> KAGGLE-NLP-Santander Customer Transaction Prediction :
<a href="https://www.kaggle.com/c/santander-customer-transaction-prediction"> Santander Customer Transaction Prediction </A> |
<a href="https://www.kaggle.com/nawidsayed/lightgbm-and-cnn-3rd-place-solution/data"> Fork of LightGBM NN kfold ebe29a </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |


<br> KAGGLE-NLP
<a href="https://www.kaggle.com/c/santander-customer-satisfaction"> KAGGLE: Santander Customer Satisfaction </a> |
<a href="https://www.kaggle.com/zhenyufan/nlp-for-yelp-reviews/notebook?utm_medium=email"> NLP for Yelp Reviews </A> |
<a href="https://www.kaggle.com/zhenyufan/nlp-for-yelp-reviews/notebook?utm_medium=email&utm_source=intercom&utm_campaign=datanotes-2019"> NLP for Yelp Reviews </A> |<br>
<a href="https://www.kaggle.com/c/contradictory-my-dear-watson">2020.08 Competition: contradictory-my-dear-watson </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |


<a href="https://www.kaggle.com/abbahaddou/bbc-automatic-document-classification"> A KERNEL: bbc-automatic-document-classification </A> |
<a href="https://towardsdatascience.com/named-entity-recognition-with-nltk-and-spacy-8c4a7d88e7da"> Named Entity Recognition with NLTK and SpaCy </a> |
<a href="https://nlpforhackers.io/named-entity-extraction/"> Name Entity Recognition </a> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |


<br> Toxic Comment Competition:
<a href="https://www.kaggle.com/paulofelipe/logistic-regression-with-nb-features"> Paulo Felipe 0.052</a> |
<a href=""> </a> |
<a href="http://ronny.rest/blog/post_2017_08_04_glove/"> pretrained Glove word embeddings </a> |
<a href="https://worksheets.codalab.org/bundles/0x15a09c8f74f94a20bec0b68a2e6703b3/"> glove.6B.100d.txt </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |<br>
<a href=""> </a> |<br>
<a href=""> </a> |<br>


<a href=""> </a> |<br>

<a href="http://blog.kaggle.com/2017/06/15/stacking-made-easy-an-introduction-to-stacknet-by-competitions-grandmaster-marios-michailidis-kazanova/"> 
Stacking by Kazanova</a> |<br>


<a href=""> </a> |<br>


<p>
FASTTEXT_FASTTEXT:<p>
<a href=""> </a> |<br>
<a href="https://fasttext.cc/docs/en/crawl-vectors.html">
https://fasttext.cc/docs/en/crawl-vectors.html </a> |<br>

<a href="https://arxiv.org/abs/1607.04606">Piotr Bojanowski, Edouard Grave, Armand Joulin, Tomas Mikolov "Enriching Word Vectors with Subword Information" </a> |<br>

<a href="https://fasttext.cc/docs/en/pretrained-vectors.html"> FastText: wiki word vectors</a> |<br>

<a href="http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html"> 
Fasttext –µ—Å—Ç—å –∏ –≤ DeepPavlov: http://docs.deeppavlov.ai/en/master/features/pretrained_vectors.html –¢–∞–∫–∂–µ —Ç–∞–º –∂–µ –µ—Å—Ç—å BERT –∏ ELMo, –∫ —Ç–æ–º—É –∂–µ –æ–Ω–∏ –Ω–µ —Ç—Ä–µ–±—É—é—Ç —Ä–µ–∫—É—Ä—Ä–µ–Ω—Ç–Ω–æ—Å—Ç–∏ –ø–æ–≤–µ—Ä—Ö —Å–µ–±—è. –î–∞–ª—å—à–µ —Å–º–æ—Ç—Ä–µ—Ç—å –ø–æ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—è–º –∂–µ–ª–µ–∑–∞ –∏ –∫–æ–ª–µ–±–∞–Ω–∏—è–º –∫–∞—á–µ—Å—Ç–≤–∞.</a> |<br>

</div>
</a>

<hr>
<hr>

<a name="YandexDataSchool">
<div id="YandexDataSchool">
<p> YandexDataSchool: <br>

<p>

 Yandex School of Data Analysis: <a href="https://github.com/yandexdataschool">   https://github.com/yandexdataschool </a> <br>

NLP_course_master (aka 2018) <a href="https://github.com/yandexdataschool/nlp_course/tree/master"> https://github.com/yandexdataschool/nlp_course/tree/master </a> |<br>

NLP_course_2019 <a href="https://github.com/yandexdataschool/nlp_course"> https://github.com/yandexdataschool/nlp_course </a> |<br>

<a href=""> </a> |
<a href=""> </a> |




<p> 

<style>
table, th, td {
  border: 1px solid black;
  border-collapse: collapse;
  text-align: center;
}
</style>
<table style="width:50%">
  <tr> 
    <th>Week</th>
    <th>2018 YANDEX NLP</th>
    <th>2019 YANDEX NLP</th> 
    <th>2020 YANDEX NLP</th>
  </tr>
  <tr>
    <td>wk01</td>
    <td>Embedding</td>
    <td>Embedding</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk02</td>
    <td>Classification</td>
    <td>Text classification</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk03</td>
    <td>LM</td>
    <td>Language Models</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk04</td>
    <td>seq2seq</td>
    <td>Seq2seq/Attention</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk05</td>
    <td>structured</td>
    <td>Structured Learning</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk06</td>
    <td>EM</td>
    <td>Expectation-Maximization</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk07</td>
    <td>MT</td>
    <td>Machine translation</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk08</td>
    <td>MuliTask</td>
    <td>Transfer learning and Multi-task learning</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk09</td>
    <td>DA</td>
    <td>Domain Adaptation</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk10</td>
    <td>Dialogue</td>
    <td>Dialogue Systems</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk11</td>
    <td>GAN</td>
    <td>Adversarial learning & Latent Variables for NLP</td>
    <td>tbd</td>
  </tr>
  <tr>
    <td>wk12</td>
    <td>Summarization</td>
    <td>Text Summarization</td>
    <td>tbd</td>
  </tr>
</table>








<a name="YandexNLP_2018">
<div id="YandexNLP_2018">

<p> <a href="https://github.com/yandexdataschool/nlp_course/tree/master"> YandexNLP_2018 &ensp; (https://github.com/yandexdataschool/nlp_course/tree/master):</a> <br>
<p>

<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week01_embeddings"--> 
01 master/week01_embeddings </A> |<br>
&emsp; <a href="https://yadi.sk/i/Ff1jVAODd4P9ug"> Natural Language Processing ‚Äì David Talbot –æ—Ç 13.09.18.mp4 </A> |<br>
&emsp; <a href="https://yadi.sk/i/wzVA1XYKS2u6NQ"> Word Embeddings ‚Äì Elena Voita –æ—Ç 13.09.18.mp4 </A> |<br>
&emsp; <a href="https://yadi.sk/i/X5UYALfyyrwzYw"> Word Embeddings ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 13.09.18.mp4 </A> |<br>

<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week02_classification"--> 
02 master/week02_classification</A>|<br>
&emsp; <a href="https://yadi.sk/i/lIcXiNuBXNAWzg"> Text Classification ‚Äì Boris Kovarsky –æ—Ç 20.09.18.mp4 </A> |<br>
&emsp; <a href="https://yadi.sk/i/2xYHH0Fj1vb82A"> Text Classification ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 20.09.18.mp4 </A> |<br>

<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week03_lm"--> 
03 master/week03_lm </A> |<br>
&emsp; <a href="https://yadi.sk/i/8JK4dH6E9Iyy8g">Language models ‚Äì Elena Voita –æ—Ç 27.09.18.mp4 </A> |<br>
&emsp; <a href="https://yadi.sk/i/HfjTLRejnVwP7g"> N-gram language models or how to write scientific papers ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 27.09.18.mp4</A> |<br>
&emsp; <a href="https://yadi.sk/i/0x-o0B2FGiHthg"> Recurrent units in detail ‚Äì Elena Voita –æ—Ç 27.09.18.mp4</A> |<br>

<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week04_seq2seq"--> 
04 master/week04_seq2seq </A> |<br>
&emsp; <a href="https://yadi.sk/i/3y9JX6Q_0ZLqpA"> Seq2seq and Attention ‚Äì Elena Voita –æ—Ç 04.10.2018.mp4 </A> |<br>
&emsp; <a href="https://yadi.sk/i/cB5606vgbOlBog"> Seq2seq ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 04.10.2018.mp4</A> |<br>

<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week05_structured"--> 
05 master/week05_structured </A> |<br>
&emsp; <a href="https://yadi.sk/i/E3Rl6x4LtYZZ7g"> –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äì –°–µ—Ä–≥–µ–∏ÃÜ –ì—É–±–∞–Ω–æ–≤ –æ—Ç 11.10.18.mp4</A> |<br>
&emsp; <a href="https://yadi.sk/i/cIkA3WV9NPjdMg"> Sparse features & Hashing trick. ‚Äì –°–µ—Ä–≥–µ–∏ÃÜ –ì—É–±–∞–Ω–æ–≤ –æ—Ç 11.10.18.mp4 </A> |<br>

<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week06_em"--> 
06 master/week06_em </A> |<br>
&emsp; <a href="https://yadi.sk/i/4ZA6BCJBQAxy3Q"> Generative Models And Expectation Maximization ‚Äì David Talbot –æ—Ç 18.10.18.mp4</A> |<br>
&emsp; <a href="https://yadi.sk/i/uPuNKeGkULujVg"> Seminar Generative Models And Expectation Maximization ‚Äì David Talbot –æ—Ç 18.10.18.mp4</A> |<br>


<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week07_mt"-->
07 master/week07_mt </A> |<br>
&emsp; <a href="https://yadi.sk/i/pDxo20nK-UnVHg"> Machine Translation —á–∞—Å—Ç—å 1 ‚Äì David Talbot –æ—Ç 25.10.2018.mp4 </A> |<br>
&emsp; <a href="https://yadi.sk/i/k1_0T_WptcSvYw"> Machine Translation —á–∞—Å—Ç—å 2 ‚Äì David Talbot –æ—Ç 25.10.2018.mp4 </A> |<br>


<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week08_multitask"--> 
08 master/week08_multitask </A> |<br>
&emsp; <a href="https://yadi.sk/i/6Sbu_oja9h_QZw"> Transfer learning Multi-task learning in NLP - Elena Voita –æ—Ç 08.11.2018.mp4</A> |<br>
&emsp; <a href="https://yadi.sk/i/AGgnMxu_UvpOsg"> Named EntityRecognition ‚Äì–§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 08.11.18.mp4 </A> |<br>


<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week09_da"--> 
09 master/week09_da </A> |<br>
&emsp; <a href="https://yadi.sk/i/MnuyuCSAOJainw"> Domain adaptation –ª–µ–∫—Ü–∏—è - Boris Kovarsky –æ—Ç 15.11.2018.mp4 </A> |<br>
&emsp; <a href="https://yadi.sk/i/15ApU2_cTFbL1g"> Domain adaptation —Å–µ–º–∏–Ω–∞—Ä - –§–µÃà–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 15.11.2018.mp4</A> |<br>


<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week10_dialogue"--> 
10 master/week10_dialogue</A> |<br>
&emsp; <a href="https://yadi.sk/i/veiL1U9KhiYFlA"> Dialogue systems ‚Äì Elena Voita –æ—Ç 22.11.18.mp4</A> |<br>
&emsp; <a href="https://yadi.sk/i/H9ysab5AiSrM4Q"> Dialogue ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 22.11.18.mp4</A> |<br>
&emsp; <a href="https://yadi.sk/i/c1pbEa0P3Up6jz"> DSSM - –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü–∞–Ω–∏–Ω 25.04.2018.mp4</A> |<br>


<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week11_gan"--> 
11 master/week11_gan </A> |<br>
&emsp; <a href="https://yadi.sk/i/bXEimWhiCwvDBQ"> Generative models ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 29.11.18.mp4</A> |<br>
&emsp; <a href="https://yadi.sk/i/31aOtnMUgYbblA"> Un(semi)-supervised word translation learning ‚Äì –±–æ—Ä–∏—Å –ö–æ–≤–∞—Ä—Å–∫–∏–∏ÃÜ –æ—Ç 29.11.18.mp4 </A> |<br>


<!--a href="https://github.com/yandexdataschool/nlp_course/tree/master/week12_summarization"--> 
12 master/week12_summarization</A> |<br>
&emsp; <a href="https://yadi.sk/i/DYRl1sxGK5fyKA">Text summarization ‚Äì Elena Voita –æ—Ç 06.12.18.mp4 </A> |<br>

</div>
</a>

<hr>
<hr>


<a href=""> </A> |<br>










<a name="YandexNLP_2019">
<div id="YandexNLP_2019">
<p><br>

<a href="https://github.com/yandexdataschool/nlp_course">  YandexNLP_2019 Syllabus &ensp;  
( https://github.com/yandexdataschool/nlp_course )</A> :<br><br>


<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week01_embeddings"> 2019/week01_embeddings </A> |<br>     
&emsp; <a href="https://yadi.sk/i/BNTJG-_rwf20Gw"> Natural Language Processing ‚Äì David Talbot –æ—Ç 2019.09.12.mp4</A> |
&ensp; &emsp; ( <a href="https://yadi.sk/i/Ff1jVAODd4P9ug"> Natural Language Processing ‚Äì David Talbot –æ—Ç 13.09.18.mp4</A> )|<br>
&emsp; <a href="https://yadi.sk/i/nUiHl4VPMOCz0g"> Word Embeddings ‚Äì Lena Voita –æ—Ç 2019.09.12.mp4 </A> |
&ensp; &emsp; ( <a href="https://yadi.sk/i/wzVA1XYKS2u6NQ"> Word Embeddings ‚Äì Elena Voita –æ—Ç 13.09.18.mp4</A> )  |<br>
&emsp; <a href="https://yadi.sk/i/QTcGA5mgdhS8jg"> Word Embeddings. –°–µ–º–∏–Ω–∞—Ä ‚Äì –§–µÃà–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 2019.09.12.mp4</A> ) |
&ensp; &emsp; ( <a href="https://yadi.sk/i/X5UYALfyyrwzYw"> Word Embeddings ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 13.09.18.mp4</A> ) | <br>

<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week02_classification"> 2019/week02_classification</A>|<br>
&emsp; <a href="https://yadi.sk/i/6HQc8122IL0fIQ"> Text Classification ‚Äì Lena Voita –æ—Ç 2019.09.19.mp4 </A> |
&ensp; &emsp; ( <a href="https://yadi.sk/i/lIcXiNuBXNAWzg"> Text Classification ‚Äì Boris Kovarsky –æ—Ç 20.09.18.mp4</A> ) |<br>
&emsp; <a href="https://yadi.sk/i/xRwCZEzp0DkL4g"> Text Classification. –°–µ–º–∏–Ω–∞—Ä ‚Äì –§–µÃà–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 2019.09.19.mp4 </A> |
&ensp; &emsp; ( <a href="https://yadi.sk/i/2xYHH0Fj1vb82A"> Text Classification ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 20.09.18.mp4</A> ) |<br>

<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week03_lm"> 2019/week03_lm </A> |<br>
&emsp; <a href="https://yadi.sk/i/ygaphbkfZinu8g"> Language Models ‚Äì Lena Voita –æ—Ç 2019.09.26.mp4 </A> |
&ensp; &emsp; (<a href="https://yadi.sk/i/8JK4dH6E9Iyy8g"> Language models ‚Äì Elena Voita –æ—Ç 27.09.18.mp4</A> )|<br>
&emsp; <a href="https://yadi.sk/i/UXa6vq2FZ5IASg"> Language Models. –°–µ–º–∏–Ω–∞—Ä ‚Äì –§–µÃà–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 2019.09.26.mp4 </A> |
&ensp; &emsp; (<a href="https://yadi.sk/i/HfjTLRejnVwP7g"> N-gram language models or how to write scientific papers ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 27.09.18.mp4</A> )|<br>
&ensp; &emsp; (<a href="https://yadi.sk/i/0x-o0B2FGiHthg"> Recurrent units in detail ‚Äì Elena Voita –æ—Ç 27.09.18.mp4</A> )|<br>


<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week04_seq2seq"> 2019/week04_seq2seq </A> |<br>
&emsp; SAME VIDEO AS IN 2018
&emsp; &emsp; (<a href="https://yadi.sk/i/3y9JX6Q_0ZLqpA"> Seq2seq and Attention ‚Äì Elena Voita –æ—Ç 04.10.2018.mp4</A> ) |<br>
&emsp; SAME VIDEO AS IN 2018
&emsp; &emsp; (<a href="https://yadi.sk/i/cB5606vgbOlBog"> Seq2seq ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 04.10.2018.mp4</A> )|<br>


<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week05_em"> 2019/week05_em </A> |<br>
&emsp; <a href="https://yadi.sk/i/4LcSl4Lg4B6Rsg"> Generative models and hidden variables. Part 1 ‚Äì David Talbot –æ—Ç 2019.10.10.mp4</A> |
&ensp; &emsp; ( <a href="https://yadi.sk/i/E3Rl6x4LtYZZ7g"> –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ ‚Äì –°–µ—Ä–≥–µ–∏ÃÜ –ì—É–±–∞–Ω–æ–≤ –æ—Ç 11.10.18.mp4</A> ) |<br>
&emsp; <a href="https://yadi.sk/i/v5LEWUQKRPpO3g"> Generative models and hidden variables. Part 2 ‚Äì David Talbot –æ—Ç 2019.10.10.mp4</A> |
&ensp; &emsp; ( <a href="https://yadi.sk/i/cIkA3WV9NPjdMg"> Sparse features & Hashing trick. ‚Äì –°–µ—Ä–≥–µ–∏ÃÜ –ì—É–±–∞–Ω–æ–≤ –æ—Ç 11.10.18.mp4</A> ) |<br>

<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week06_mt"> 2019/week06_mt </A> |<br>
&emsp; <a href="https://yadi.sk/i/3tvn0eGg96VRZA"> World Alignment Models. Part 1 ‚Äì David Talbot –æ—Ç 2019.10.17.mp4</A> |
&emsp; &ensp; ( <a href="https://yadi.sk/i/4ZA6BCJBQAxy3Q"> Generative Models And Expectation Maximization ‚Äì David Talbot –æ—Ç 18.10.18.mp4</A> ) |<br>
&emsp; <a href="https://yadi.sk/i/qn19dYPx1AD92w"> World Alignment Models. Part 2 ‚Äì David Talbot –æ—Ç 2019.10.17.mp4</A> |
&emsp; &ensp; ( <a href="https://yadi.sk/i/uPuNKeGkULujVg"> Seminar Generative Models And Expectation Maximization ‚Äì David Talbot –æ—Ç 18.10.18.mp4</A> ) |<br>


<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week07_structured">2019/week07_structured </A> |<br>
&emsp; <a href="https://yadi.sk/i/d_jetJsHF5BY7A"> –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –ß–∞—Å—Ç—å 1 ‚Äì –°–µ—Ä–≥–µ–∏ÃÜ –ì—É–±–∞–Ω–æ–≤ –æ—Ç 2019.10.24.mp4
 </A> |
&emsp; &ensp; (<a href="https://yadi.sk/i/pDxo20nK-UnVHg"> Machine Translation —á–∞—Å—Ç—å 1 ‚Äì David Talbot –æ—Ç 25.10.2018.mp4 </A> )|<br>
&emsp; <a href="https://yadi.sk/i/ANKXdifrghKOJg"> –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ. –ß–∞—Å—Ç—å 2 ‚Äì –°–µ—Ä–≥–µ–∏ÃÜ –ì—É–±–∞–Ω–æ–≤ –æ—Ç 2019.10.24.mp4 </A> |
&emsp; &ensp; ( <a href="https://yadi.sk/i/k1_0T_WptcSvYw"> Machine Translation —á–∞—Å—Ç—å 2 ‚Äì David Talbot –æ—Ç 25.10.2018.mp4 </A> ) |<br>



<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week08_conversation"> 2019/week08_conversation </A> |<br>
&emsp; <a href="https://yadi.sk/i/6_NKQn0cCvFLVQ"> Dialogue systems, –ª–µ–∫—Ü–∏—è ‚Äì –í—è—á–µ—Å–ª—è–≤ –ê–ª–∏–ø–æ–≤ 07-11-2019.mp4
 </A> |
&emsp; &ensp; (<a href="https://yadi.sk/i/6Sbu_oja9h_QZw"> Transfer learning Multi-task learning in NLP - Elena Voita –æ—Ç 08.11.2018.mp4</A> )|<br>

&emsp; <a href="https://yadi.sk/i/H9ysab5AiSrM4Q"> Dialogue ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 22.11.18.mp4 </A> |
&emsp; &ensp; ( <a href="https://yadi.sk/i/AGgnMxu_UvpOsg"> Named EntityRecognition ‚Äì–§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 08.11.18.mp4 </A> )|<br>




<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week09_nmt"> 2019/week09_nmt </A> |<br>
&emsp; NO VIDEO for 2019 
&emsp; &ensp; ( <a href="https://yadi.sk/i/MnuyuCSAOJainw"> Domain adaptation –ª–µ–∫—Ü–∏—è - Boris Kovarsky –æ—Ç 15.11.2018.mp4 </A> )|<br>
&emsp; NO VIDEO for 2019 
&emsp; &ensp; ( <a href="https://yadi.sk/i/15ApU2_cTFbL1g"> Domain adaptation —Å–µ–º–∏–Ω–∞—Ä - –§–µÃà–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 15.11.2018.mp4</A> )|<br>


<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week10_asr"> 2019/week10_asr </A> |<br>
&emsp; <a href="https://yadi.sk/i/PRRQpGzRtUDFUg"> ASR Features ‚Äì –ê–Ω–¥—Ä–µ–∏ÃÜ –ñ–∏–≥—É–Ω–æ–≤ –æ—Ç 21.11.19.mp4</A> |
&emsp; &ensp; ( <a href="https://yadi.sk/i/veiL1U9KhiYFlA"> Dialogue systems ‚Äì Elena Voita –æ—Ç 22.11.18.mp4</A> )|<br>
&emsp; <a href="https://yadi.sk/i/VvqYneX2G2bWlw"> Speech Recognition ‚Äì Pavel Bogomolov –æ—Ç 21.11.19.mp4 </A> |
&emsp; &ensp; ( <a href="https://yadi.sk/i/H9ysab5AiSrM4Q"> Dialogue ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 22.11.18.mp4</A> )|<br>
&emsp; <a href="https://yadi.sk/i/qaIGDE1G7t4Elg"> Voice Command Recognition ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 21.11.19.mp4</A> |
&emsp; &ensp; ( <a href="https://yadi.sk/i/c1pbEa0P3Up6jz"> DSSM - –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –ü–∞–Ω–∏–Ω 25.04.2018.mp4</A> )|<br>




<a href="https://github.com/yandexdataschool/nlp_course/tree/2019/week11_tts"> 2019/week11_tts </A> |<br>
&emsp; <a href="https://yadi.sk/i/bXEimWhiCwvDBQ"> Generative models ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 29.11.18.mp4</A> |<br>
&emsp; &ensp; ( <a href="https://yadi.sk/i/bXEimWhiCwvDBQ"> Generative models ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 29.11.18.mp4</A> )|<br>


&emsp; <a href="https://yadi.sk/i/31aOtnMUgYbblA"> Un(semi)-supervised word translation learning ‚Äì –±–æ—Ä–∏—Å –ö–æ–≤–∞—Ä—Å–∫–∏–∏ÃÜ –æ—Ç 29.11.18.mp4 </A> |<br>


<a href="https://github.com/yandexdataschool/nlp_course/tree/master/week12_summarization"> 12 master/week12_summarization</A> |<br>
&emsp; <a href="https://yadi.sk/i/DYRl1sxGK5fyKA">Text summarization ‚Äì Elena Voita –æ—Ç 06.12.18.mp4 </A> |<br>

<a href=""> </A> |<br>
<a href=""> –õ–µ–Ω–∞ - –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–∞—è –ì–µ–æ–º–µ—Ç—Ä–∏—è </A> |<br>
<a href="https://yandexdataschool.ru/edu-process/nlp-week"> Yandex NLP week </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


</div>
</a>

<hr>
<hr>


<a name="CLASS">
<div id="CLASS">
<p>CLASS:<br>
<a href="http://mccormickml.com/"> Chris McCormick   http://mccormickml.com/ </A> |<br>
HVASS: <br>
<a href="http://www.hvass-labs.org/"> http://www.hvass-labs.org/ </A> |<br>
<a href="https://www.youtube.com/playlist?list=PL9Hr9sNUjfsmEu1ZniY0XpHSzl5uihcXZ"> HVASS Youtube Videos </A> |<br>
<br>
YANDEX:<br>
<a href="https://github.com/yandexdataschool/"> yandexdataschool/ </A> |<br>
<a href="https://github.com/yandexdataschool/nlp_course"> Yandex NLP course on github </A> |<br>
<a href="https://github.com/KirillTushin/nlp_course_yandex"> Yandex NLP course SOLUTIONS (K.Tushin) </A> |<br>
<a href="https://github.com/yandexdataschool/Practical_RL"> yandexdataschool/Practical_RL </A> |<br>
<a href="https://github.com/sshkhr/Practical_RL"> SOLUTION: sshkhr/Practical_RL </A> |<br>
<br>
Nice KAGGLE Courses: <br>
<a href="https://www.kaggle.com/learn/intermediate-machine-learning"> Alexis Cook: Intermediate Machine Learning </A> |<br>
<a href="https://www.kaggle.com/alexisbcook/pipelines"> alexisbcook/pipelines </A> |<br>
<a href="https://www.kaggle.com/learn/natural-language-processing?utm_medium=email&utm_source=intercom&utm_campaign=nlp-course-launch"> Dan Becker and Mat Leonard Kaggle MiniCourse: Natural Language Processing </A> |<br>
<a href="https://www.youtube.com/watch?v=mPFq5KMxKVw"> Dan Becker: transfer-learning (youtube) </A> |<br>
<a href="https://www.kaggle.com/dansbecker/transfer-learning"> Dan Becker: transfer-learning</A> |<br>
<a href="https://www.kaggle.com/dansbecker/underfitting-and-overfitting"> Dan Becker: Underfitting and Overfitting  </A> |<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<br>
iPavlov.AI:<br>
<a href="http://edu.ipavlov.ai/NLP36/"> edu.ipavlov.ai: –∑–∞–Ω—è—Ç–∏—è –∫—É—Ä—Å–∞ DL in NLP </A> |<br>
<a href="https://github.com/deepmipt/NLPCourseBot"> NLPCourseBot </A> |<br>

<br> Py Class from MFTI:<br>
<a href="http://judge.mipt.ru/mipt_cs_on_python3/"> –∫—É—Ä—Å: –ò–Ω—Ñ–æ—Ä–º–∞—Ç–∏–∫–∞. –ê–ª–≥–æ—Ä–∏—Ç–º—ã –∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã –¥–∞–Ω–Ω—ã—Ö –Ω–∞ Python 3.
(05.09.2017, –•–∏—Ä—å—è–Ω–æ–≤ –¢–∏–º–æ—Ñ–µ–π –§—ë–¥–æ—Ä–æ–≤–∏—á)</A> |<br>
<a href="https://www.youtube.com/watch?v=KdZ4HF1SrFs"> –ê–ª–≥–æ—Ä–∏—Ç–º—ã –Ω–∞ Python 3. –õ–µ–∫—Ü–∏—è ‚Ññ1 </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<br>
HSE/VSE NLP like: 
<a href="https://github.com/gameofdimension/aml-nlp/blob/master/week4/week4-seq2seq.ipynb"> Game of Dimension (github) </A> |<br>
<a href="https://chatbotslife.com/full-tutorial-on-how-to-create-and-deploy-a-telegram-bot-using-python-69c6781a8c8f"> Full Tutorial On How to Create and Deploy a Telegram Bot using Python </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="http://qaru.site/questions/152411/doc2vec-how-to-get-document-vectors"> qaru: demo of d2v with gensim </A> |<br>

<p> VSE NLP PROJECTS: check out numerous blogs and newsletters (e.g. http://newsletter.ruder.io/), EMNLP and ACL papers, or attent one of NLP summer schools (e.g. http://lxmls.it.pt/).<p>
<a href="https://www.youtube.com/watch?v=8aeoTryJqyo"> –ê–ª–µ–∫—Å–µ–π (Code or Die) : –ö–∞–∫ —Å–æ–∑–¥–∞—Ç—å —Ç–µ–ª–µ–≥—Ä–∞–º –±–æ—Ç–∞ –∑–∞ 20 –º–∏–Ω—É—Ç (telegram/python/Amazon AWS) </A> |<br>
<a href="https://github.com/abesmon/ivan-petrov-bot"> GitHub: abesmon/ivan-petrov-bot </A> |<br>
<a href="https://www.coursera.org/learn/language-processing">Coursera: main course page </A> |
<a href="https://www.coursera.org/learn/language-processing/supplement/27CC6/getting-started-with-practical-assignments"> Instructions for wk1</A> |<br>
<a href="https://github.com/hse-aml/natural-language-processing"> github: Natural Language Processing course resources + Running on Google Colab</A> |<br>
<a href="https://github.com/hse-aml/natural-language-processing/blob/master/AWS-tutorial.md"> Github/NLP: Tutorial for setting up an AWS Virtual Machine </A> |<br>
<a href="https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EC2_GetStarted.html#ec2-launch-instance"> AWS: Getting Started with Amazon EC2 Linux Instances</A> |<br>
<a href="https://core.telegram.org/bots/api"> Telegram Bot API </A> (TxBot: t.me/txplanobot, HTTP API: xxx-look-somewhere-else-xxx ) |<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

<hr>

---------- 

<a href="https://colab.research.google.com/github/yandexdataschool/nlp_course/blob/master/week01_embeddings/seminar.ipynb"> COLAB: Seminar 1: Fun with Word Embeddings (3 points) </A> |<br>
---------- 

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://github.com/ageron/handson-ml2"> Aur√©lien G√©ron book: Hands-on Machine Learning with Scikit-Learn, Keras and TensorFlow </A> |<br>
<a href="https://github.com/fchollet/deep-learning-with-python-notebooks"> Francois Chollet book: 
deep-learning-with-python-notebooks </A> |<br>
<a href=""> </A> |<br>
<a href="https://www.youtube.com/watch?v=h6ajHkgM3ZU&list=PLe5rNUydzV9Q01vWCP9BV7NhJG3j7mz62"> [DeepBayes2018] </A> |<br>
<a href="https://github.com/bayesgroup/deepbayes-2018"> [DeepBayes2018] github </A> |<br>

<a href="https://www.youtube.com/watch?v=S56OBkUqxHM&feature=youtu.be"> –ì–µ–Ω–Ω–∞–¥–∏–π –®—Ç–µ—Ö. –ù–∞—Å—Ç–æ—è—â–µ–µ –∏ –±—É–¥—É—â–µ–µ –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ –Ω–∞ —Ç–µ–∫—Å—Ç–∞—Ö NLP –∏ production </A> |<br>
<a href="https://github.com/ShT3ch/public_workshop"> –ì–µ–Ω–Ω–∞–¥–∏–π –®—Ç–µ—Ö github </A> |<br>

<a href="https://www.youtube.com/watch?v=0EtD5ybnh_s&feature=youtu.be"> Martin Andrews @ reddragon.ai: Language Learning with BERT - TensorFlow and Deep Learning Singapore </A> |<br>
<a href="https://www.youtube.com/watch?v=OYygPG4d9H0&feature=youtu.be"> 
Ivan Bilan: Understanding and Applying Self-Attention for NLP | PyData Berlin 2018 </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://www.kaggle.com/learn/intro-to-machine-learning?utm_medium=email&utm_source=intercom&utm_campaign=beginner-friendly-competition-announcement"> Kaggle: Intro to ML </A> |<br>

<a href="https://www.kaggle.com/learn/intermediate-machine-learning?utm_medium=email&utm_source=intercom&utm_campaign=beginner-friendly-competition-announcement"> Kaggle: Intermediate ML </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://app.slack.com/client/T040HKJE3/C04N3UMSL/thread/C04N3UMSL-1567427465.220500"> –∫—É—Ä—Å –∂–µ–ª–∞—Ç–µ–ª—å–Ω–æ —Å –ø—Ä–∞–∫—Ç–∏–∫–æ–π –ø–æ –Ω–ª–ø  </A> |<br>

<br>
Yandex Stixi: <br>
<a href="https://events.yandex.ru/lib/talks/7010/"> –ö–∞–∫ –Ω–∞—É—á–∏—Ç—å –Ω–µ–π—Ä–æ—Å–µ—Ç—å –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å—Ç–∏—Ö–∏ </A> |<br>
<a href="https://events.yandex.ru/events/ds/"> –ö–æ–Ω—Ñ–µ—Ä–µ–Ω—Ü–∏–∏, –º–∏—Ç–∞–ø—ã, —Å–µ–º–∏–Ω–∞—Ä—ã –∏ –¥—Ä—É–≥–∏–µ —Å–æ–±—ã—Ç–∏—è, –ø–æ—Å–≤—è—â–µ–Ω–Ω—ã–µ data science. </A> |<br>
<a href="https://github.com/IlyaGusev/deep-nlp-seminars/blob/master/seminar_02/embeddings.ipynb"> Word Emb Code Gusev  </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://www.youtube.com/watch?v=UC9msiP40jg&list=PLJOzdkh8T5kq7O4f6_d_zBsgOqayrKxzP"> Yandex Data School: 64 video lectures </A> |<br>
<br>
STIHI.Ru corpus Taiga<br>

TravisCI
Keras -> PyTorch + AllenNLP
–ê–≤—Ç–æ–ø–æ—ç—Ç –Ø–Ω–¥–µ–∫—Å–∞ / –ù–µ–π—Ä–æ–ø–æ—ç—Ç –õ—ë—à–∏ –¢–∏—Ö–æ–Ω–æ–≤–∞
–Ø–Ω–¥–µ–∫—Å.–†–µ—Ñ–µ—Ä–∞—Ç—ã–ü—Ä–æ–≥—Ä–∞–º–º–∞-–ø–æ—ç—Ç –ö–∞–≥–∞–Ω–æ–≤–∞


<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://github.com/shmam/BeepoBot"> BeepoBot in js </A> |<br>
<a href="https://www.youtube.com/watch?v=OypPjvm4kiA&t=109s"> machead: How I'm Learning AI and Machine Learning </A> |<br>
</div>
</a>

<hr>
<hr>




<a name="RNN_LSTM">
<div id="RNN_LSTM">
<p> RNN_LSTM <br>
RNN, Text Generation:
<a href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/sequences/text_generation.ipynb"> github: Text generation using a RNN with eager execution </A> |<br>
<a href="https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/generative_examples/text_generation.ipynb"> Text Generation using a RNN </A> |<br>

LSTM: <br>
<a href="https://hackernoon.com/understanding-architecture-of-lstm-cell-from-scratch-with-code-8da40f0b71f4"> Understanding architecture of LSTM cell from scratch with code</A> |<br>
<a href="https://arxiv.org/pdf/1806.07366.pdf"> Paper on ODE-NET</a> |<br>
<a href="https://towardsdatascience.com/sentence-classification-using-bi-lstm-b74151ffa565">Sentence classification using Bi-LSTM </A> |<br>

Language Models:<br>
<a href="https://github.com/openai/gpt-2"> OpenAI's GPT-2 Github </a> |<br>
<a href="https://openai.com/blog/better-language-models/"> OpenAI's GPT-2 Blogs </a> |<br>


<hr>
<hr>




<a name="CNN_CNN">
<div id="CNN_CNN">
<p> CNN_CNN: <br>
<a href="https://towardsdatascience.com/cnn-architectures-a-deep-dive-a99441d18049"> CNN Architectures </A> |<br>
<a href="https://www.kaggle.com/cdeotte/25-million-images-0-99757-mnist">Chris Deotte on Kaggle: 25 Million Images! [0.99757] MNIST  </A> |
<a href="http://www.ccom.ucsd.edu/~cdeotte/"> CHRIS DEOTTE </a> |<br>
<a href=""> </A> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </A> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
</div>
</a>

</div>
</a>
<hr>
<hr>



<a name="PEOPLE_PEOPLE">
<div id="PEOPLE_PEOPLE">
<p> PEOPLE_PEOPLE:<br>

==========> Michael Galkin : <br>
<a href="https://www.notion.so/yads/Knowledge-Graphs-Course-2021-312c41e528b247d6921bfaa82bcd99ea">Boyadzhi-notion: Knowledge Graphs Course 2021: –æ–±—â–∏–π –Ω–æ—Ç–∏–æ–Ω –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–Ω—ã—Ö –∑–∞–ø–∏—Å–æ–∫ –ø–æ –∫—É—Ä—Å—É. A new tool for teams & individuals that blends everyday work apps into one.</A> |<br>
<a href="https://www.gitmemory.com/migalkin"> https://www.gitmemory.com/migalkin</A> |<br>
<a href="https://migalkin.github.io/"> https://migalkin.github.io/ </A> |<br>
<a href="https://migalkin.github.io/kgcourse2021/"> https://migalkin.github.io/kgcourse2021/</A> |<br>
<a href="https://github.com/migalkin/kgcourse2021"> https://github.com/migalkin/kgcourse2021</A> |<br>

<a href="https://www.youtube.com/watch?v=y8OmCRNQoWU&feature=youtu.be"> KG: Lecture 1</A> |<br>
<a href="https://wikidata.metaphacts.com/resource/app:Start"> https://wikidata.metaphacts.com</A> |<br>
<a href=""> </A> |<br>
<a href="https://migalkin.github.io/kgcourse2021/lectures/lecture3"> kgcourse2021/lectures/lecture3 </A> |<br>
<a href="https://www.youtube.com/watch?v=gX_KHaU8ChI"> YT 2021: Knowledge Graph Embeddings Tutorial: From Theory to Practice</A> |<br>
<a href="https://kge-tutorial-ecai2020.github.io/">Knowledge Graph Embeddings Tutorial: From Theory to Practice </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://twitter.com/mataneyal1/status/1359978174885605379?s=09"> Matan Eyal
How do you train a relation extractor without training data? We show simple and effective bootstrap approaches that work with as little as 3 user-provided examples. Our EACL is now available on https://t.co/fw2Xvp7Jko With https://twitter.com/asaf_amr https://twitter.com/hilleltt and https://twitter.com/yoavgo</A> |<br>







==========> –°–µ—Ä–≥–µ–π –ò–≤–∞–Ω–æ–≤ (–ü–∞—Ä–∏–∂, –ö—Ä–∏—Ç–µ–æ) : <br>
<a href="https://t.me/graphML">C–µ—Ä–≥–µ–π –æ–¥–∏–Ω –∏–∑ –ª–∏–¥–µ—Ä–æ–≤ –≥—Ä–∞—Ñ–æ–≤–æ–≥–æ –∫–æ–º—å—é–Ω–∏—Ç–∏ –≤ –û–î–° –∏ –∞–≤—Ç–æ—Ä –ª—É—á—à–µ–≥–æ –∫–∞–Ω–∞–ª–∞ –≤ —Ç–≥ –ø–æ –≥—Ä–∞—Ñ-–º–ª  https://t.me/graphML </A> |<br>
<a href="https://www.youtube.com/watch?v=rMNA68wLAxk&feature=youtu.be"> Boost then Convolve: Gradient Boosting Meets Graph Neural Networks</A> |<br>
<a href="https://openreview.net/forum?id=ebS5NUfoMKL"> paper: Boost then Convolve: Gradient Boosting Meets Graph Neural Networks</A> |<br>
<a href="https://github.com/nd7141/bgnn"> The code and data for the ICLR 2021 paper: Boost then Convolve: Gradient Boosting Meets Graph Neural Networks </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

==========> Anvar Kumukov : <br>
<a href="https://www.youtube.com/playlist?list=PLiptW-XRuYFXTlSmu4qR7dmVXe6eFelV9"> Anvar Kumukov PlayList</A> |<br>

<a href="https://youtu.be/TLVhGs2ckPY">SBERLOGA: A–Ω–≤–∞—Ä –ö—É—Ä–º—É–∫–æ–≤ :   "GNN & GCN for complete beginners with examples in python" </A> |<br>
–ë—É–¥–µ—Ç –æ–±—ä—è—Å–Ω–µ–Ω–æ, —á—Ç–æ —Ç–∞–∫–æ–µ –≥—Ä–∞—Ñ–æ–≤—ã–µ –Ω–µ–π—Ä–æ–Ω–Ω—ã–µ —Å–µ—Ç–∏, –≤ —á–∞—Å—Ç–Ω–æ—Å—Ç–∏ —Ä–∞–∑–±–µ—Ä–µ–º 3 –ø–æ–ø—É–ª—è—Ä–Ω—ã–µ —Å—Ç–∞—Ç—å–∏ –ø–æ—Å–≤—è—â–µ–Ω–Ω—ã–µ –≥—Ä–∞—Ñ–æ–≤—ã–º —Å–µ—Ç–∫–∞–º:<br>
<a href="https://arxiv.org/pdf/1609.02907.pdf">1. Kipf GCN https://arxiv.org/pdf/1609.02907.pdf </A> |<br>
<a href="https://arxiv.org/pdf/1706.02216.pdf"> 2. Graph SAGE https://arxiv.org/pdf/1706.02216.pdf</A> |<br>
<a href="https://www.cse.wustl.edu/~ychen/public/DGCNN.pdf"> 3.  An End-to-End Deep Learning Architecture for Graph Classification https://www.cse.wustl.edu/~ychen/public/DGCNN.pdf </A> |<br>
–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –≤ —Ç–µ–ª–µ–≥—Ä–∞–º —á–∞—Ç–µ: @sberlogawithgraphs - –ø—Ä–∏—Å–æ–µ–¥–∏–Ω—è–π—Ç–µ—Å—å<br>
–ó—É–º: https://us02web.zoom.us/j/81647783013?pwd=cWNoWk0vSlR2bFdpTlhzSXJNY0RlQT09<br>
<a href="https://www.youtube.com/c/SBERLOGA/videos">–ó–∞–ø–∏—Å–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–æ–∫–ª–∞–¥–æ–≤ - https://www.youtube.com/c/SBERLOGA/videos </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

<br>
<p>

–ú–Ω–æ–≥–∏–µ, –Ω–∞–≤–µ—Ä–Ω–æ, –≤–∏–¥–µ–ª–∏ —ç—Ç–∏ —á—É–¥–æ –∫–∞—Ä—Ç–∏–Ω–∫–∏, –∫–æ—Ç–æ—Ä—ã–µ –¥–µ–ª–∞–µ—Ç UMAP –ø—Ä–æ—Å—Ç–æ –∏–∑ –º–Ω–æ–∂–µ—Å—Ç–≤–∞ —Ü–µ–ª—ã—Ö —á–∏—Å–µ–ª –∏—Å–ø–æ–ª—å–∑—É—è —Ä–∞–∑–ª–æ–∂–µ–Ω–∏–µ –Ω–∞ –ø—Ä–æ—Å—Ç—ã–µ —á–∏—Å–ª–∞ https://mathoverflow.net/a/355631/10446 . –û—Å—Ç–∞–µ—Ç—Å—è –∑–∞–≥–∞–¥–∫–æ–π –æ—Ç—Ä–∞–∂–∞—é—Ç –ª–∏ –æ–Ω–∏ —Å—Ç—Ä—É–∫—Ç—É—Ä—É –ø—Ä–æ—Å—Ç—ã—Ö —á–∏—Å–µ–ª –∏–ª–∏ –∞—Ä—Ç–∏—Ñ–∞–∫—Ç UMAP.  –í–æ—Ç —Ç—É—Ç –∞—Ä–≥—É–º–µ—Ç–∞—Ü–∏—è –∑–∞ –≤—Ç–æ—Ä–æ–µ https://twitter.com/hippopedoid/status/1318917878364672001 (—á—Ç–æ –≤–æ–æ–±—â–µ–º –∏ —Ä–∞–Ω—å—à–µ –æ–∂–∏–¥–∞–ª–æ—Å—å)
==========> : <br>

<a href=""> </A> |<br>
<a href="https://en.wikipedia.org/wiki/Runge%27s_phenomenon"> Runge's phenomenon - similar to the Gibbs phenomenon in Fourier series approximations.</A> |<br>
<a href="https://en.wikipedia.org/wiki/Chebyshev_nodes"> Chebyshev nodes</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<a href="https://www.youtube.com/user/EugeneKhutoryansky/playlists"> https://www.youtube.com/user/EugeneKhutoryansky/playlists</A> |<br>

<a href="https://arxiv.org/abs/2010.06992"> InstantEmbedding: Efficient Local Node Representations paper by Anton Tsitsulin from Bonn</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

===> Steve Jobs:<br>
<a href="https://www.youtube.com/watch?v=rQKis2Cfpeo&feature=youtu.be"> Steve Jobs on how to hire, manage, and lead people: Great people can self-manage. What is needed is a vision. If you get a core group of 10 great people it becomes self-policing whom they let into that group. </A> |<br>
<a href="https://www.youtube.com/watch?v=oeqPrUmVz-o&feature=youtu.be"> Steve Jobs Insult Response - Highest Quality: start with the customer </A> |<br>
<a href="https://www.youtube.com/watch?v=5hOZ8wQ9Ka4"> Steve Job's philosophy  </A> |<br>

<a href="https://youtu.be/oNC8LEj5X4U">YT: Steve Jobs Lost Interview 1990 Highly Recommended For Entreprenuers" </A> |<br>
<p>
<a href="https://www.youtube.com/watch?v=fg8lKeJZ7vA&feature=youtu.be"> 2020.07: –ö–∞–∫ –∑–∞–¥–µ—Ç–µ–∫—Ç–∏—Ç—å Deepfake –∏ –≤—ã–∏–≥—Ä–∞—Ç—å $500k ‚Äî –°–µ–ª–∏–º –°–µ—Ñ–µ—Ä–±–µ–∫–æ–≤</A> |<br>
<a href="https://www.youtube.com/watch?v=5wMAPUrd0ag">2018.08: Data Science: Kaggle GRANDMASTER in 6 months? | Pavel Pleskov </A> |<br>
<a href="https://www.youtube.com/watch?v=o6u_Od27IFw"> 2018.11: –ö–∞–∫ –≤—ã–∏–≥—Ä—ã–≤–∞—Ç—å –ª—é–±—ã–µ Data Science —Å–æ—Ä–µ–≤–Ω–æ–≤–∞–Ω–∏—è. –ü–∞–≤–µ–ª –ü–ª–µ—Å–∫–æ–≤</A> |<br>
<p>
===> Abhishek Thakur : <p>
<a href="https://www.youtube.com/watch?v=8lniZVqRLA0&feature=youtu.be"> 2019.07: Interview with Abhishek Thakur | World's First Triple Grandmaster | Kaggle</A> |<br>
<a href="https://www.youtube.com/watch?v=uWVR_axaVwk"> 2019.08: Approaching (almost) Any Machine Learning Problem | by Abhishek Thakur | Kaggle Days Dubai |</A> |<br>
<a href="https://www.youtube.com/watch?v=XaQ0CBlQ4cY"> 2020.03: Text Extraction From a Corpus Using BERT (AKA Question Answering)</A> |<br>
<a href=""> </A> |<br>
<a href="https://www.youtube.com/watch?v=u91645MFytY"> 2019.03 Kaggle Coffee Chat: Jacob Devlin (Google Researcher, BERT author)</A> |<br>
<a href="https://www.youtube.com/watch?v=VAg8pMJRbgA">2020.08: Talk#9: Vladimir Iglovikov; Detecting Masked Faces In The Pandemic World </A> |<br>
<a href="https://www.youtube.com/watch?v=cTnWvYJJRCY">2020: –ö–∞–∫ –º—ã –ø–æ–±–µ–¥–∏–ª–∏ —Å –ø–æ–º–æ—â—å—é CatBoost –∏ AutoML ‚Äî –ê–ª–µ–∫—Å–∞–Ω–¥—Ä –õ–µ–ª—é–∫, –ü–µ—Ç—Ä –ì—É—Ä–∏–Ω–æ–≤ </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

=======> Lex Fridman:<br>
<a href="https://arxiv.org/pdf/1503.08479.pdf">Lex Fridman's paper on UE location </A> |<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="http://books.google.com/books?id=sZ-NPuM-ScYC&pg=PR10&dq=Denis+Gudovskiy&ei=kjm-SbSTEY6WzgST66XQCA&hl=ru"> 
Design for Manufacturability and Statistical Design: A Constructive Approach
–ê–≤—Ç–æ—Ä—ã: Michael Orshansky, Sani Nassif, Duane Boning - 
BOOK for EE382V: Digital IC Design Course. "In theory, there is no difference between theory and practice. In practice, there is." Chuck Reid</A> |<br>

manuel amunategui / people:
<a href="https://www.viralml.com/video-content.html?v=2hKGeDStUOY&Title=My%20Writing%20&%20Publishing%20Stack%20-%20Tools%20to%20Get%20Published%20-%20Part%202">
Publishing Part2 </A> |<br>

<a href="http://www.viralml.com/mueller"> "Unredacted" Muller Report </A> |<br>
<a href="https://www.viralml.com/#Tools"> Tools - lots of short videos  </A> |<br>
<a href=""> </A> |
<a href="https://www.youtube.com/watch?v=C95FgeekrSw"> –∑–¥–µ—Å—å —Å 14:30 –±–∞–±—É—à–∫–∏–Ω —Ä–∞—Å—Å–∫–∞–∑—ã–≤–∞–µ—Ç –æ –î–° –ø—Ä–æ–µ–∫—Ç–∞—Ö –≤ —Ö5 - –æ—á–µ–Ω—å –ª—é–±–æ–ø—ã—Ç–Ω–æ </A> |<br>
<a href=""> </A> |


<a href="https://towardsdatascience.com/training-machine-learning-models-online-for-free-gpu-tpu-enabled-5def6a5c1ce3"> 
Training machine learning models online for free(GPU, TPU enabled)!!!</A> |<br>
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |


</div>
</a>
<hr>
<hr>


<a name="Research_Google">
<div id="Research_Google">
<p> Research.Google.Com: <br>
<a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true"> Colab Welcome </A> |
<a href="https://research.google.com/seedbank/"> SeedBank</A> |
<a href="https://research.google.com/seedbank/seeds?keyword=text"> NLP seeds </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |

<br> COLAB Acceleration: <br>
COLAB enable GPU/TPU:  Edit -> Notebook settings -> H/W accelerator<br>
or VIEW -> NOTEBOOK INFO -> Modify Notebook Settings -> H/W Accelerator<br>
<p> COLAB Acceleration demos: 
<a href="https://colab.research.google.com/notebooks/gpu.ipynb#scrollTo=QXRh0DPiZRyG">COLAB with GPU demo </A> |
<a href="https://colab.research.google.com/notebooks/tpu.ipynb"> Enabling TPU at Colab </A> |

<p> COLAB Text Gen:
<a href="https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb"> Predict Shakespeare with Cloud TPUs and Keras </A> |<br>
Replace Shakespeare text with this: http://www.gutenberg.org/cache/epub/59514/pg59514.txt<br>
seed_txt = 'He realized, even as he spoke, that the telling had changed even since his own youth. As a boy of ten'<br>

<p>
How to run external notebook in COLAB<br>
<br>
1/ File -> New Py3 Ntbk<br>
2/ File -> Upload Ntbk<br>
        -> Choose File (if local)<br>
        -> Github -> https://github.com/ftk1000/style_xfer/blob/master/style_xfer_demo.ipynb  <br>
        -> GoogleDrive<br>


<p>

<p> COLAB Style Transfer:
<a href="https://colab.research.google.com/github/tensorflow/models/blob/master/research/nst_blogpost/4_Neural_Style_Transfer_with_Eager_Execution.ipynb#scrollTo=X8w8WLkKvzXu"> Neural Style Transfer Code on Colab </A> |

<p> COLAB/Accessing Local file system from google colab:
<a href="https://stackoverflow.com/questions/48376580/google-colab-how-to-read-data-from-my-google-drive"> Stackoverlow suggestions </A> |
<a href="https://colab.research.google.com/notebooks/io.ipynb#scrollTo=0ENMqxq25szn"> Google's suggestions </A> |
<a href="https://drive.google.com/file/u/0/d/1srw_HFWQ2SMgmWIawucXfusGzrj1_U0q/edit"> One example not accessible from vz  </A> |
<a href="https://myaccount.google.com/data-and-personalization"> myaccount.google.com </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href="https://research.google.com/seedbank/"> research.google.com/seedbank/ </A> |
<a href="https://towardsdatascience.com/training-machine-learning-models-online-for-free-gpu-tpu-enabled-5def6a5c1ce3"> 
Training machine learning models online for free(GPU, TPU enabled)!!!</A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
</div>
</a>

<hr>
<hr>



<hr>
<hr>
<a name="COLAB">
<div id="COLAB">
<p> COLAB:<br>

</div>
</a>




<hr>
<p>
<p> UDACITY: 
<a name="UDACITY">
<div id="UDACITY">

<a href="https://classroom.udacity.com/courses/ud187/lessons/ff58baf7-22c2-4052-9f9f-a8b2415ba7df/concepts/d1fd8523-2e59-4f3b-8e0c-cfa0a636760b"> 
Intro to TensorFlow for Deep Learning byTensorFlow. This course is a practical approach to deep learning for software developers</A> |

<a href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l01c01_introduction_to_colab_and_python.ipynb"> Udacity Intro to TF Colab link </A> |

<a href="https://colab.research.google.com/github/tensorflow/examples/blob/master/courses/udacity_intro_to_tensorflow_for_deep_learning/l02c01_celsius_to_fahrenheit.ipynb"> TF: C2F </A> |

<a href="https://www.youtube.com/watch?v=IpGxLWOIZy4"> A Friendly Introduction to Machine Learning (youtube video) </A> |
<a href=""> </A> |<br>
<a href="https://www.youtube.com/watch?v=BR9h47Jtqyw"> A friendly introduction to Deep Learning and Neural Networks </A> |<br>


<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |
<a href=""> </A> |


</div>
</a>

<hr>



<a name="CONFERENCE">
<div id="CONFERENCE">
<p>
CONFERENCE:<br>
<a href="https://colab.research.google.com/notebooks/welcome.ipynb#recent=true">colab.research.google.com </A> |<br>
<a href="http://neuralnetworksanddeeplearning.com/chap5.html"> Read Mike's book</A> |<br>
<a href="https://adeshpande3.github.io/adeshpande3.github.io/A-Beginner's-Guide-To-Understanding-Convolutional-Neural-Networks/"> intro to CNN </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<p>
======> RUDER: <br>
<a href="https://medium.com/dair-ai/nlp-year-in-review-2019-fb8d523bcb19"> NLP Year in Review ‚Äî 2019 - dair.ai - Medium
 </A> |<br>
<a href="http://ruder.io/a-review-of-the-recent-history-of-nlp/"> keep reading Ruder's blog  </A> |<br>
<a href="https://ruder.io/research-highlights-2019/index.html#9-efficient-and-long-range-transformers"> ruder.io/research-highlights-2019 </A> |<br>
<a href="http://newsletter.ruder.io/issues/accelerating-science-memorizing-vs-learning-to-look-things-up-schmidhuber-s-2010s-greek-bert-arc-illustrated-reformer-annotated-gpt-2-olmpics-223195"> newsletter.ruder.io: Issue #50, Feb 25, 2020: accelerating-science, memorizing-vs-learning, etc</A> |<br>
<a href="http://newsletter.ruder.io/issues/covid-19-edition-236509"> April 15  Issue #52 http://newsletter.ruder.io/issues/covid-19-edition-236509 </A> |<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

<a href="https://arxiv.org/abs/2001.08055?utm_campaign=NLP%20News&utm_medium=email&utm_source=Revue%20newsletter"> 2020: Up to two billion times acceleration of scientific simulations with deep neural architecture search, by M. F. Kasim et al </A> |<br>

<a href="https://github.com/yandexdataschool/nlp_course"> Yandex NLP course on github </A> |<br>
<a href="https://github.com/yandexdataschool/nlp_course/blob/master/resources/slides/lecture1_word_embeddings.pdf"> Elena Voita Word Embedding </A> |<br>
<a href="https://keras.io/visualization/"> keras.io/visualization/</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<p>






<hr>
<hr>
<a name="DIALOGS_DIALOGS">
<div id="DIALOGS_DIALOGS">
<p> DIALOGS_DIALOGS:<br>
<a href="http://nlpprogress.com/english/dialogue.html"> http://nlpprogress.com/english/dialogue.html  </A> |<br>
<a href="https://www.kaggle.com/nltkdata/switchboard?"> https://www.kaggle.com/nltkdata/switchboard?  </A> |<br>
<a href="https://catalog.ldc.upenn.edu/LDC97S62 "> https://catalog.ldc.upenn.edu/LDC97S62 </A> |<br>

<a href="https://breakend.github.io/DialogDatasets/"> A Survey of Available Corpora for Building Data-Driven Dialogue Systems</a> |<br>
<a href="https://ai.google/tools/datasets/coached-conversational-preference-elicitation"> coached-conversational-preference-elicitation </a> |<br>
<a href="https://www.kaggle.com/Cornell-University/movie-dialog-corpus"> https://www.kaggle.com/Cornell-University/movie-dialog-corpus</a> |<br>
<a href="https://www.kaggle.com/tovarischsukhov/southparklines"> https://www.kaggle.com/tovarischsukhov/southparklines</a> |<br>
<a href="https://www.google.com/search?q=Switchboard-1+corpus&rlz=1C1GCEA_enUS800US800&oq=Switchboard-1+corpus&aqs=chrome..69i57j33.651j0j7&sourceid=chrome&ie=UTF-8 "> google search results </A> |<br>
<br>
<a href="https://patents.google.com/patent/US8364485B2/en">Method for automatically identifying sentence boundaries in noisy conversational data</a>|<br>
<a href="https://scholar.google.com/scholar?hl=en&as_sdt=0%2C44&as_vis=1&q=nlp+noisy+transcript+summary&btnG=">Google search for 'nlp noisy transcript summary'</a>|<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
</div>
</a>


<hr>
<hr>
<a name="GANS">
<div id="GANS">
<p> GANs:<br>
<a href="https://yadi.sk/i/bXEimWhiCwvDBQ"> Generative models ‚Äì –§–µ–¥–æ—Ä –†–∞—Ç–Ω–∏–∫–æ–≤ –æ—Ç 29.11.18.mp4</A> |
<a href="https://openai.com/blog/generative-models/"> Open AI Generative Models BLOG</A> |
<a href="https://bit.ly/2zqYI9F"> tons of GANs on github </A> |
<a href="https://arxiv.org/abs/1406.2661"> Original paper: Generative Adversarial Nets, Ian J. Goodfellow et al 2014 </A> |


<a href="https://machinelearningmastery.com/how-to-implement-wasserstein-loss-for-generative-adversarial-networks/"> 
Jason Brownlee: 
How to Implement Wasserstein Loss for Generative Adversarial Networks</a> |
<a href="https://myurasov.github.io/2017/09/24/wasserstein-gan-keras.html"> M Yurasov: Wasserstein GAN in Keras </a> |
<a href="https://machinelearningmastery.com/how-to-code-a-wasserstein-generative-adversarial-network-wgan-from-scratch/"> Jason Brownlee: How to Develop a Wasserstein Generative Adversarial Network (WGAN) From Scratch
</a> |
<a href="https://machinelearningmastery.com/how-to-develop-an-auxiliary-classifier-gan-ac-gan-from-scratch-with-keras/"> 
Jason Brownlee: 
How to Develop an Auxiliary Classifier GAN (AC-GAN) From Scratch with Keras</a> |
<a href=""> </a> |
<a href="https://arxiv.org/abs/1710.04087"> Word Translation Without Parallel Data by Alexis Conneau et al</a> |
<a href=""> </a> |

<br> GAN Portrait of Edmond Belamy :<br>
<a href="https://www.christies.com/features/A-collaboration-between-two-artists-one-human-one-a-machine-9332-1.aspx"> www.christies.com </a> |
<a href="http://obvious-art.com/"> http://obvious-art.com/ </a> |
<a href="https://arxiv.org/pdf/1701.00160.pdf"> NIPS 2016 Tutorial: Generative Adversarial Networks, Ian Goodfellow</a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
</div>
</a>

<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |








<hr>
<hr>

<a name="EMBEDDING_EMBEDDING">
<div id="EMBEDDING_EMBEDDING">
<p>EMBEDDING_EMBEDDING:<br>
<a href="https://github.com/ftk1000/YandexNLP_2018/blob/master/wk01_seminar.ipynb"> EMBEDDING DEMO: https://github.com/ftk1000/YandexNLP_2018/blob/master/wk01_seminar.ipynb</a> |

<a href="https://tfhub.dev/google/universal-sentence-encoder/4"> Universal Sentence Encode v4  https://tfhub.dev/google/universal-sentence-encoder/4</a> | <br>
<a href="https://tfhub.dev/google/universal-sentence-encoder/3"> Universal Sentence Encode v3  https://tfhub.dev/google/universal-sentence-encoder/3</a> | <br>
<a href="https://arxiv.org/abs/1803.11175"> https://arxiv.org/abs/1803.11175  Universal Sentence Encoder by Daniel Cer, et al  </a> | <br>

<a href="https://www.kaggle.com/cpmpml/spell-checker-using-word2vec"> Kaggle Notebook by CPMP: Spell Checker using Word2vec</a> |<br>
<a href="https://www.kaggle.com/pierremegret/gensim-word2vec-tutorial"> Kaggle: gensim-word2vec-tutorial</a> |<br>
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |
<a href=""> </a> |

<br> ====> Word2Vec:<br>
<a href="https://github.com/ftk1000/w2v_ftk1000"> https://github.com/ftk1000/w2v_ftk1000 </a> |
<a href="https://github.com/tmikolov/word2vec"> https://github.com/tmikolov/word2vec </a> |
<a href="https://arxiv.org/abs/1301.3781"> "Efficient Estimation of Word Representations in Vector Space", by
Tomas Mikolov, Kai Chen, Greg Corrado, Jeffrey Dean, 2013</a> |<br>
<a href="https://arxiv.org/pdf/1405.4053.pdf">  "Distributed Representations of Sentences and Documents", by Quoc Le, Tomas Mikolov, 2014 </A> |<br>
<a href="http://mccormickml.com/2016/04/19/word2vec-tutorial-the-skip-gram-model/">Chris McCormick: Word2Vec Tutorial - The Skip-Gram Model </A> |<br>
<br>

<a href="https://nlp.stanford.edu/projects/glove/"> GloVe webpage: https://nlp.stanford.edu/projects/glove/</a> |<br>
<a href=" https://nlp.stanford.edu/pubs/glove.pdf"> GloVe paper:  https://nlp.stanford.edu/pubs/glove.pdf</a> |<br>
<a href="https://streamhacker.com/2014/12/29/word2vec-nltk/"> Jacob Perkins: word2vec with NLTK </a> |<br>
<a href="https://code.google.com/archive/p/word2vec/"> google word2vec </A> |<br>
<a href="https://github.com/svn2github/word2vec"> github clone of an SVN repository at http://word2vec.googlecode.com/svn/trunk. </a> |<br>
<a href="https://github.com/danielfrg/word2vec"> danielfrg/word2vec </a> |<br>
<a href="https://nbviewer.jupyter.org/github/danielfrg/word2vec/blob/master/examples/word2vec.ipynb"> Danilefrg: word2vec.ipynb </a> |<br>
<a href=""> </a> |<br>
<a href="https://youtu.be/U0LOSHY7U5Q"> –õ—å–≤–∞ –ö–æ–Ω—Å—Ç–∞–Ω—Ç–∏–Ω–æ–≤—Å–∫–æ–≥–æ: –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–∞–Ω—è—Ç–∏–µ –ø–æ –æ–±—Ä–∞–±–æ—Ç–∫–µ —Ç–µ–∫—Å—Ç–∞ –≤ gensim —Å –ø–æ–º–æ—â—å—é –∞–ª–≥–æ—Ä–∏—Ç–º–∞ word2vec </a> |<br>
<a href="https://www.aclweb.org/anthology/Q15-1016/"> Improving Distributional Similarity with Lessons Learned from Word Embeddings, by Omer Levy, Yoav Goldberg, Ido Dagan</a> |<br>

<p>
====> W2V & GloVe Videos:<br> 
<a href="https://youtu.be/hjx-zwVdfjc">W2V by Andrew Ng </a> |
<a href="https://youtu.be/UqRCEmrv1gQ">W2V from Semicolon ; (start from min 4:00) </a> |
<a href="https://youtu.be/Qu-cvY4HP4g"> NER Example by Andrew Ng </a> |
<a href="https://youtu.be/xtPXjvwCt64"> Lang Model with Embedding by A.Ng </a> |

<a href="https://youtu.be/64qSgA66P-8">W2V in TF by a Korean Dude </a> |
<a href=""> </a> |
<a href="https://youtu.be/QyrUentbkvw"> W2V by Jordan Boyd-Graber </a> |
<a href="https://youtu.be/aCLMeJuGx9k"> W2V video from the dude (Jordan Boyd-Graber) again </a> |

<a href="https://youtu.be/Kc2IXCpdEoM"> GloVe by The LazyProgrammer (Nice!)</a> |
<a href="https://youtu.be/Bzmforcxp8w"> GloVe by A.Ng</a> |

<p>
====> Other resources:<br>

<a href="https://colab.research.google.com/notebooks/mlcc/intro_to_sparse_data_and_embeddings.ipynb#scrollTo=jGWqDqFFL_NZ"> Embedding Demo on COLAB </A> |<br>
<a href="https://www.youtube.com/watch?v=64qSgA66P-8"> Minsuk Heo video: Word2Vec (TF implementation) </A> |<br>
<a href="https://github.com/minsuk-heo/python_tutorial/blob/master/data_science/nlp/word2vec_tensorflow.ipynb"> </a> 2018:  Minsuk Heo: Word2Vec github repo|<br>
<br>
<a href="http://qaru.site/questions/152411/doc2vec-how-to-get-document-vectors"> qaru: demo of d2v with gensim </A> |<br>
<a href="http://qaru.site/questions/86593/how-to-calculate-the-sentence-similarity-using-word2vec-model-of-gensim-with-python"> qaru.site: –ó–ê–ú–ï–ß–ê–¢–ï–õ–¨–ù–û–ï (!) –æ–±—Å—É–∂–¥–µ–Ω–∏–µ doc2vec, par2vec etc</A> |<br>
<a href=""> </a> |<br>

<p>
==> /STARSPACE MODEL:
<br>
<a href="https://rdrr.io/cran/ruimtehol/man/starspace_save_model.html"> starspace_save_model: Save a starspace model as a binary or tab-delimited TSV file </A> |<br>
<a href="https://github.com/facebookresearch/StarSpace"> facebookresearch/StarSpace </A> |<br>
<a href="https://github.com/cran/ruimtehol"> StarSpace in R (really good!) </A> |<br>
<a href="http://qaru.site/questions/15259757/how-to-load-embeddings-in-tsv-file-generated-from-starspace"> how-to-load-embeddings-in-tsv-file-generated-from-starspace </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://www.youtube.com/watch?v=5PL0TmQhItY"> macheads101: Word Embeddings  https://embeddings.macheads101.com </A> |<br>
<a href="https://www.youtube.com/watch?v=6xPnEh_tJEc"> Beyond word2vec: GloVe, fastText, StarSpace - Konstantinos Perifanos ARGOS </A> |<br>
<a href="https://www.youtube.com/watch?v=Eku_pbZ3-Mw"> WE by Daniel Preo≈£iuc-Pietro , Penn Positive Psychology Center</A> |<br>


<p> WORD EMBEDDINGS:
<a href="https://research.google.com/seedbank/seed/pretrained_word_embeddings"> research.google.com/seedbank/seed/pretrained_word_embeddings </A> |<br>
<a href="https://colab.research.google.com/drive/1r8nZitVTd9grTGepUE04mu7eao_7dpnZ#forceEdit=true&offline=true&sandboxMode=true&scrollTo=gZRWEtDzC_zy"> colab notebook </A> |<br>
<a href="https://www.tensorflow.org/tutorials/representation/word2vec"> Vector Representations of Words using TF </A> |<br>
<a href="https://medium.com/artists-and-machine-intelligence/ami-residency-part-1-exploring-word-space-andprojecting-meaning-onto-noise-98af7252f749"> or this series of blog posts by Memo Akten </A> |<br>
<a href="https://medium.com/deeper-learning/glossary-of-deep-learning-word-embedding-f90c3cec34ca"> glossary of deep learning word embedding</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<p> WORD EMBEDDINGS DEMOS:
<a href="http://bionlp-www.utu.fi/wv_demo/">filtered demo </A> |<br>
<a href="http://turbomaze.github.io/word2vecjson/"> unfiltered demo </A> |<br>
<a href="https://github.com/memo/ofxMSAWord2Vec"> Memo Atken code: memo/ofxMSAWord2Vec </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="http://www.thushv.com/natural_language_processing/word2vec-part-1-nlp-with-deep-learning-with-tensorflow-skip-gram/"> Word2Vec (Part 1): NLP With Deep Learning with Tensorflow (Skip-gram) </a> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

<a href=""> </A> |<br>


<p> doc2vec:
<a href="https://rare-technologies.com/doc2vec-tutorial/">Radim Rehurek: Doc2vec tutorial </A> |<br>
<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/doc2vec-IMDB.ipynb"> Gensim Doc2Vec Tutorial on the IMDB Sentiment Dataset</A> |<br>
<a href=""> </A> |<br>
<a href="https://www.youtube.com/watch?v=zFScws0mb7M"> Robert Meyer - Analysing user comments with Doc2Vec and Machine Learning classification </A> |<br>
<a href="https://www.youtube.com/watch?v=N4DimsHMu4g&list=PLYLrWv8e1ExV6sTZkYfje_Guf8-JYzqiu"> combo of 4 youtube videos </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<p> StreamHacker :<br>
<a href="https://streamhacker.com/2018/11/13/nlp-log-analysis-tokenization/"> nlp-log-analysis-tokenization </A> |<br>
<a href="https://streamhacker.com/2009/02/23/chunk-extraction-with-nltk/"> chunk-extraction-with-nltk </A> |<br>
<a href="https://streamhacker.com/2014/12/29/word2vec-nltk/"> word2vec-nltk </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

</div>
</a>








<hr>
<hr>
<a name="CRF">
<div id="CRF">
<p> CRF (Conditional Random Field):
<br>
NER is a word classification problem where each word of the sentence has to be classified among the labelled tags.
<br>
<a href="https://www.chokkan.org/software/crfsuite"> www.chokkan.org/software/crfsuite CRFsuite: A fast implementation of Conditional Random Fields (CRFs)</A> |<br>
<a href="https://github.com/chokkan/crfsuite"> github.com/chokkan/crfsuite - C++ implementation</A> |<br>
<a href="https://github.com/aleju/ner-crf"> github.com/aleju/ner-crf - NER implementation of crfsuite </A> |<br>
<a href="https://github.com/scrapinghub/python-crfsuite"> github.com/scrapinghub/python-crfsuite - Cython implementation </A> |<br>
<a href="https://github.com/lancifollia/crf"> github.com/lancifollia/crf pure-Python implementation </A> |<br>

<br>         
         
<a href="https://github.com/TeamHG-Memex/sklearn-crfsuite"> github.com/TeamHG-Memex/sklearn-crfsuite - python-crfsuite wrapper which provides interface simlar to scikit-learn </A> |<br>

<a href="https://eli5.readthedocs.io/en/latest/tutorials/sklearn_crfsuite.html"> eli5.readthedocs.io/en/latest/tutorials/sklearn_crfsuite.html - NER example 1 with sklearn-crfsuite </A> |<br>

<a href="https://towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2"> towardsdatascience.com/named-entity-recognition-and-classification-with-scikit-learn-f05372f07ba2 - NER example 2 with sklearn-crfsuite </A> |<br>

<a href="https://www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/"> www.depends-on-the-definition.com/named-entity-recognition-conditional-random-fields-python/  - NER example 3 with sklearn-crfsuite</A> |<br>
<br>
Keras biLSTM-CRF:<br>
<a href="https://arxiv.org/pdf/1508.01991v1.pdf"> arxiv.org/pdf/1508.01991v1.pdf - biLSTM-CRF </A> |<br>
<a href="https://github.com/keras-team/keras-contrib/tree/master/keras_contrib/layers"> github.com/keras-team/keras-contrib/tree/master/keras_contrib/layers - Keras implementation biLSTM-CRF </A> |<br>

<a href="https://medium.com/@rohit.sharma_7010/a-complete-tutorial-for-named-entity-recognition-and-extraction-in-natural-language-processing-71322b6fb090"> medium.com/@rohit.sharma_7010/a-complete-tutorial-for-named-entity-recognition-and-extraction-in-natural-language-processing-71322b6fb090 - EXAMPLE 1 of Keras implementation </A> |<br>

<a href="https://appliedmachinelearning.blog/2019/04/01/training-deep-learning-based-named-entity-recognition-from-scratch-disease-extraction-hackathon/"> appliedmachinelearning.blog/2019/04/01/training-deep-learning-based-named-entity-recognition-from-scratch-disease-extraction-hackathon/ - EXAMPLE 2 of Keras implementation </A> |<br>


<br>
<a href="http://blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/"> blog.echen.me/2012/01/03/introduction-to-conditional-random-fields/</A> |<br>

<a href="https://www.cs.ubc.ca/~murphyk/MLbook/pml-print3-ch19.pdf"> Undirected graphical models (Markov random fields) From
"Machine Learning: a Probabilistic Perspective" by Kevin Patrick Murphy </A> |<br>
<a href="https://www.amazon.com/Probabilistic-Networks-Expert-Systems-Computational/dp/0387987673"> Book by Robert G. Cowell et al "Probabilistic Networks and Expert Systems"</A> |<br>
<a href="https://en.wikipedia.org/wiki/Markov_random_field"> en.wikipedia.org/wiki/Markov_random_field </A> |<br>
<a href="https://en.wikipedia.org/wiki/Conditional_random_field"> en.wikipedia.org/wiki/Conditional_random_field </A> |<br>
<a href="https://en.wikipedia.org/wiki/Clique_(graph_theory)"> en.wikipedia.org/wiki/Clique_(graph_theory) </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>




<hr>
<hr>
<a name="KAGGLE_CURRENT_COMP">
<div id="KAGGLE_CURRENT_COMP">
<p> KAGGLE_CURRENT_COMP:
<br>
<a href="https://github.com/Kaggle/kaggle-api">https://github.com/Kaggle/kaggle-api </A> |<br>

<p>
KAGGLE SUBMISSION : <br>
<a href="https://www.youtube.com/watch?v=1G6eehnBUwI"> Submitting Predictions to Kaggle Competitions </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://www.kaggle.com/c/nlp-getting-started/notebooks"> Real or Not? NLP with Disaster Tweets -> Notebooks ->
SELECT A NOTEBOOK AND CLICK ON IT (eg Bert Starter)  ->  Click on OUTPUT on the LEFT -> Select OUTPUT FILE(s) -> Submit to Competition </A> |<br>
<a href="https://www.kaggle.com/c/nlp-getting-started/discussion"> https://www.kaggle.com/c/nlp-getting-started/discussion</A> |<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<hr>
<hr>
<a name="NLP_VIDEOS">
<div id="NLP_VIDEOS">
<p> NLP_VIDEOS:
<br>

<a href="https://www.youtube.com/watch?v=FLZvOKSCkxY&list=PLQVvvaa0QuDf2JswnfiGkliBInZnIC4HL"> Sentdex: NLTK with Python 3 for Natural Language Processing 21 videos </A> |<br>
<a href="https://pythonprogramming.net/part-of-speech-tagging-nltk-tutorial/"> pythonprogramming.net video: Part of Speech Tagging with NLTK </A> |<br>
<a href="https://pythonprogramming.net/chunking-nltk-tutorial/?completed=/part-of-speech-tagging-nltk-tutorial/"> 
pythonprogramming.net video: Chunking - Natural Language Processing With Python and NLTK p.5</A> |<br>
<br>


<a href="https://www.youtube.com/watch?v=3Dt_yh1mf_U&list=PLQiyVNMpDLKnZYBTUOlSI9mi9wAErFtFm"> Natural Language Processing | Dan Jurafsky, Christophe 101 videos </A> |<br>
<a href="https://www.youtube.com/watch?v=n25JjoixM3I&list=PLLssT5z_DsK8BdawOVCCaTCO99Ya58ryR"> Natural Language Processing [FULL COURSE] | University of Michigan 81 videos </A> |<br>
<a href="https://www.youtube.com/watch?v=cce8ntxP_XI&list=PLtmWHNX-gukKocXQOkQjuVxglSDYWsSh9&index=1"> 
Rachel Thomas: fast.ai Code-First Intro to Natural Language Processing 19 videos series </A> |<br>
<a href="https://www.youtube.com/playlist?list=PLoROMvodv4rOhcuXMZkNm7j3fVwBBY42z"> Christopher Manning & Co: CS224N: Natural Language Processing with Deep Learning 20 videos </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<p>
<hr>
<hr>
<a name="SENTIMENT_ANALYSIS">
<div id="SENTIMENT_ANALYSIS">
<p> SENTIMENT_ANALYSIS:
<br>

<a href="https://towardsdatascience.com/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3"> 
Reza Chu: Speech Emotion Recognition with Convolutional Neural Network Recognizing Human Emotion from AUDIO Recording </A> |<br>




<p>
<hr>
<hr>
<a name="Punctuation">
<div id="Punctuation">
<p> PUNCTUATION:
<br>
<a href="http://bark.phon.ioc.ee/punctuator"> PUNCTUATION restoration DEMO: http://bark.phon.ioc.ee/punctuator </A> |<br>
<a href="https://github.com/ottokart/punctuator2"> Punctuation2 restoration (bLSTM) : https://github.com/ottokart/punctuator2 </A> |<br>
<a href="https://github.com/mkhon/punctuator2/tree/python3">Py3:  https://github.com/mkhon/punctuator2/tree/python3 </A> |<br>
<a href="https://github.com/alpoktem/punkProse"> fork from punctuator2 </A> |<br>
<a href="https://github.com/ottokart/punctuator">punctuator1 by Otto Kart </A> |<br>

<br>
<a href="https://github.com/vackosar/keras-punctuator">keras-punctuator (CNN) </A> |<br>
<a href="https://www.youtube.com/watch?v=w-w3QamQIKY"> V√°clav Ko≈°a≈ô lecture: 
Keras Convolutional Text Punctuator Presentation </A> |<br>

<br>
<a href="https://medium.com/@praneethbedapudi/deepcorrection2-automatic-punctuation-restoration-ac4a837d92d9"> Deepcorrect and deepsegment article:</A> |<br>
<a href="https://github.com/bedapudi6788/deepcorrect"> deepcorrect (good for periods)</A> |<br>
<a href="http://bpraneeth.com/projects/deeppunct"> DEMO http://bpraneeth.com/projects/deeppunct</A> |<br>
<a href="https://github.com/ottokart/sequence-labeler"> https://github.com/ottokart/sequence-labeler </A> |<br>
<a href="https://github.com/nipunsadvilkar/pySBD (rule-based)">https://github.com/nipunsadvilkar/pySBD (rule-based) </A> |<br>
<a href="https://github.com/kaituoxu/X-Punctuator"> https://github.com/kaituoxu/X-Punctuator (PyTorch implementation) </A> |<br>
<a href="https://github.com/fnl/syntok"> https://github.com/fnl/syntok</A> |<br>
<a href="https://github.com/IsaacChanghau/neural_sequence_labeling">https://github.com/IsaacChanghau/neural_sequence_labeling </A> |<br>
<a href="https://github.com/LanguageMachines/ucto">https://github.com/LanguageMachines/ucto </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>


<p>
Good Sentence<br>

<a href="https://datascience.stackexchange.com/questions/37319/algorithms-that-can-determine-whether-a-string-is-an-english-sentence/37664"> determine-whether-a-string-is-an-english-sentence </A> |<br>
<a href="https://datascience.stackexchange.com/questions/19452/how-to-determine-the-complexity-of-an-english-sentence">determine-the-complexity-of-an-english-sentence </A> |<br>
<a href="https://github.com/chartbeat-labs/textacy">https://github.com/chartbeat-labs/textacy </A> |<br>
<a href="https://chartbeat-labs.github.io/textacy/getting_started/quickstart.html#working-with-text"> textacy/getting_started/quickstart.html#working-with-text </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>









<hr>
<hr>
<a name="TEXT_Summarization">
<div id="TEXT_Summarization">
<p> TEXT_Summarization:
<br>
<a href="https://hsuwanting.github.io/unified_summ/"> hsuwanting.github.io/unified_summ/</a> |<br>
<a href="https://www.aclweb.org/anthology/attachments/P18-1013.Presentation.pdf"> P18-1013.Presentation.pdf </a> |<br>

<br>
Extractive Summarization:<br>
<a href="https://github.com/dmmiller612/bert-extractive-summarizer/blob/master/colab/summary_gpu.ipynb> github.com/dmmiller612/bert-extractive-summarizer </A> |<br>
<a href="https://github.com/alexvlis/extractive-document-summarization">
github.com/alexvlis/extractive-document-summarization </A> |<br>
<a href=""> </A> |<br>
<a href="https://becominghuman.ai/text-summarization-in-5-steps-using-nltk-65b21e352b65"> 
Akash Panchal: Text summarization in 5 steps using NLTK: WordFrequency Algorithm</A> |<br>
<a href="https://stackabuse.com/text-summarization-with-nltk-in-python/"> 
Usman Malik: Text Summarization with NLTK in Python</A> |<br>

<a href=""> </a> |<br>
<a href="https://arxiv.org/pdf/1903.10318.pdf"> 2019.09 Yang Liu: Fine-tune BERT for Extractive Summarization </a> |<br>
<a href="https://github.com/nlpyang/BertSum"> 2019.09: github code: Fine-tune BERT for Extractive Summarization</a> |<br>
<a href=""> </a> |<br>



<br>
Attention based (abstractive) Text Summarization:<br>


<a href="https://www.analyticsvidhya.com/blog/2019/06/comprehensive-guide-text-summarization-using-deep-learning-python/"> 
2019.07 FAravind Pai: Comprehensive Guide to Text Summarization using Deep Learning in Python
</A> |<br>
<a href="https://www.kaggle.com/snap/amazon-fine-food-reviews/kernels"> 
Amazon Fine Food Reviews: (Analyze ~500,000 food reviews from Amazon) :
https://www.kaggle.com/snap/amazon-fine-food-reviews/kernels</A> |<br>
<a href="https://github.com/aravindpai/How-to-build-own-text-summarizer-using-deep-learning/blob/master/How_to_build_own_text_summarizer_using_deep_learning.ipynb"> 2019.07 aravindpai: github code: How-to-build-own-text-summarizer-using-deep-learning </A> |<br>
<a href=""> </A> |<br>


<a href="https://deeplearninganalytics.org/text-summarization/"> 2019.07 PRIYA TORONTO: Text Summarization using BERT </A> |<br>
<a href="https://youtu.be/ogrJaOIuBx4"> 2017 Siraj Raval: How to Make a Text Summarizer - Intro to Deep Learning #10 </A> |<br>
<a href="https://github.com/llSourcell/How_to_make_a_text_summarizer"> 
This is the code for "How to Make a Text Summarizer - Intro to Deep Learning #10" by Siraj Raval on Youtube
</A> |<br>
<a href="https://github.com/jiexunsee/rudimentary-ai-composer"> A very basic LSTM composer, doesn't compose any proper music for now </A> |<br>
<a href=""> </A> |<br>
<a href="https://arxiv.org/ftp/arxiv/papers/1906/1906.04165.pdf"> Leveraging BERT for Extractive Text Summarization on Lectures, by Derek Miller  </a> |<br>
<a href=""> </A> |<br>

<a href="https://arxiv.org/pdf/1805.06266.pdf"> 2018 Hsu et al: A Unified Model for Extractive and Abstractive Summarization
using Inconsistency Loss </A> |<br>

<a href="https://github.com/HsuWanTing/unified-summarization"> 2017 github Py 2.7: HsuWanTing/unified-summarization </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>






<hr>
<hr>
<a name="Allen_NLP">
<div id="Allen_NLP">
<p> Allen_NLP:
<br>

<a href="https://allennlp.org/"> Allen NLP: An open-source NLP research library, built on PyTorch</A> |<br>
<a href="https://demo.allennlp.org/reading-comprehension"> NLP DEMO: https://demo.allennlp.org/reading-comprehension</A> |<br>
<a href="https://github.com/allenai/writing-code-for-nlp-research-emnlp2018/blob/master/writing_code_for_nlp_research.pdf"> 
Slides: Writing Code for NLP Research </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>



<br><br>



<p>







- Adversarial Zoo: arXiv:1406.2661, 1609.03126, 1611.04076, https://bit.ly/2zqYI9F<br>
- Adversarial Dictionary Learning https://arxiv.org/abs/1710.04087 []<br>
- Adversarial Domain Adaptation https://arxiv.org/abs/1409.7495 [Unsupervised Domain Adaptation by Backpropagation by Yaroslav Ganin, Victor Lempitsky]<br>
- Multimodality Problem http://bit.ly/2o404Tz<br>
- VAE Language Model https:/arxiv.org/abs/1511.06349 <br>
- VAE features: phrase morphing https://bit.ly/2Q1Gsy3 <br>
- VAE prototype editor https://arxiv.org/abs/1709.08878 <br>
- VAE for fast inference in seq2seg (NLP) https://arxiv.org/abs/1803.03382<br>
- Summary on learning distributions:
--- MLfit ==>> Pros: Explicit, Stable --- Cons: Slow inference, requires order <br>
--- GANs  ==>> Pros: Great samples, Fast --- Cons: Implicit, Unstable training <br>
--- VAEs  ==>> Pros: Explicit, Stable, Fast --- Cons: Blurry samples <br>
- Hack: https://github.com/soumith/ganhacks<br>

<p>



<hr>
<hr>
<a name="Chervov_Chervov">
<div id="Chervov_Chervov">
<p> Chervov_Chervov:<br>

–¢–æ–≥–¥–∞, –≤–æ—Ç. –ú–æ–∂–µ—Ç, –∫–æ–º—É –ø—Ä–∏–≥–æ–¥–∏—Ç—Å—è. –°–æ–±–∏—Ä–∞—é –¥–ª—è —Å–µ–±—è. <br>
–°–∏—Å—Ç–µ–º–∞ –∏–∑ –º–µ—Ç–∞-–∫–∞–Ω–∞–ª–æ–≤:<br>
https://t.me/meta_it ‚Äî —á–∞—Ç—ã –∏ –∫–∞–Ω–∞–ª—ã –æ –º–∏—Ä–µ IT<br>
https://t.me/meta_jobs ‚Äî —á–∞—Ç—ã –∏ –∫–∞–Ω–∞–ª—ã –¥–ª—è –ø–æ–∏—Å–∫–∞ —Ä–∞–±–æ—Ç—ã<br>
https://t.me/meta_business ‚Äî —á–∞—Ç—ã –∏ –∫–∞–Ω–∞–ª—ã –ø—Ä–æ –±–∏–∑–Ω–µ—Å<br>
https://t.me/meta_news_channel ‚Äî –∫–∞–Ω–∞–ª—ã —Å –Ω–æ–≤–æ—Å—Ç—è–º–∏<br>
https://t.me/meta_blogs ‚Äî –±–ª–æ–≥–∏<br>
https://t.me/meta_finance ‚Äî—á–∞—Ç—ã –∏ –∫–∞–Ω–∞–ª—ã –ø—Ä–æ —Ñ–∏–Ω–∞–Ω—Å—ã –∏ –∏–Ω–≤–µ—Å—Ç–∏—Ü–∏–∏<br>
https://t.me/meta_bots ‚Äî –±–æ—Ç—ã<br>
https://t.me/meta_git ‚Äî git-—Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏<br>

<a href="https://github.com/3b1b/manim">  https://github.com/3b1b/manim</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

==========><br>

<a href="https://habr.com/ru/post/516098/">2020: –ü—Ä–æ–µ–∫—Ç Natasha. –ù–∞–±–æ—Ä –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—Ç–∫—Ä—ã—Ç—ã—Ö –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —Ä—É—Å—Å–∫–æ–≥–æ —è–∑—ã–∫–∞ (NLP) / –•–∞–±—Ä </A> |<br>
<a href="https://twizz.ru/20-udivitelnyx-kart-kotorye-pokazyvayut-nam-to-o-chyom-my-ne-zadumyvalis/"> 20 —É–¥–∏–≤–∏—Ç–µ–ª—å–Ω—ã—Ö –∫–∞—Ä—Ç, –∫–æ—Ç–æ—Ä—ã–µ –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç –Ω–∞–º —Ç–æ, –æ —á—ë–º –º—ã –Ω–µ –∑–∞–¥—É–º—ã–≤–∞–ª–∏—Å—å
</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="http://oneweirdkerneltrick.com/rank.pdf"> http://oneweirdkerneltrick.com/rank.pdf</A> |<br>
<a href="https://www.kaggle.com/c/champs-scalar-coupling"> https://www.kaggle.com/c/champs-scalar-coupling: CHAMPS (CHemistry And Mathematics in Phase Space)
2,749 teams a year ago:  Predicting Molecular Properties
Can you measure the magnetic interactions between a pair of atoms?</A> |<br>
==================> Andrew Lukyanenko:<p>
<a href="https://www.kaggle.com/artgor/coronavirus-infection-in-finland"> https://www.kaggle.com/artgor/coronavirus-infection-in-finland</A> |<br>
<a href="https://www.kaggle.com/c/champs-scalar-coupling/discussion/106304"> Andrew Lukyanenko: This is my first gold medal!!! Experiencing emotional overflow! </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href="https://stepik.org/course/3356/promo"> https://stepik.org/course/3356/promo</A> |<br>
==================> maximilian nickel <p>
https://github.com/mnick?tab=repositories<br>
https://github.com/mnick/sent-conv-torch<br>
https://github.com/mnick/scikit-kge<br>
==================> <p>
<a href="https://arxiv.org/abs/1908.06267v2"> 2019: Giannis Nikolentzos, Antoine J.-P. Tixier, Michalis Vazirgiannis: Message Passing Attention Networks for Document Understanding</A> |<br>
<a href="https://github.com/giannisnik/mpad"> GitHub: Message Passing Attention Networks for Document Understanding</A> |<br>
<a href="https://paperswithcode.com/area/natural-language-processing"> https://paperswithcode.com/area/natural-language-processing</A> |<br>
<a href="https://paperswithcode.com/sota/"> https://paperswithcode.com/sota/ </A> |<br>
<a href="https://arxiv.org/abs/1804.06872"> 2018: Bo Han et al: Co-teaching: Robust Training of Deep Neural Networks with Extremely Noisy Labels</A> |<br>

==================> <p>

<a href="http://www.tylervigen.com/spurious-correlations"> http://www.tylervigen.com/spurious-correlations </A> |<br>
<a href="https://t.me/sberloga_ds/1457"> LogReg is all you need </A> |<br>
<a href="https://www.youtube.com/watch?v=5wMAPUrd0ag"> Data Science: Kaggle GRANDMASTER in 6 months? | Pavel Pleskov, Data Nerds </A> |<br>
<a href="https://www.youtube.com/watch?v=ymSqI0hVj-Q&feature=youtu.be"> 
Data Scientist: –∫—Ç–æ –Ω—É–∂–µ–Ω –±–∏–∑–Ω–µ—Å—É –∏ –∫–∞–∫ –∏—Ö –æ–±—É—á–∏—Ç—å | –í–∏–∫—Ç–æ—Ä –ö–∞–Ω—Ç–æ—Ä, Data Mining in Action </A> |<br>

===> Kaggle MoA : <p>
<a href="https://www.kaggle.com/alexandervc/moa-correlation-analysis-use-igraph">2020: Analysis of correlated features for MoA context. In particular we try to select groups of correllated genes (using graph utils). </A> |<br>
<a href="https://www.kaggle.com/c/lish-moa/discussion/180744"> 2020: MoA: Some materials on kaggle related to genes expressions</A> |<br>
<a href="https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2"> SIMAKOV's kernel </A> |<br>
<a href="https://www.kaggle.com/simakov/keras-multilabel-neural-network-v1-2?scriptVersionId=41964857">
SIMAKOV Selection of Important Features v4 cell 6</A> |<br>
<a href="https://www.kaggle.com/jpmiller/finding-patterns-in-the-scored-targets">JPMiller (FortWrth DS, West point and MIT graduate): Finding patterns in the scored targets </A> |<br>
<a href="https://www.kaggle.com/pavansanagapati/knowledge-graph-nlp-tutorial-bert-spacy-nltk">
Knowledge Graph & NLP Tutorial-(BERT,spaCy,NLTK) </A> |<br>
<a href="https://www.youtube.com/watch?v=z9KmhOyTX5g"> Neuroinformatics - 2019 - –ù. –ú–∞–∫–∞—Ä–µ–Ω–∫–æ </A> |<br>
<a href="https://www.kaggle.com/alexandervc/for-each-target-find-a-gene-best-predicting-it"> 
Chervov notebook: MoA kaggle contest: 
For each target find best predicting it gene (and several best predicting - top1 , top2 ...). Best here - means in the sense of rocauc.</A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>
<a href=""> </A> |<br>

<p>
<p>
==============> 

–≠—Ç–∏ —Ç—Ä–∏ —Å–Ω–∞ –º–Ω–µ —Ä–∞—Å—Å–∫–∞–∑–∞–ª –ü–æ—Ä—Ñ–∏—Ä—å–µ–≤–∏—á (https://porfirevich.ru/). 
–ß—Ç–æ–±—ã –ü–æ—Ä—Ñ–∏—Ä—å–µ–≤–∏—á –∑–∞–≥–æ–≤–æ—Ä–∏–ª, —è –¥–∞–ª –µ–º—É —Ç—Ä–∏ —Å–ª–æ–≤–∞ (—É–∫–∞–∑–∞–Ω–Ω—ã–µ –≤ —Å–∫–æ–±–∫–∞—Ö).
–°–Ω—ã –ø–æ–ª—É—á–∏–ª–∏—Å—å, –∫–∞–∫ –ø–æ–ª–æ–∂–µ–Ω–æ –≤—Å–µ–º —Å–Ω–∞–º - —Ñ—Ä–µ–π–¥–æ–≤—Å–∫–∏–µ.
–ü–æ—Ä—Ñ–∏—Ä—å–µ–≤–∏—á–∞ –æ–±—É—á–∞–ª–∏ "—Ä—É—Å—Å–∫–æ–º—É —è–∑—ã–∫—É" –Ω–∞ —Ç–æ–π –º–∞—Å—Å–µ –∏ —Ö–æ—Ä–æ—à–∏—Ö –∏ –Ω–µ –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤, 
–∫–æ—Ç–æ—Ä—ã–º–∏ –∑–∞–ø–æ–ª–Ω–µ–Ω–∞ —Å–µ—Ç—å. –í—ã–≤–æ–¥—ã –¥–µ–ª–∞–π—Ç–µ —Å–∞–º–∏.

-------------------- —Å–æ–Ω 1 
( –ü—É—Ç–∏–Ω—É –ø—Ä–∏—Å–Ω–∏–ª—Å—è –ù–∞–≤–∞–ª—å–Ω—ã–π ); —Å—Ç–∞–ª–æ —è—Å–Ω–æ, —á—Ç–æ –ø–æ—Ä–∞ –¥–µ–π—Å—Ç–≤–æ–≤–∞—Ç—å. –ù–∞–≤–∞–ª—å–Ω—ã–π –±—ã–ª –Ω–µ –æ–¥–∏–Ω ‚Äî —Å–æ —Å–≤–æ–µ–π –ø–æ–¥—Ä—É–≥–æ–π –∏ –¥–≤—É–º—è —Ç—è–∂–µ–ª–µ–Ω–Ω—ã–º–∏ –±—É—Ç—ã–ª–∫–∞–º–∏ ¬´–°—Ç–æ–ª–∏—á–Ω–æ–π¬ª. –ù–∞–≤–∞–ª—å–Ω—ã–π —Å–æ–≤—Å–µ–º –Ω–µ–¥–∞–≤–Ω–æ —Å–µ–ª –≤ —Ä—É—Å—Å–∫—É—é —Ä—É–ª–µ—Ç–∫—É –∏ –≤—ã–∏–≥—Ä–∞–ª —Ç—Ä–∏ —Ç—ã—Å—è—á–∏. –í–æ–π–¥—è –≤ –∑–∞–ª—É, –æ–Ω —Å–∫–∞–∑–∞–ª –ø–∞—Ä—Ç–Ω–µ—Ä—à–µ, —á—Ç–æ —Ö–æ—á–µ—Ç —Ä–∞–∑–¥–µ–ª–∏—Ç—å –≤—ã–∏–≥—Ä—ã—à –ø–æ–ø–æ–ª–∞–º, –∏ —Å–ø—Ä–æ—Å–∏–ª: ‚Äî –ö–∞–∫ —ç—Ç–æ –ø–æ–Ω—è—Ç—å? –î–µ–≤—É—à–∫–∞ —É–¥–∏–≤–ª–µ–Ω–Ω–æ –ø–æ—Å–º–æ—Ç—Ä–µ–ª–∞ –Ω–∞ –Ω–µ–≥–æ, –ø–æ—Ç–æ–º –Ω–∞ –∫–∞—Ä—Ç—ã, —Å—Ç–æ—è—â–∏–µ –Ω–∞ —Å—Ç–æ–ª–∏–∫–µ. –ù–∞–∫–æ–Ω–µ—Ü –æ–Ω–∞ –æ—Ç–≤–µ—Ç–∏–ª–∞: ‚Äî –•–æ—Ä–æ—à–æ, –¥—è–¥—è. –î–∞–≤–∞–π –≤—ã–ø—å–µ–º, —Ö–æ—Ç—å —Ä–∞–∑ –≤—ã–ø—å–µ–º –≤–º–µ—Å—Ç–µ.  –ù–∞–≤–∞–ª—å–Ω—ã–π —É—Å–º–µ—Ö–Ω—É–ª—Å—è: ‚Äî –ò –≤—ã–ø—å–µ–º, –∏ –Ω–µ –∑–∞–±—É–¥–µ–º, –∫—Ä–∞—Å–∞–≤–∏—Ü–∞‚Ä¶ –ö–æ–≥–¥–∞ —è —à–µ–ª –∫ —Å—Ç–æ–ª—É, –≤ –¥—É—à–µ —É –º–µ–Ω—è —Ä–æ—Å–ª–æ –±–µ—Å–ø–æ–∫–æ–π—Å—Ç–≤–æ.  –Ø –∑–∞–º–µ—Ç–∏–ª, —á—Ç–æ –î–∏–º–∞ —Å–∏–¥–∏—Ç —Å –±—É—Ç—ã–ª–∫–∞–º–∏ –≤ —Ä—É–∫–µ ‚Äî —Å—É–¥—è –ø–æ –≤—Å–µ–º—É, –æ–Ω –≤—ã–ø–∏–ª —á—Ç–æ-—Ç–æ –Ω–µ —Ç–æ.  –ß—Ç–æ–±—ã –∫–∞–∫-—Ç–æ —Ä–∞–∑—Ä—è–¥–∏—Ç—å –æ–±—Å—Ç–∞–Ω–æ–≤–∫—É, —è —Å–∫–∞–∑–∞–ª: ‚Äî –≠–π, –ø–∞—Ä—è, –Ω–µ –∑–µ–≤–∞–π! –ò–≥—Ä–∞—Ç—å –±—É–¥–µ—à—å?


-------------------- —Å–æ–Ω 2
( –¢—Ä–∞–º–ø—É –ø—Ä–∏—Å–Ω–∏–ª—Å—è –ü—É—Ç–∏–Ω. )  –ü–µ—Ä–µ–¥ –Ω–∏–º –ª–µ–∂–∞–ª –º–µ–≥–∞—Ñ–æ–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ–ª–µ—Ñ–æ–Ω. –û–Ω –ø–æ–∑–≤–æ–Ω–∏–ª –ü—É—Ç–∏–Ω—É –∏ –ø–æ–ø—Ä–æ—Å–∏–ª —Å–æ–æ–±—â–∏—Ç—å, –∫–∞–∫ –∏–¥—É—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã. –¢–æ–≥–¥–∞ –ü—É—Ç–∏–Ω —Å–∫–∞–∑–∞–ª: ¬´–ù–æ—Ä–º–∞–ª—å–Ω–æ¬ª. –°–ª–æ–≤–æ ¬´–Ω–æ—Ä–º–∞–ª—å–Ω–æ¬ª –∑–¥–µ—Å—å –∞–±—Å–æ–ª—é—Ç–Ω–æ –Ω–∏ –ø—Ä–∏ —á–µ–º. –ù–æ —Ç–µ–ø–µ—Ä—å, —á–µ—Ä–µ–∑ —Ç—Ä–∏ —Å –ø–æ–ª–æ–≤–∏–Ω–æ–π —á–∞—Å–∞ –ø–æ—Å–ª–µ –≤–∏–∑–∏—Ç–∞ –ü—É—Ç–∏–Ω–∞ –≤ –∞–º–µ—Ä–∏–∫–∞–Ω—Å–∫–æ–µ –ø–æ—Å–æ–ª—å—Å—Ç–≤–æ, —ç—Ç–æ—Ç –∑–≤–æ–Ω–æ–∫ —Å—Ç–∞–ª —Å–æ–±—ã—Ç–∏–µ–º. –°–∞–º–∞ –º—ã—Å–ª—å –æ —Ç–æ–º, —á—Ç–æ –ü—É—Ç–∏–Ω –º–æ–∂–µ—Ç —Å–¥–µ–ª–∞—Ç—å —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ –ø–æ–ª–µ–∑–Ω–æ–µ –≤–æ –≤—Å–µ–º —ç—Ç–æ–º —Ö–∞–æ—Å–µ, ‚Äì –ø—Ä–æ—Å—Ç–æ –Ω–µ–≤–µ—Ä–æ—è—Ç–Ω–∞. –†–µ—á—å —à–ª–∞ –æ–± –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω–æ–º –ø—Ä–æ—Å—Ç—Ä–∞–Ω—Å—Ç–≤–µ, –≥–¥–µ –Ω–µ –º–æ–≥—É—Ç –∏–≥—Ä–∞—Ç—å –¥–∞–∂–µ —Å–∞–º—ã–µ —Ç–∞–ª–∞–Ω—Ç–ª–∏–≤—ã–µ –º–æ—Å–∫–æ–≤—Å–∫–∏–µ –∂—É—Ä–Ω–∞–ª–∏—Å—Ç—ã, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç –Ω–∞—Ö–æ–¥–∏—Ç—å—Å—è –≤ –ú–æ—Å–∫–≤–µ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å. –ì–æ–≤–æ—Ä—è—Ç, —á—Ç–æ –ü—É—Ç–∏–Ω —Å–∫–∞–∑–∞–ª –æ–¥–Ω–æ–º—É –∏–∑ –∑–∞–ø–∞–¥–Ω—ã—Ö —ç–∫—Å–ø–µ—Ä—Ç–æ–≤: ¬´–î–ª—è –∏—Ö –ú–æ—Å–∫–≤—ã –µ—Å—Ç—å –¥–≤–∞ —Å–µ—Ä—å–µ–∑–Ω—ã—Ö –≤–∞—Ä–∏–∞–Ω—Ç–∞. –õ–∏–±–æ –æ–Ω–∞ –ø–æ—Å—Ç–æ—è–Ω–Ω–æ —Ä–∞—Å—à–∏—Ä—è–µ—Ç—Å—è, –ª–∏–±–æ –ø–µ—Ä–µ—Å—Ç–∞–µ—Ç —Ä–∞—Å—à–∏—Ä—è—Ç—å—Å—è.


-------------------- —Å–æ–Ω 3 
( –ü—É—Ç–∏–Ω—É –ø—Ä–∏—Å–Ω–∏–ª—Å—è –¢—Ä–∞–º–ø ) ‚Äì —Å–æ–≤—Å–µ–º –∫—Ä–æ—à–µ—á–Ω—ã–π, –∑–∞—Ç–æ –∑–∞–Ω—è—Ç–Ω—ã–π: —Å –Ω–∞–∫–ª–µ–µ–Ω–Ω—ã–º–∏ –∑–æ–ª–æ—Ç—ã–º–∏ —É—Å–∞–º–∏, —Å –∑–æ–Ω—Ç–∏–∫–æ–º –Ω–∞ –≥–æ–ª–æ–≤–µ –∏ —Å —Ç—É–≥–æ –Ω–∞–±–∏—Ç–æ–π –≥—É–±–Ω–æ–π –≥–∞—Ä–º–æ—à–∫–æ–π. –¢—Ä–∞–º–ø –±—ã–ª —Ç–æ—á–Ω–æ —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ –≤–æ –≤—Å–µ—Ö –æ—Å—Ç–∞–ª—å–Ω—ã—Ö —Å–Ω–∞—Ö, –∏ –µ–º—É —Å–Ω–∏–ª–∏—Å—å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ —Å–Ω—ã ‚Äì —Ç–æ–∂–µ –≤ –≤–∏–¥–µ –ø–∞–∑–ª–æ–≤. –í–æ —Å–Ω–µ —É –Ω–µ–≥–æ –±—ã–ª–æ –º–Ω–æ–≥–æ —Ä–∞–∑–Ω—ã—Ö –¥–µ–ª ‚Äì –æ–Ω —Å—Ç—Ä–æ–∏–ª –∏–∑ –ø–ª–∞—Å—Ç–∏–ª–∏–Ω–∞ –¥–æ–º–∞, —Å—Ç—Ä–æ–∏–ª –≤–æ —Å–Ω–µ —Å–∞–º–æ–ª–µ—Ç, –∏–∑ –∫–∞–ª—å–∫–∏ —Ä–∏—Å–æ–≤–∞–ª –¥–≤–æ—Ä–µ—Ü –¥–ª—è —Å–≤–æ–µ–≥–æ —Å—ã–Ω–∞, –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ. –£ –Ω–µ–≥–æ –±—ã–ª–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–∞—è –º–∞—à–∏–Ω–∞ ‚Äì —Ç–æ—Ç —Å–∞–º—ã–π ¬´–†–æ–ª–ª—Å-–†–æ–π—Å¬ª, –æ –∫–æ—Ç–æ—Ä–æ–º –º—ã —É–∂–µ –≥–æ–≤–æ—Ä–∏–ª–∏. –í—Å–µ –µ–≥–æ –±–µ–¥—ã –∏ —É–¥–∞—á–∏ –±—ã–ª–∏ —Å–≤—è–∑–∞–Ω—ã —Å —ç—Ç–∏–º –∞–≤—Ç–æ–º–æ–±–∏–ª–µ–º. –ê –º–µ–∂–¥—É —Ç–µ–º –≤–æ —Å–Ω–µ –≤—Å–µ –µ–≥–æ –±–µ–¥—ã –ø—Ä–æ—Ö–æ–¥–∏–ª–∏ —Å—Ç–æ—Ä–æ–Ω–æ–π. –¢—Ä–∞–º–ø –±—ã–ª —Ö–æ—Ä–æ—à–∏–º –º–∞–ª—å—á–∏–∫–æ–º. –ù–æ –≤ –¥—É—à–µ —É –Ω–µ–≥–æ –ø—Ä—è—Ç–∞–ª—Å—è –º–∞–ª–µ–Ω—å–∫–∏–π –º–∞–ª—å—á–∏–∫ ‚Äì —Ç–∞–∫–æ–π –∂–µ, –∫–∞–∫ —Ç–æ–≥–¥–∞, –∫–æ–≥–¥–∞ –æ–Ω –∏–∑–æ–±—Ä–∞–∂–∞–ª –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –º–∞–ª—å—á–∏–∫–∞. –û–Ω –¥–æ–ª–∂–µ–Ω –±—ã–ª –ø—Ä–∏–¥—É–º–∞—Ç—å –∫–∞–∫–æ–µ-–Ω–∏–±—É–¥—å —Ö–∏—Ç—Ä–æ–µ –æ–±—Ä–∞—â–µ–Ω–∏–µ –∫ —Ä–µ–±–µ–Ω–∫—É, —Å–ø–æ—Å–æ–±–Ω–æ–µ –æ–±—ä—è—Å–Ω–∏—Ç—å –¢—Ä–∞–º–ø—É, –∫—Ç–æ –∏–º–µ–Ω–Ω–æ –∏–∑–æ–±—Ä–∞–∂–∞–ª –º–∞–ª–µ–Ω—å–∫–æ–≥–æ –º–∞–ª—å—á–∏–∫–∞.




<p>
( –¢—Ä–∞–º–ø—É –ø—Ä–∏—Å–Ω–∏–ª—Å—è –ü—É—Ç–∏–Ω.)  –ü–µ—Ä–µ–¥ –Ω–∏–º –ª–µ–∂–∞–ª –º–µ–≥–∞—Ñ–æ–Ω–∏—á–µ—Å–∫–∏–π —Ç–µ–ª–µ—Ñ–æ–Ω. –û–Ω –ø–æ–∑–≤–æ–Ω–∏–ª –ü—É—Ç–∏–Ω—É –∏ –ø–æ–ø—Ä–æ—Å–∏–ª —Å–æ–æ–±—â–∏—Ç—å, –∫–∞–∫ –∏–¥—É—Ç –ø–µ—Ä–µ–≥–æ–≤–æ—Ä—ã. –¢–æ–≥–¥–∞ –ü—É—Ç–∏–Ω —Å–∫–∞–∑–∞–ª: ¬´–ù–æ—Ä–º–∞–ª—å–Ω–æ¬ª. –ò —Ç—É—Ç –∂–µ –µ–º—É –∑–∞–∑–≤–æ–Ω–∏–ª –¥—Ä—É–≥–æ–π —Ç–µ–ª–µ—Ñ–æ–Ω. –ü—É—Ç–∏–Ω —Å–∫–∞–∑–∞–ª: ¬´–ò —á—Ç–æ —Å –≤–∞–º–∏ –±—É–¥–µ—Ç? –¢–∞–º –∂–µ —Ü–µ–ª–∞—è —Å—ä–µ–º–æ—á–Ω–∞—è –≥—Ä—É–ø–ø–∞!¬ª ‚Äî –∏ –±—Ä–æ—Å–∏–ª —Ç—Ä—É–±–∫—É –Ω–∞ —Ä—ã—á–∞–≥.







<p>
<hr>
<hr>
<a name="SET_THEORY_SET_THEORY">
<div id="SET_THEORY_SET_THEORY">
<p> SET_THEORY_SET_THEORY:
<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF="http://people.clas.ufl.edu/wjm/files/inner_model_history.pdf">Inner Model History by W.Mitchell </A> |<br>
<A HREF="https://faculty.unt.edu/editprofile.php?pid=2006#34"> William (Bill) Eugene Acree</A> |<br>
<A HREF="http://digital.library.unt.edu/explore/collections/UNTETD/browse/?fq=untl_decade%3A1990-1999"> UNT Digital Library</A> |<br>
<A HREF="https://faculty.unt.edu/editprofile.php?onlyview=1&pid=2024">Steve Jackson </A> |<br>
<A HREF="http://www.moreheadstate.edu/content_template.aspx?id=2317"> Rus May</A> |<br>
<A HREF="http://dblp.uni-trier.de/pers/hy/j/Jackson:Steve.html">Steve, Rus Paper </A> |<br>
<A HREF="https://www.youtube.com/watch?v=ZC7wglkBWMM">Woodin on CH </A> |<br>
<A HREF="http://www.worldsciencefestival.com/2014/01/full_program_infinity/"> Infinity</A> |<br>
<A HREF="https://www.youtube.com/watch?v=faQBrAQ87l4">Video: Hilbert's Infinite Hotel </A> |<br>
<A HREF="https://www.youtube.com/watch?v=s86-Z-CbaHA">Video: Banach-Tarski Paradox</A> |<br>
<A HREF="https://www.youtube.com/watch?v=nVF4N1Ix5WI"> The Continuum Hypothesis and the search for Mathematical Infinity, W. Hugh Woodin</A> |<br>
<A HREF="https://www.youtube.com/watch?v=4bvuwsFL04g"> Woodin: The trouble with Infinity</A> |<br>
<A HREF="https://youtu.be/TbeA1rhV0D0"> How Big are All Infinities Combined? (Cantor's Paradox) | Infinite Series </A> |<br>
<p>
<A HREF="https://www.youtube.com/watch?v=JPBf8hj2bYA"> Exploring the Frontiers of Incompleteness: Hugh Woodin</A> |<br>
<A HREF="http://logic.harvard.edu/efi.php#"> Exploring the Frontiers of Incompleteness</A> |<br>
<A HREF="https://www.youtube.com/user/glavobolie25"> Ivan Grozev</A> |<br>
<A HREF="http://aquilaaquilonis.livejournal.com/268535.html"> Russian Math School</A> |<br>
<A HREF="http://bookz.ru/authors/kollektiv-avtorov/duh-v-tv_979/page-5-duh-v-tv_979.html"> III. –ë–µ—Å–∫–æ–Ω–µ—á–Ω–æ—Å—Ç—å –≤ –†–æ—Å—Å–∏–∏</A> |<br>
<A HREF=""> </A> |
<A HREF="http://www.worldsciencefestival.com/2014/01/full_program_infinity/"> Infinity. Wooding appears at 30-27mins, 55-62 mins (interesting comments made by Steven Strogatz), 76 mins</A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<p>
============>–ú–∞–∫–∞—Ä –°–≤–µ—Ç–ª—ã–π<br>
<A HREF="https://www.youtube.com/user/MakarSvet13"> –ú–∞–∫–∞—Ä –°–≤–µ—Ç–ª—ã–π  </A> |<br>
<A HREF="https://www.youtube.com/watch?reload=9&v=2e6heBWGIVs&feature=youtu.be"> 2020: YT –ú–∞–∫–∞—Ä –°–≤–µ—Ç–ª—ã–π: –¢–µ–æ—Ä–∏—è –º–Ω–æ–∂–µ—Å—Ç–≤: –ª–æ–≥–∏–∫–∞, —Ñ–æ—Ä–º–∞–ª–∏–∑–º –∏ –∫—Ä–∏–∑–∏—Å </A> |<br>
<A HREF="https://www.youtube.com/watch?v=B7Rxqagb27Q"> –°–≤–µ—Ç–ª—ã–π –±–ª–æ–≥ #10. –¢–û–ü–õ–ï–° –ù–ï –ù–ê–£–ß–ü–û–ü </A> |<br>
<A HREF="https://www.youtube.com/watch?v=tWe1WcWLRec"> –î–∏—Ñ—Ñ–µ—Ä–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ —É—Ä–∞–≤–Ω–µ–Ω–∏—è. –†–∞–≤–Ω–æ–≤–µ—Å–∏–µ –∏ –•–∞–æ—Å </A> |<br>
<A HREF="https://www.youtube.com/watch?v=Hwd0755SCx0"> Science show. –í—ã–ø—É—Å–∫ ‚Ññ 62. –¢–µ–æ—Ä–∏—è –Ø–Ω–≥–∞-–ú–∏–ª–ª—Å–∞ –∏ –º–∞—Å—Å–æ–≤–∞—è —â–µ–ª—å </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>

<A HREF="https://www.youtube.com/c/veritasium/videos"> Veritasium</A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<A HREF=""> </A> |<br>
<p>


